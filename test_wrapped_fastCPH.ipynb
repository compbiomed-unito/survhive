{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d5ccd9-853f-4e52-b3c4-696e7212e5a0",
   "metadata": {},
   "source": [
    "# Testing fastCPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387d1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4083da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7d6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import survhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210bf36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = survhive.load_test_data()\n",
    "X.shape, y.shape\n",
    "min(survhive.get_time(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4b7a48-fc83-4238-8d55-9570bf1c2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_data='gbsg2'\n",
    "#my_data_df=survhive.datasets.get_data(my_data)\n",
    "seed=2401\n",
    "\n",
    "#X, y= my_data_df.get_X_y()\n",
    "#X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbcb80-0388-4f46-ba3a-29c3f9ee5bee",
   "metadata": {},
   "source": [
    "### Generate a (stratified) train-test split and Scale the features (only) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f553a-ae5b-476e-bb42-ec48efe8ab89",
   "metadata": {},
   "source": [
    "First do the stratified splitting THEN do scaling, parameterized on X_train set ONLY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75714389-2def-4259-8052-bcf7977a2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd02c821-86c7-4515-986b-07e769f9581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = survhive.survival_train_test_split(X, y,rng_seed=2311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "261585e2-6d7a-4dc8-9861-da0aee20e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((148, 84), (50, 84))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "[X_train, X_test] = [ scaler.transform(_) for _ in  [X_train, X_test] ]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f89b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523fc60-5251-4640-ab08-b4288c7a65b4",
   "metadata": {},
   "source": [
    "balanced partitioning OK. Robst scaler damages the performance of DSM A LOT.\n",
    "maybe did something wrong. It is standard scaler for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "449b6bde-498c-437f-8170-e86f34aafc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 38, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survhive.get_indicator(y).sum(), survhive.get_indicator(y_train).sum(), survhive.get_indicator(y_test).sum(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b452cd-4763-4f78-800f-3e8c78ec3bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 12, 13, 13, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "splitter = survhive.survival_crossval_splitter(X_train,y_train,n_splits=3, n_repeats=2,rng_seed=2309)\n",
    "print([ (survhive.get_indicator(y_train[_[1]]).sum()) for _ in splitter])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18343e1d-30ec-42b0-be0e-ac85df1298d9",
   "metadata": {},
   "source": [
    "## check possible dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fde3a10a-379c-42b8-8070-485e7c4326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad7a35b-9295-40f8-a9c4-a02ca8cf83c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca= PCA(n_components=0.995, random_state=2308).fit(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3f2d1-45e8-42cc-9763-69836b1aaa71",
   "metadata": {},
   "source": [
    "Only a modest dimensionality reduction is possible using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7a81c3-ced7-48df-8eba-cacb1e814bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stratified CV spliter for survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e63ce8de-3339-4d4f-9df3-d11c8e61feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56be10c8-e601-4cbd-b2bf-47ce6c0be63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FastCPH(rng_seed=2401, layer_sizes=[4, 4], tie_approximation=&#x27;breslow&#x27;, lambda_seq=[0.001, 0.0010249999999999999, 0.001050625, 0.0010768906249999997, 0.0011038128906249996, 0.0011314082128906244, 0.00115969341821289, 0.0011886857536682123, 0.0012184028975099172, 0.0012488629699476653, 0.0012800845441963567, 0.0013120866578012656, 0.0013448888242462971, 0.0013785110448524542, 0.0014129738209737656, 0.0014482981664981096, 0.0014845056206605623, 0.001521618261177076, 0.001559658717706503, 0.0015986501856491654, 0.0016386164402903943, 0.0016795818512976542, 0.0017215713975800952, 0.0017646106825195975, 0.0018087259495825872, 0.0018539440983221518, 0.0019002927007802054, 0.0019478000182997105, 0.001996495018757203, 0.002046407394226133, 0.002097567579081786, 0.0021500067685588306, 0.002203756937772801, 0.0022588508612171212, 0.0023153221327475485, 0.0023732051860662373, 0.002432535315717893, 0.00249334869861084, 0.002555682416076111, 0.0026195744764780132, 0.0026850638383899635, 0.0027521904343497123, 0.0028209951952084546, 0.002891520075088666, 0.0029638080769658825, 0.0030379032788900293, 0.0031138508608622795, 0.0031916971323838364, 0.003271489560693432, 0.0033532767997107677, 0.0034371087197035363, 0.003523036437696125, 0.0036111123486385274, 0.0037013901573544903, 0.0037939249112883522, 0.003888773034070561, 0.003985992359922324, 0.004085642168920382, 0.004187783223143391, 0.0042924778037219755, 0.004399789748815024, 0.0045097844925354, 0.004622529104848784, 0.004738092332470004, 0.004856544640781753, 0.004977958256801297, 0.005102407213221329, 0.0052299673935518615, 0.005360716578390657, 0.005494734492850423, 0.005632102855171683, 0.005772905426550975, 0.005917228062214749, 0.006065158763770117, 0.0062167877328643695, 0.006372207426185978, 0.006531512611840627, 0.006694800427136643, 0.006862170437815058, 0.007033724698760434, 0.007209567816229444, 0.00738980701163518, 0.007574552186926058, 0.007763915991599209, 0.007958013891389188, 0.008156964238673917, 0.008360888344640766, 0.008569910553256782, 0.008784158317088201, 0.009003762275015407, 0.009228856331890791, 0.00945957774018806, 0.00969606718369276, 0.009938468863285078, 0.010186930584867204, 0.010441603849488884, 0.010702643945726105, 0.010970210044369256, 0.011244465295478486, 0.011525576927865449, 0.011813716351062082, 0.012109059259838635, 0.0124117857413346, 0.012722080384867962, 0.01304013239448966, 0.013366135704351901, 0.013700289096960697, 0.014042796324384713, 0.014393866232494331, 0.014753712888306687, 0.015122555710514353, 0.01550061960327721, 0.01588813509335914, 0.016285338470693116, 0.016692471932460443, 0.017109783730771953, 0.017537528324041252, 0.01797596653214228, 0.018425365695445835, 0.018885999837831978, 0.019358149833777778, 0.019842103579622218, 0.020338156169112772, 0.02084661007334059, 0.021367775325174105, 0.021901969708303454, 0.02244951895101104, 0.023010756924786314, 0.023586025847905968, 0.024175676494103616, 0.0247800684064562, 0.025399570116617608, 0.026034559369533045, 0.02668542335377137, 0.02735255893761565, 0.02803637291105604, 0.02873728223383244, 0.029455714289678244, 0.0301921071469202, 0.030946909825593203, 0.03172058257123303, 0.03251359713551385, 0.0333264370639017, 0.03415959799049924, 0.035013587940261715, 0.035888927638768255, 0.03678615082973745, 0.03770580460048089, 0.03864844971549291, 0.039614660958380225, 0.04060502748233973, 0.04162015316939822, 0.042660656998633174, 0.043727173423599, 0.04482035275918896, 0.045940861578168686, 0.0470893831176229, 0.04826661769556347, 0.04947328313795255, 0.05071011521640136, 0.051977868096811396, 0.053277314799231666, 0.054609247669212456, 0.05597447886094276, 0.05737384083246633, 0.05880818685327798, 0.060278391524609926, 0.06178535131272517, 0.0633299850955433, 0.06491323472293187, 0.06653606559100515, 0.06819946723078028, 0.06990445391154979, 0.07165206525933851, 0.07344336689082198, 0.07527945106309253, 0.07716143733966983, 0.07909047327316157, 0.0810677351049906, 0.08309442848261536, 0.08517178919468073, 0.08730108392454775, 0.08948361102266142, 0.09172070129822796, 0.09401371883068364, 0.09636406180145074, 0.09877316334648699, 0.10124249243014916, 0.10377355474090287, 0.10636789360942545, 0.10902709094966107, 0.11175276822340259, 0.11454658742898764, 0.11741025211471233, 0.12034550841758011, 0.1233541461280196, 0.12643799978122008, 0.1295989497757506, 0.13283892352014434, 0.13615989660814792], lambda_start=0.001, path_multiplier=1.025, backtrack=False, device=None, verbose=1, fit_lambda_=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FastCPH</label><div class=\"sk-toggleable__content\"><pre>FastCPH(rng_seed=2401, layer_sizes=[4, 4], tie_approximation=&#x27;breslow&#x27;, lambda_seq=[0.001, 0.0010249999999999999, 0.001050625, 0.0010768906249999997, 0.0011038128906249996, 0.0011314082128906244, 0.00115969341821289, 0.0011886857536682123, 0.0012184028975099172, 0.0012488629699476653, 0.0012800845441963567, 0.0013120866578012656, 0.0013448888242462971, 0.0013785110448524542, 0.0014129738209737656, 0.0014482981664981096, 0.0014845056206605623, 0.001521618261177076, 0.001559658717706503, 0.0015986501856491654, 0.0016386164402903943, 0.0016795818512976542, 0.0017215713975800952, 0.0017646106825195975, 0.0018087259495825872, 0.0018539440983221518, 0.0019002927007802054, 0.0019478000182997105, 0.001996495018757203, 0.002046407394226133, 0.002097567579081786, 0.0021500067685588306, 0.002203756937772801, 0.0022588508612171212, 0.0023153221327475485, 0.0023732051860662373, 0.002432535315717893, 0.00249334869861084, 0.002555682416076111, 0.0026195744764780132, 0.0026850638383899635, 0.0027521904343497123, 0.0028209951952084546, 0.002891520075088666, 0.0029638080769658825, 0.0030379032788900293, 0.0031138508608622795, 0.0031916971323838364, 0.003271489560693432, 0.0033532767997107677, 0.0034371087197035363, 0.003523036437696125, 0.0036111123486385274, 0.0037013901573544903, 0.0037939249112883522, 0.003888773034070561, 0.003985992359922324, 0.004085642168920382, 0.004187783223143391, 0.0042924778037219755, 0.004399789748815024, 0.0045097844925354, 0.004622529104848784, 0.004738092332470004, 0.004856544640781753, 0.004977958256801297, 0.005102407213221329, 0.0052299673935518615, 0.005360716578390657, 0.005494734492850423, 0.005632102855171683, 0.005772905426550975, 0.005917228062214749, 0.006065158763770117, 0.0062167877328643695, 0.006372207426185978, 0.006531512611840627, 0.006694800427136643, 0.006862170437815058, 0.007033724698760434, 0.007209567816229444, 0.00738980701163518, 0.007574552186926058, 0.007763915991599209, 0.007958013891389188, 0.008156964238673917, 0.008360888344640766, 0.008569910553256782, 0.008784158317088201, 0.009003762275015407, 0.009228856331890791, 0.00945957774018806, 0.00969606718369276, 0.009938468863285078, 0.010186930584867204, 0.010441603849488884, 0.010702643945726105, 0.010970210044369256, 0.011244465295478486, 0.011525576927865449, 0.011813716351062082, 0.012109059259838635, 0.0124117857413346, 0.012722080384867962, 0.01304013239448966, 0.013366135704351901, 0.013700289096960697, 0.014042796324384713, 0.014393866232494331, 0.014753712888306687, 0.015122555710514353, 0.01550061960327721, 0.01588813509335914, 0.016285338470693116, 0.016692471932460443, 0.017109783730771953, 0.017537528324041252, 0.01797596653214228, 0.018425365695445835, 0.018885999837831978, 0.019358149833777778, 0.019842103579622218, 0.020338156169112772, 0.02084661007334059, 0.021367775325174105, 0.021901969708303454, 0.02244951895101104, 0.023010756924786314, 0.023586025847905968, 0.024175676494103616, 0.0247800684064562, 0.025399570116617608, 0.026034559369533045, 0.02668542335377137, 0.02735255893761565, 0.02803637291105604, 0.02873728223383244, 0.029455714289678244, 0.0301921071469202, 0.030946909825593203, 0.03172058257123303, 0.03251359713551385, 0.0333264370639017, 0.03415959799049924, 0.035013587940261715, 0.035888927638768255, 0.03678615082973745, 0.03770580460048089, 0.03864844971549291, 0.039614660958380225, 0.04060502748233973, 0.04162015316939822, 0.042660656998633174, 0.043727173423599, 0.04482035275918896, 0.045940861578168686, 0.0470893831176229, 0.04826661769556347, 0.04947328313795255, 0.05071011521640136, 0.051977868096811396, 0.053277314799231666, 0.054609247669212456, 0.05597447886094276, 0.05737384083246633, 0.05880818685327798, 0.060278391524609926, 0.06178535131272517, 0.0633299850955433, 0.06491323472293187, 0.06653606559100515, 0.06819946723078028, 0.06990445391154979, 0.07165206525933851, 0.07344336689082198, 0.07527945106309253, 0.07716143733966983, 0.07909047327316157, 0.0810677351049906, 0.08309442848261536, 0.08517178919468073, 0.08730108392454775, 0.08948361102266142, 0.09172070129822796, 0.09401371883068364, 0.09636406180145074, 0.09877316334648699, 0.10124249243014916, 0.10377355474090287, 0.10636789360942545, 0.10902709094966107, 0.11175276822340259, 0.11454658742898764, 0.11741025211471233, 0.12034550841758011, 0.1233541461280196, 0.12643799978122008, 0.1295989497757506, 0.13283892352014434, 0.13615989660814792], lambda_start=0.001, path_multiplier=1.025, backtrack=False, device=None, verbose=1, fit_lambda_=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FastCPH(rng_seed=2401, layer_sizes=[4, 4], tie_approximation='breslow', lambda_seq=[0.001, 0.0010249999999999999, 0.001050625, 0.0010768906249999997, 0.0011038128906249996, 0.0011314082128906244, 0.00115969341821289, 0.0011886857536682123, 0.0012184028975099172, 0.0012488629699476653, 0.0012800845441963567, 0.0013120866578012656, 0.0013448888242462971, 0.0013785110448524542, 0.0014129738209737656, 0.0014482981664981096, 0.0014845056206605623, 0.001521618261177076, 0.001559658717706503, 0.0015986501856491654, 0.0016386164402903943, 0.0016795818512976542, 0.0017215713975800952, 0.0017646106825195975, 0.0018087259495825872, 0.0018539440983221518, 0.0019002927007802054, 0.0019478000182997105, 0.001996495018757203, 0.002046407394226133, 0.002097567579081786, 0.0021500067685588306, 0.002203756937772801, 0.0022588508612171212, 0.0023153221327475485, 0.0023732051860662373, 0.002432535315717893, 0.00249334869861084, 0.002555682416076111, 0.0026195744764780132, 0.0026850638383899635, 0.0027521904343497123, 0.0028209951952084546, 0.002891520075088666, 0.0029638080769658825, 0.0030379032788900293, 0.0031138508608622795, 0.0031916971323838364, 0.003271489560693432, 0.0033532767997107677, 0.0034371087197035363, 0.003523036437696125, 0.0036111123486385274, 0.0037013901573544903, 0.0037939249112883522, 0.003888773034070561, 0.003985992359922324, 0.004085642168920382, 0.004187783223143391, 0.0042924778037219755, 0.004399789748815024, 0.0045097844925354, 0.004622529104848784, 0.004738092332470004, 0.004856544640781753, 0.004977958256801297, 0.005102407213221329, 0.0052299673935518615, 0.005360716578390657, 0.005494734492850423, 0.005632102855171683, 0.005772905426550975, 0.005917228062214749, 0.006065158763770117, 0.0062167877328643695, 0.006372207426185978, 0.006531512611840627, 0.006694800427136643, 0.006862170437815058, 0.007033724698760434, 0.007209567816229444, 0.00738980701163518, 0.007574552186926058, 0.007763915991599209, 0.007958013891389188, 0.008156964238673917, 0.008360888344640766, 0.008569910553256782, 0.008784158317088201, 0.009003762275015407, 0.009228856331890791, 0.00945957774018806, 0.00969606718369276, 0.009938468863285078, 0.010186930584867204, 0.010441603849488884, 0.010702643945726105, 0.010970210044369256, 0.011244465295478486, 0.011525576927865449, 0.011813716351062082, 0.012109059259838635, 0.0124117857413346, 0.012722080384867962, 0.01304013239448966, 0.013366135704351901, 0.013700289096960697, 0.014042796324384713, 0.014393866232494331, 0.014753712888306687, 0.015122555710514353, 0.01550061960327721, 0.01588813509335914, 0.016285338470693116, 0.016692471932460443, 0.017109783730771953, 0.017537528324041252, 0.01797596653214228, 0.018425365695445835, 0.018885999837831978, 0.019358149833777778, 0.019842103579622218, 0.020338156169112772, 0.02084661007334059, 0.021367775325174105, 0.021901969708303454, 0.02244951895101104, 0.023010756924786314, 0.023586025847905968, 0.024175676494103616, 0.0247800684064562, 0.025399570116617608, 0.026034559369533045, 0.02668542335377137, 0.02735255893761565, 0.02803637291105604, 0.02873728223383244, 0.029455714289678244, 0.0301921071469202, 0.030946909825593203, 0.03172058257123303, 0.03251359713551385, 0.0333264370639017, 0.03415959799049924, 0.035013587940261715, 0.035888927638768255, 0.03678615082973745, 0.03770580460048089, 0.03864844971549291, 0.039614660958380225, 0.04060502748233973, 0.04162015316939822, 0.042660656998633174, 0.043727173423599, 0.04482035275918896, 0.045940861578168686, 0.0470893831176229, 0.04826661769556347, 0.04947328313795255, 0.05071011521640136, 0.051977868096811396, 0.053277314799231666, 0.054609247669212456, 0.05597447886094276, 0.05737384083246633, 0.05880818685327798, 0.060278391524609926, 0.06178535131272517, 0.0633299850955433, 0.06491323472293187, 0.06653606559100515, 0.06819946723078028, 0.06990445391154979, 0.07165206525933851, 0.07344336689082198, 0.07527945106309253, 0.07716143733966983, 0.07909047327316157, 0.0810677351049906, 0.08309442848261536, 0.08517178919468073, 0.08730108392454775, 0.08948361102266142, 0.09172070129822796, 0.09401371883068364, 0.09636406180145074, 0.09877316334648699, 0.10124249243014916, 0.10377355474090287, 0.10636789360942545, 0.10902709094966107, 0.11175276822340259, 0.11454658742898764, 0.11741025211471233, 0.12034550841758011, 0.1233541461280196, 0.12643799978122008, 0.1295989497757506, 0.13283892352014434, 0.13615989660814792], lambda_start=0.001, path_multiplier=1.025, backtrack=False, device=None, verbose=1, fit_lambda_=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_fastcph=survhive.FastCPH(rng_seed=2401, layer_sizes=[4,4], verbose=1)\n",
    "w_fastcph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2d8acbf-9e96-4445-9b2e-d56b4a2c4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "loss: 4.988934516906738\n",
      "epoch: 1\n",
      "loss: 4.967841148376465\n",
      "epoch: 2\n",
      "loss: 4.947465896606445\n",
      "epoch: 3\n",
      "loss: 4.927479267120361\n",
      "epoch: 4\n",
      "loss: 4.907822132110596\n",
      "epoch: 5\n",
      "loss: 4.888700485229492\n",
      "epoch: 6\n",
      "loss: 4.87002420425415\n",
      "epoch: 7\n",
      "loss: 4.851817607879639\n",
      "epoch: 8\n",
      "loss: 4.834465980529785\n",
      "epoch: 9\n",
      "loss: 4.817636013031006\n",
      "epoch: 10\n",
      "loss: 4.801114082336426\n",
      "epoch: 11\n",
      "loss: 4.784931182861328\n",
      "epoch: 12\n",
      "loss: 4.769046306610107\n",
      "epoch: 13\n",
      "loss: 4.7536845207214355\n",
      "epoch: 14\n",
      "loss: 4.739511489868164\n",
      "epoch: 15\n",
      "loss: 4.725949287414551\n",
      "epoch: 16\n",
      "loss: 4.712684154510498\n",
      "epoch: 17\n",
      "loss: 4.699960231781006\n",
      "epoch: 18\n",
      "loss: 4.687402725219727\n",
      "epoch: 19\n",
      "loss: 4.674999237060547\n",
      "epoch: 20\n",
      "loss: 4.662703990936279\n",
      "epoch: 21\n",
      "loss: 4.650566577911377\n",
      "epoch: 22\n",
      "loss: 4.63859224319458\n",
      "epoch: 23\n",
      "loss: 4.626996040344238\n",
      "epoch: 24\n",
      "loss: 4.6154656410217285\n",
      "epoch: 25\n",
      "loss: 4.603992938995361\n",
      "epoch: 26\n",
      "loss: 4.592909336090088\n",
      "epoch: 27\n",
      "loss: 4.582374572753906\n",
      "epoch: 28\n",
      "loss: 4.572081089019775\n",
      "epoch: 29\n",
      "loss: 4.561834812164307\n",
      "epoch: 30\n",
      "loss: 4.551592826843262\n",
      "epoch: 31\n",
      "loss: 4.541404724121094\n",
      "epoch: 32\n",
      "loss: 4.531553268432617\n",
      "epoch: 33\n",
      "loss: 4.521735191345215\n",
      "epoch: 34\n",
      "loss: 4.511999607086182\n",
      "epoch: 35\n",
      "loss: 4.502277851104736\n",
      "epoch: 36\n",
      "loss: 4.4925432205200195\n",
      "epoch: 37\n",
      "loss: 4.482823848724365\n",
      "epoch: 38\n",
      "loss: 4.473175048828125\n",
      "epoch: 39\n",
      "loss: 4.463521957397461\n",
      "epoch: 40\n",
      "loss: 4.453856945037842\n",
      "epoch: 41\n",
      "loss: 4.444219589233398\n",
      "epoch: 42\n",
      "loss: 4.434562683105469\n",
      "epoch: 43\n",
      "loss: 4.424951076507568\n",
      "epoch: 44\n",
      "loss: 4.415400981903076\n",
      "epoch: 45\n",
      "loss: 4.405874729156494\n",
      "epoch: 46\n",
      "loss: 4.396328926086426\n",
      "epoch: 47\n",
      "loss: 4.3868231773376465\n",
      "epoch: 48\n",
      "loss: 4.377947807312012\n",
      "epoch: 49\n",
      "loss: 4.369158744812012\n",
      "epoch: 50\n",
      "loss: 4.360405445098877\n",
      "epoch: 51\n",
      "loss: 4.351581573486328\n",
      "epoch: 52\n",
      "loss: 4.34265661239624\n",
      "epoch: 53\n",
      "loss: 4.333661079406738\n",
      "epoch: 54\n",
      "loss: 4.324814319610596\n",
      "epoch: 55\n",
      "loss: 4.315921783447266\n",
      "epoch: 56\n",
      "loss: 4.306935787200928\n",
      "epoch: 57\n",
      "loss: 4.298212051391602\n",
      "epoch: 58\n",
      "loss: 4.289447784423828\n",
      "epoch: 59\n",
      "loss: 4.280625820159912\n",
      "epoch: 60\n",
      "loss: 4.271814823150635\n",
      "epoch: 61\n",
      "loss: 4.262975692749023\n",
      "epoch: 62\n",
      "loss: 4.254064559936523\n",
      "epoch: 63\n",
      "loss: 4.24526834487915\n",
      "epoch: 64\n",
      "loss: 4.236437797546387\n",
      "epoch: 65\n",
      "loss: 4.227598667144775\n",
      "epoch: 66\n",
      "loss: 4.2188401222229\n",
      "epoch: 67\n",
      "loss: 4.210164546966553\n",
      "epoch: 68\n",
      "loss: 4.201572895050049\n",
      "epoch: 69\n",
      "loss: 4.193121433258057\n",
      "epoch: 70\n",
      "loss: 4.184811592102051\n",
      "epoch: 71\n",
      "loss: 4.176508903503418\n",
      "epoch: 72\n",
      "loss: 4.16841459274292\n",
      "epoch: 73\n",
      "loss: 4.160294532775879\n",
      "epoch: 74\n",
      "loss: 4.152143955230713\n",
      "epoch: 75\n",
      "loss: 4.1439385414123535\n",
      "epoch: 76\n",
      "loss: 4.13572883605957\n",
      "epoch: 77\n",
      "loss: 4.127482891082764\n",
      "epoch: 78\n",
      "loss: 4.119148254394531\n",
      "epoch: 79\n",
      "loss: 4.1107916831970215\n",
      "epoch: 80\n",
      "loss: 4.102336406707764\n",
      "epoch: 81\n",
      "loss: 4.093864917755127\n",
      "epoch: 82\n",
      "loss: 4.08538818359375\n",
      "epoch: 83\n",
      "loss: 4.076940536499023\n",
      "epoch: 84\n",
      "loss: 4.068380832672119\n",
      "epoch: 85\n",
      "loss: 4.059727191925049\n",
      "epoch: 86\n",
      "loss: 4.051146507263184\n",
      "epoch: 87\n",
      "loss: 4.042520523071289\n",
      "epoch: 88\n",
      "loss: 4.033790588378906\n",
      "epoch: 89\n",
      "loss: 4.025070667266846\n",
      "epoch: 90\n",
      "loss: 4.016283988952637\n",
      "epoch: 91\n",
      "loss: 4.007647514343262\n",
      "epoch: 92\n",
      "loss: 3.999089002609253\n",
      "epoch: 93\n",
      "loss: 3.9905004501342773\n",
      "epoch: 94\n",
      "loss: 3.9820384979248047\n",
      "epoch: 95\n",
      "loss: 3.9735870361328125\n",
      "epoch: 96\n",
      "loss: 3.9651145935058594\n",
      "epoch: 97\n",
      "loss: 3.9565930366516113\n",
      "epoch: 98\n",
      "loss: 3.948040008544922\n",
      "epoch: 99\n",
      "loss: 3.9393601417541504\n",
      "epoch: 100\n",
      "loss: 3.9306163787841797\n",
      "epoch: 101\n",
      "loss: 3.921835422515869\n",
      "epoch: 102\n",
      "loss: 3.9130401611328125\n",
      "epoch: 103\n",
      "loss: 3.904177188873291\n",
      "epoch: 104\n",
      "loss: 3.8952765464782715\n",
      "epoch: 105\n",
      "loss: 3.886417865753174\n",
      "epoch: 106\n",
      "loss: 3.8775453567504883\n",
      "epoch: 107\n",
      "loss: 3.8687868118286133\n",
      "epoch: 108\n",
      "loss: 3.860081195831299\n",
      "epoch: 109\n",
      "loss: 3.851370334625244\n",
      "epoch: 110\n",
      "loss: 3.8426694869995117\n",
      "epoch: 111\n",
      "loss: 3.8338236808776855\n",
      "epoch: 112\n",
      "loss: 3.824930191040039\n",
      "epoch: 113\n",
      "loss: 3.816019058227539\n",
      "epoch: 114\n",
      "loss: 3.807051420211792\n",
      "epoch: 115\n",
      "loss: 3.797914981842041\n",
      "epoch: 116\n",
      "loss: 3.788727283477783\n",
      "epoch: 117\n",
      "loss: 3.7795140743255615\n",
      "epoch: 118\n",
      "loss: 3.7702369689941406\n",
      "epoch: 119\n",
      "loss: 3.760946273803711\n",
      "epoch: 120\n",
      "loss: 3.7517004013061523\n",
      "epoch: 121\n",
      "loss: 3.742366313934326\n",
      "epoch: 122\n",
      "loss: 3.7330174446105957\n",
      "epoch: 123\n",
      "loss: 3.72371244430542\n",
      "epoch: 124\n",
      "loss: 3.7144622802734375\n",
      "epoch: 125\n",
      "loss: 3.7051851749420166\n",
      "epoch: 126\n",
      "loss: 3.6958250999450684\n",
      "epoch: 127\n",
      "loss: 3.686276435852051\n",
      "epoch: 128\n",
      "loss: 3.6766700744628906\n",
      "epoch: 129\n",
      "loss: 3.667147636413574\n",
      "epoch: 130\n",
      "loss: 3.657541036605835\n",
      "epoch: 131\n",
      "loss: 3.6478896141052246\n",
      "epoch: 132\n",
      "loss: 3.638343334197998\n",
      "epoch: 133\n",
      "loss: 3.6287453174591064\n",
      "epoch: 134\n",
      "loss: 3.6191139221191406\n",
      "epoch: 135\n",
      "loss: 3.6094305515289307\n",
      "epoch: 136\n",
      "loss: 3.599790096282959\n",
      "epoch: 137\n",
      "loss: 3.590130567550659\n",
      "epoch: 138\n",
      "loss: 3.5804896354675293\n",
      "epoch: 139\n",
      "loss: 3.5708446502685547\n",
      "epoch: 140\n",
      "loss: 3.5611655712127686\n",
      "epoch: 141\n",
      "loss: 3.5515079498291016\n",
      "epoch: 142\n",
      "loss: 3.541903495788574\n",
      "epoch: 143\n",
      "loss: 3.5322604179382324\n",
      "epoch: 144\n",
      "loss: 3.522669792175293\n",
      "epoch: 145\n",
      "loss: 3.5131452083587646\n",
      "epoch: 146\n",
      "loss: 3.5035510063171387\n",
      "epoch: 147\n",
      "loss: 3.493898391723633\n",
      "epoch: 148\n",
      "loss: 3.4842519760131836\n",
      "epoch: 149\n",
      "loss: 3.474600315093994\n",
      "epoch: 150\n",
      "loss: 3.46490216255188\n",
      "epoch: 151\n",
      "loss: 3.4552783966064453\n",
      "epoch: 152\n",
      "loss: 3.4457154273986816\n",
      "epoch: 153\n",
      "loss: 3.4361889362335205\n",
      "epoch: 154\n",
      "loss: 3.426685333251953\n",
      "epoch: 155\n",
      "loss: 3.417232036590576\n",
      "epoch: 156\n",
      "loss: 3.40775728225708\n",
      "epoch: 157\n",
      "loss: 3.3982481956481934\n",
      "epoch: 158\n",
      "loss: 3.3888020515441895\n",
      "epoch: 159\n",
      "loss: 3.3793084621429443\n",
      "epoch: 160\n",
      "loss: 3.3698501586914062\n",
      "epoch: 161\n",
      "loss: 3.360459327697754\n",
      "epoch: 162\n",
      "loss: 3.351083755493164\n",
      "epoch: 163\n",
      "loss: 3.3417367935180664\n",
      "epoch: 164\n",
      "loss: 3.3323802947998047\n",
      "epoch: 165\n",
      "loss: 3.323075532913208\n",
      "epoch: 166\n",
      "loss: 3.313724994659424\n",
      "epoch: 167\n",
      "loss: 3.304378032684326\n",
      "epoch: 168\n",
      "loss: 3.295151710510254\n",
      "epoch: 169\n",
      "loss: 3.2858762741088867\n",
      "epoch: 170\n",
      "loss: 3.2766265869140625\n",
      "epoch: 171\n",
      "loss: 3.267427444458008\n",
      "epoch: 172\n",
      "loss: 3.2583298683166504\n",
      "epoch: 173\n",
      "loss: 3.2491979598999023\n",
      "epoch: 174\n",
      "loss: 3.240079879760742\n",
      "epoch: 175\n",
      "loss: 3.2310824394226074\n",
      "epoch: 176\n",
      "loss: 3.222071647644043\n",
      "epoch: 177\n",
      "loss: 3.2131409645080566\n",
      "epoch: 178\n",
      "loss: 3.204191207885742\n",
      "epoch: 179\n",
      "loss: 3.195140838623047\n",
      "epoch: 180\n",
      "loss: 3.1862857341766357\n",
      "epoch: 181\n",
      "loss: 3.1774439811706543\n",
      "epoch: 182\n",
      "loss: 3.1685714721679688\n",
      "epoch: 183\n",
      "loss: 3.1598048210144043\n",
      "epoch: 184\n",
      "loss: 3.1509482860565186\n",
      "epoch: 185\n",
      "loss: 3.1421642303466797\n",
      "epoch: 186\n",
      "loss: 3.1334190368652344\n",
      "epoch: 187\n",
      "loss: 3.124660015106201\n",
      "epoch: 188\n",
      "loss: 3.1158862113952637\n",
      "epoch: 189\n",
      "loss: 3.10725474357605\n",
      "epoch: 190\n",
      "loss: 3.098590850830078\n",
      "epoch: 191\n",
      "loss: 3.089961051940918\n",
      "epoch: 192\n",
      "loss: 3.081357955932617\n",
      "epoch: 193\n",
      "loss: 3.0727977752685547\n",
      "epoch: 194\n",
      "loss: 3.0643749237060547\n",
      "epoch: 195\n",
      "loss: 3.055972099304199\n",
      "epoch: 196\n",
      "loss: 3.0475387573242188\n",
      "epoch: 197\n",
      "loss: 3.0390665531158447\n",
      "epoch: 198\n",
      "loss: 3.0306906700134277\n",
      "epoch: 199\n",
      "loss: 3.022366523742676\n",
      "epoch: 200\n",
      "loss: 3.0140163898468018\n",
      "epoch: 201\n",
      "loss: 3.0056710243225098\n",
      "epoch: 202\n",
      "loss: 2.9972801208496094\n",
      "epoch: 203\n",
      "loss: 2.9890294075012207\n",
      "epoch: 204\n",
      "loss: 2.9808359146118164\n",
      "epoch: 205\n",
      "loss: 2.9726500511169434\n",
      "epoch: 206\n",
      "loss: 2.9644603729248047\n",
      "epoch: 207\n",
      "loss: 2.9563519954681396\n",
      "epoch: 208\n",
      "loss: 2.9482030868530273\n",
      "epoch: 209\n",
      "loss: 2.940098762512207\n",
      "epoch: 210\n",
      "loss: 2.932131767272949\n",
      "epoch: 211\n",
      "loss: 2.9241466522216797\n",
      "epoch: 212\n",
      "loss: 2.916231155395508\n",
      "epoch: 213\n",
      "loss: 2.9082188606262207\n",
      "epoch: 214\n",
      "loss: 2.900259017944336\n",
      "epoch: 215\n",
      "loss: 2.8924076557159424\n",
      "epoch: 216\n",
      "loss: 2.8845951557159424\n",
      "epoch: 217\n",
      "loss: 2.87686824798584\n",
      "epoch: 218\n",
      "loss: 2.869185447692871\n",
      "epoch: 219\n",
      "loss: 2.8614158630371094\n",
      "epoch: 220\n",
      "loss: 2.8537864685058594\n",
      "epoch: 221\n",
      "loss: 2.8461456298828125\n",
      "epoch: 222\n",
      "loss: 2.838460922241211\n",
      "epoch: 223\n",
      "loss: 2.830886125564575\n",
      "epoch: 224\n",
      "loss: 2.8232779502868652\n",
      "epoch: 225\n",
      "loss: 2.8157777786254883\n",
      "epoch: 226\n",
      "loss: 2.8083081245422363\n",
      "epoch: 227\n",
      "loss: 2.8007826805114746\n",
      "epoch: 228\n",
      "loss: 2.7932820320129395\n",
      "epoch: 229\n",
      "loss: 2.785834789276123\n",
      "epoch: 230\n",
      "loss: 2.778458595275879\n",
      "epoch: 231\n",
      "loss: 2.771091938018799\n",
      "epoch: 232\n",
      "loss: 2.7636990547180176\n",
      "epoch: 233\n",
      "loss: 2.7562897205352783\n",
      "epoch: 234\n",
      "loss: 2.748936176300049\n",
      "epoch: 235\n",
      "loss: 2.7417006492614746\n",
      "epoch: 236\n",
      "loss: 2.7343878746032715\n",
      "epoch: 237\n",
      "loss: 2.7271244525909424\n",
      "epoch: 238\n",
      "loss: 2.7199411392211914\n",
      "epoch: 239\n",
      "loss: 2.7128119468688965\n",
      "epoch: 240\n",
      "loss: 2.70572566986084\n",
      "epoch: 241\n",
      "loss: 2.698552131652832\n",
      "epoch: 242\n",
      "loss: 2.6915180683135986\n",
      "epoch: 243\n",
      "loss: 2.6843996047973633\n",
      "epoch: 244\n",
      "loss: 2.67722225189209\n",
      "epoch: 245\n",
      "loss: 2.67020320892334\n",
      "epoch: 246\n",
      "loss: 2.6631550788879395\n",
      "epoch: 247\n",
      "loss: 2.656073570251465\n",
      "epoch: 248\n",
      "loss: 2.649190902709961\n",
      "epoch: 249\n",
      "loss: 2.642274856567383\n",
      "epoch: 250\n",
      "loss: 2.635193347930908\n",
      "epoch: 251\n",
      "loss: 2.6282286643981934\n",
      "epoch: 252\n",
      "loss: 2.6212964057922363\n",
      "epoch: 253\n",
      "loss: 2.614471435546875\n",
      "epoch: 254\n",
      "loss: 2.607536792755127\n",
      "epoch: 255\n",
      "loss: 2.6008195877075195\n",
      "epoch: 256\n",
      "loss: 2.594238758087158\n",
      "epoch: 257\n",
      "loss: 2.587440013885498\n",
      "epoch: 258\n",
      "loss: 2.5808629989624023\n",
      "epoch: 259\n",
      "loss: 2.5743136405944824\n",
      "epoch: 260\n",
      "loss: 2.5676798820495605\n",
      "epoch: 261\n",
      "loss: 2.561101198196411\n",
      "epoch: 262\n",
      "loss: 2.5546114444732666\n",
      "epoch: 263\n",
      "loss: 2.5480785369873047\n",
      "epoch: 264\n",
      "loss: 2.541461944580078\n",
      "epoch: 265\n",
      "loss: 2.5347952842712402\n",
      "epoch: 266\n",
      "loss: 2.528163433074951\n",
      "epoch: 267\n",
      "loss: 2.5217156410217285\n",
      "epoch: 268\n",
      "loss: 2.515397548675537\n",
      "epoch: 269\n",
      "loss: 2.508890151977539\n",
      "epoch: 270\n",
      "loss: 2.5023984909057617\n",
      "epoch: 271\n",
      "loss: 2.496077299118042\n",
      "epoch: 272\n",
      "loss: 2.4896864891052246\n",
      "epoch: 273\n",
      "loss: 2.483276128768921\n",
      "epoch: 274\n",
      "loss: 2.476959466934204\n",
      "epoch: 275\n",
      "loss: 2.4707107543945312\n",
      "epoch: 276\n",
      "loss: 2.4644062519073486\n",
      "epoch: 277\n",
      "loss: 2.458026885986328\n",
      "epoch: 278\n",
      "loss: 2.4517483711242676\n",
      "epoch: 279\n",
      "loss: 2.445518970489502\n",
      "epoch: 280\n",
      "loss: 2.43922758102417\n",
      "epoch: 281\n",
      "loss: 2.4328737258911133\n",
      "epoch: 282\n",
      "loss: 2.426638603210449\n",
      "epoch: 283\n",
      "loss: 2.4203476905822754\n",
      "epoch: 284\n",
      "loss: 2.4142003059387207\n",
      "epoch: 285\n",
      "loss: 2.4079604148864746\n",
      "epoch: 286\n",
      "loss: 2.4017462730407715\n",
      "epoch: 287\n",
      "loss: 2.3955211639404297\n",
      "epoch: 288\n",
      "loss: 2.3892858028411865\n",
      "epoch: 289\n",
      "loss: 2.383136749267578\n",
      "epoch: 290\n",
      "loss: 2.3769893646240234\n",
      "epoch: 291\n",
      "loss: 2.370790958404541\n",
      "epoch: 292\n",
      "loss: 2.364656448364258\n",
      "epoch: 293\n",
      "loss: 2.358604669570923\n",
      "epoch: 294\n",
      "loss: 2.352447509765625\n",
      "epoch: 295\n",
      "loss: 2.346367359161377\n",
      "epoch: 296\n",
      "loss: 2.3403658866882324\n",
      "epoch: 297\n",
      "loss: 2.3344454765319824\n",
      "epoch: 298\n",
      "loss: 2.3283157348632812\n",
      "epoch: 299\n",
      "loss: 2.3223934173583984\n",
      "epoch: 300\n",
      "loss: 2.3165283203125\n",
      "epoch: 301\n",
      "loss: 2.310682535171509\n",
      "epoch: 302\n",
      "loss: 2.3047094345092773\n",
      "epoch: 303\n",
      "loss: 2.2987136840820312\n",
      "epoch: 304\n",
      "loss: 2.292628288269043\n",
      "epoch: 305\n",
      "loss: 2.2865171432495117\n",
      "epoch: 306\n",
      "loss: 2.2804760932922363\n",
      "epoch: 307\n",
      "loss: 2.274468183517456\n",
      "epoch: 308\n",
      "loss: 2.2684218883514404\n",
      "epoch: 309\n",
      "loss: 2.2623648643493652\n",
      "epoch: 310\n",
      "loss: 2.2563376426696777\n",
      "epoch: 311\n",
      "loss: 2.2502870559692383\n",
      "epoch: 312\n",
      "loss: 2.244302749633789\n",
      "epoch: 313\n",
      "loss: 2.2383289337158203\n",
      "epoch: 314\n",
      "loss: 2.2323708534240723\n",
      "epoch: 315\n",
      "loss: 2.226215362548828\n",
      "epoch: 316\n",
      "loss: 2.2199549674987793\n",
      "epoch: 317\n",
      "loss: 2.2136456966400146\n",
      "epoch: 318\n",
      "loss: 2.2073984146118164\n",
      "epoch: 319\n",
      "loss: 2.2012462615966797\n",
      "epoch: 320\n",
      "loss: 2.195129871368408\n",
      "epoch: 321\n",
      "loss: 2.188971996307373\n",
      "epoch: 322\n",
      "loss: 2.1828651428222656\n",
      "epoch: 323\n",
      "loss: 2.176884174346924\n",
      "epoch: 324\n",
      "loss: 2.1708269119262695\n",
      "epoch: 325\n",
      "loss: 2.1648683547973633\n",
      "epoch: 326\n",
      "loss: 2.158963203430176\n",
      "epoch: 327\n",
      "loss: 2.1530699729919434\n",
      "epoch: 328\n",
      "loss: 2.1472039222717285\n",
      "epoch: 329\n",
      "loss: 2.141352415084839\n",
      "epoch: 330\n",
      "loss: 2.135606288909912\n",
      "epoch: 331\n",
      "loss: 2.1298630237579346\n",
      "epoch: 332\n",
      "loss: 2.1241776943206787\n",
      "epoch: 333\n",
      "loss: 2.1187644004821777\n",
      "epoch: 334\n",
      "loss: 2.113149642944336\n",
      "epoch: 335\n",
      "loss: 2.107612133026123\n",
      "epoch: 336\n",
      "loss: 2.102224826812744\n",
      "epoch: 337\n",
      "loss: 2.096778392791748\n",
      "epoch: 338\n",
      "loss: 2.0913896560668945\n",
      "epoch: 339\n",
      "loss: 2.0860252380371094\n",
      "epoch: 340\n",
      "loss: 2.0807230472564697\n",
      "epoch: 341\n",
      "loss: 2.0754950046539307\n",
      "epoch: 342\n",
      "loss: 2.070272445678711\n",
      "epoch: 343\n",
      "loss: 2.0651471614837646\n",
      "epoch: 344\n",
      "loss: 2.059925079345703\n",
      "epoch: 345\n",
      "loss: 2.054807662963867\n",
      "epoch: 346\n",
      "loss: 2.049715280532837\n",
      "epoch: 347\n",
      "loss: 2.0447211265563965\n",
      "epoch: 348\n",
      "loss: 2.0396957397460938\n",
      "epoch: 349\n",
      "loss: 2.0346903800964355\n",
      "epoch: 350\n",
      "loss: 2.0298283100128174\n",
      "epoch: 351\n",
      "loss: 2.025017738342285\n",
      "epoch: 352\n",
      "loss: 2.0202536582946777\n",
      "epoch: 353\n",
      "loss: 2.01547908782959\n",
      "epoch: 354\n",
      "loss: 2.0108015537261963\n",
      "epoch: 355\n",
      "loss: 2.0061042308807373\n",
      "epoch: 356\n",
      "loss: 2.001467704772949\n",
      "epoch: 357\n",
      "loss: 1.9967576265335083\n",
      "epoch: 358\n",
      "loss: 1.9921280145645142\n",
      "epoch: 359\n",
      "loss: 1.987501621246338\n",
      "epoch: 360\n",
      "loss: 1.982914924621582\n",
      "epoch: 361\n",
      "loss: 1.9783934354782104\n",
      "epoch: 362\n",
      "loss: 1.9739598035812378\n",
      "epoch: 363\n",
      "loss: 1.9694219827651978\n",
      "epoch: 364\n",
      "loss: 1.9649814367294312\n",
      "epoch: 365\n",
      "loss: 1.960557460784912\n",
      "epoch: 366\n",
      "loss: 1.9562263488769531\n",
      "epoch: 367\n",
      "loss: 1.9517669677734375\n",
      "epoch: 368\n",
      "loss: 1.9473892450332642\n",
      "epoch: 369\n",
      "loss: 1.943099021911621\n",
      "epoch: 370\n",
      "loss: 1.9386954307556152\n",
      "epoch: 371\n",
      "loss: 1.934486746788025\n",
      "epoch: 372\n",
      "loss: 1.9301459789276123\n",
      "epoch: 373\n",
      "loss: 1.925908088684082\n",
      "epoch: 374\n",
      "loss: 1.9216779470443726\n",
      "epoch: 375\n",
      "loss: 1.917370080947876\n",
      "epoch: 376\n",
      "loss: 1.9131534099578857\n",
      "epoch: 377\n",
      "loss: 1.9089967012405396\n",
      "epoch: 378\n",
      "loss: 1.9047828912734985\n",
      "epoch: 379\n",
      "loss: 1.9006255865097046\n",
      "epoch: 380\n",
      "loss: 1.8965380191802979\n",
      "epoch: 381\n",
      "loss: 1.892377495765686\n",
      "epoch: 382\n",
      "loss: 1.888290286064148\n",
      "epoch: 383\n",
      "loss: 1.884205937385559\n",
      "epoch: 384\n",
      "loss: 1.880152940750122\n",
      "epoch: 385\n",
      "loss: 1.876111388206482\n",
      "epoch: 386\n",
      "loss: 1.8721586465835571\n",
      "epoch: 387\n",
      "loss: 1.8681398630142212\n",
      "epoch: 388\n",
      "loss: 1.8642511367797852\n",
      "epoch: 389\n",
      "loss: 1.8602770566940308\n",
      "epoch: 390\n",
      "loss: 1.8563642501831055\n",
      "epoch: 391\n",
      "loss: 1.8524292707443237\n",
      "epoch: 392\n",
      "loss: 1.8486101627349854\n",
      "epoch: 393\n",
      "loss: 1.8447473049163818\n",
      "epoch: 394\n",
      "loss: 1.8410402536392212\n",
      "epoch: 395\n",
      "loss: 1.8373125791549683\n",
      "epoch: 396\n",
      "loss: 1.8334444761276245\n",
      "epoch: 397\n",
      "loss: 1.8297064304351807\n",
      "epoch: 398\n",
      "loss: 1.8259738683700562\n",
      "epoch: 399\n",
      "loss: 1.822263240814209\n",
      "epoch: 400\n",
      "loss: 1.818565845489502\n",
      "epoch: 401\n",
      "loss: 1.8148709535598755\n",
      "epoch: 402\n",
      "loss: 1.811156988143921\n",
      "epoch: 403\n",
      "loss: 1.8075469732284546\n",
      "epoch: 404\n",
      "loss: 1.8038846254348755\n",
      "epoch: 405\n",
      "loss: 1.8002631664276123\n",
      "epoch: 406\n",
      "loss: 1.7966605424880981\n",
      "epoch: 407\n",
      "loss: 1.7930799722671509\n",
      "epoch: 408\n",
      "loss: 1.78947913646698\n",
      "epoch: 409\n",
      "loss: 1.7858787775039673\n",
      "epoch: 410\n",
      "loss: 1.782348394393921\n",
      "epoch: 411\n",
      "loss: 1.7787423133850098\n",
      "epoch: 412\n",
      "loss: 1.7753543853759766\n",
      "epoch: 413\n",
      "loss: 1.7716972827911377\n",
      "epoch: 414\n",
      "loss: 1.7682383060455322\n",
      "epoch: 415\n",
      "loss: 1.7647441625595093\n",
      "epoch: 416\n",
      "loss: 1.7612720727920532\n",
      "epoch: 417\n",
      "loss: 1.7578436136245728\n",
      "epoch: 418\n",
      "loss: 1.7544001340866089\n",
      "epoch: 419\n",
      "loss: 1.7509855031967163\n",
      "epoch: 420\n",
      "loss: 1.7476181983947754\n",
      "epoch: 421\n",
      "loss: 1.744202733039856\n",
      "epoch: 422\n",
      "loss: 1.7408220767974854\n",
      "epoch: 423\n",
      "loss: 1.7375497817993164\n",
      "epoch: 424\n",
      "loss: 1.7341076135635376\n",
      "epoch: 425\n",
      "loss: 1.730759620666504\n",
      "epoch: 426\n",
      "loss: 1.7274144887924194\n",
      "epoch: 427\n",
      "loss: 1.724078893661499\n",
      "epoch: 428\n",
      "loss: 1.7207728624343872\n",
      "epoch: 429\n",
      "loss: 1.7176798582077026\n",
      "epoch: 430\n",
      "loss: 1.7142274379730225\n",
      "epoch: 431\n",
      "loss: 1.711021900177002\n",
      "epoch: 432\n",
      "loss: 1.7078124284744263\n",
      "epoch: 433\n",
      "loss: 1.7045992612838745\n",
      "epoch: 434\n",
      "loss: 1.7013944387435913\n",
      "epoch: 435\n",
      "loss: 1.698378562927246\n",
      "epoch: 436\n",
      "loss: 1.6951802968978882\n",
      "epoch: 437\n",
      "loss: 1.6920158863067627\n",
      "epoch: 438\n",
      "loss: 1.6889305114746094\n",
      "epoch: 439\n",
      "loss: 1.685818076133728\n",
      "epoch: 440\n",
      "loss: 1.6827276945114136\n",
      "epoch: 441\n",
      "loss: 1.679634928703308\n",
      "epoch: 442\n",
      "loss: 1.6765577793121338\n",
      "epoch: 443\n",
      "loss: 1.673597812652588\n",
      "epoch: 444\n",
      "loss: 1.6704363822937012\n",
      "epoch: 445\n",
      "loss: 1.6674120426177979\n",
      "epoch: 446\n",
      "loss: 1.6643880605697632\n",
      "epoch: 447\n",
      "loss: 1.661474585533142\n",
      "epoch: 448\n",
      "loss: 1.6584217548370361\n",
      "epoch: 449\n",
      "loss: 1.6554489135742188\n",
      "epoch: 450\n",
      "loss: 1.6524680852890015\n",
      "epoch: 451\n",
      "loss: 1.6494829654693604\n",
      "epoch: 452\n",
      "loss: 1.646594762802124\n",
      "epoch: 453\n",
      "loss: 1.6435595750808716\n",
      "epoch: 454\n",
      "loss: 1.6406230926513672\n",
      "epoch: 455\n",
      "loss: 1.6376993656158447\n",
      "epoch: 456\n",
      "loss: 1.6347739696502686\n",
      "epoch: 457\n",
      "loss: 1.6318588256835938\n",
      "epoch: 458\n",
      "loss: 1.6290544271469116\n",
      "epoch: 459\n",
      "loss: 1.626177191734314\n",
      "epoch: 460\n",
      "loss: 1.6233474016189575\n",
      "epoch: 461\n",
      "loss: 1.6205806732177734\n",
      "epoch: 462\n",
      "loss: 1.617817997932434\n",
      "epoch: 463\n",
      "loss: 1.6151396036148071\n",
      "epoch: 464\n",
      "loss: 1.6123859882354736\n",
      "epoch: 465\n",
      "loss: 1.6095778942108154\n",
      "epoch: 466\n",
      "loss: 1.6068189144134521\n",
      "epoch: 467\n",
      "loss: 1.6040799617767334\n",
      "epoch: 468\n",
      "loss: 1.6013954877853394\n",
      "epoch: 469\n",
      "loss: 1.5986883640289307\n",
      "epoch: 470\n",
      "loss: 1.5959877967834473\n",
      "epoch: 471\n",
      "loss: 1.5932893753051758\n",
      "epoch: 472\n",
      "loss: 1.5906513929367065\n",
      "epoch: 473\n",
      "loss: 1.5879437923431396\n",
      "epoch: 474\n",
      "loss: 1.585312008857727\n",
      "epoch: 475\n",
      "loss: 1.5826444625854492\n",
      "epoch: 476\n",
      "loss: 1.5800811052322388\n",
      "epoch: 477\n",
      "loss: 1.5774198770523071\n",
      "epoch: 478\n",
      "loss: 1.574828863143921\n",
      "epoch: 479\n",
      "loss: 1.5722373723983765\n",
      "epoch: 480\n",
      "loss: 1.5696380138397217\n",
      "epoch: 481\n",
      "loss: 1.5670337677001953\n",
      "epoch: 482\n",
      "loss: 1.5644383430480957\n",
      "epoch: 483\n",
      "loss: 1.561916708946228\n",
      "epoch: 484\n",
      "loss: 1.5593068599700928\n",
      "epoch: 485\n",
      "loss: 1.5567572116851807\n",
      "epoch: 486\n",
      "loss: 1.5542067289352417\n",
      "epoch: 487\n",
      "loss: 1.5516581535339355\n",
      "epoch: 488\n",
      "loss: 1.5492336750030518\n",
      "epoch: 489\n",
      "loss: 1.5466138124465942\n",
      "epoch: 490\n",
      "loss: 1.544147253036499\n",
      "epoch: 491\n",
      "loss: 1.5416535139083862\n",
      "epoch: 492\n",
      "loss: 1.5392423868179321\n",
      "epoch: 493\n",
      "loss: 1.5367438793182373\n",
      "epoch: 494\n",
      "loss: 1.5343855619430542\n",
      "epoch: 495\n",
      "loss: 1.5318492650985718\n",
      "epoch: 496\n",
      "loss: 1.529429316520691\n",
      "epoch: 497\n",
      "loss: 1.5269925594329834\n",
      "epoch: 498\n",
      "loss: 1.5245577096939087\n",
      "epoch: 499\n",
      "loss: 1.5221800804138184\n",
      "epoch: 500\n",
      "loss: 1.519778847694397\n",
      "epoch: 501\n",
      "loss: 1.517362117767334\n",
      "epoch: 502\n",
      "loss: 1.5150001049041748\n",
      "epoch: 503\n",
      "loss: 1.5126538276672363\n",
      "epoch: 504\n",
      "loss: 1.5102930068969727\n",
      "epoch: 505\n",
      "loss: 1.5079578161239624\n",
      "epoch: 506\n",
      "loss: 1.5056178569793701\n",
      "epoch: 507\n",
      "loss: 1.503321886062622\n",
      "epoch: 508\n",
      "loss: 1.5009647607803345\n",
      "epoch: 509\n",
      "loss: 1.4986549615859985\n",
      "epoch: 510\n",
      "loss: 1.4963470697402954\n",
      "epoch: 511\n",
      "loss: 1.4940557479858398\n",
      "epoch: 512\n",
      "loss: 1.4917593002319336\n",
      "epoch: 513\n",
      "loss: 1.489460825920105\n",
      "epoch: 514\n",
      "loss: 1.4872416257858276\n",
      "epoch: 515\n",
      "loss: 1.4849236011505127\n",
      "epoch: 516\n",
      "loss: 1.4826934337615967\n",
      "epoch: 517\n",
      "loss: 1.480594515800476\n",
      "epoch: 518\n",
      "loss: 1.4782582521438599\n",
      "epoch: 519\n",
      "loss: 1.4760890007019043\n",
      "epoch: 520\n",
      "loss: 1.4738495349884033\n",
      "epoch: 521\n",
      "loss: 1.4716596603393555\n",
      "epoch: 522\n",
      "loss: 1.4694736003875732\n",
      "epoch: 523\n",
      "loss: 1.4673417806625366\n",
      "epoch: 524\n",
      "loss: 1.4651172161102295\n",
      "epoch: 525\n",
      "loss: 1.4629478454589844\n",
      "epoch: 526\n",
      "loss: 1.4607819318771362\n",
      "epoch: 527\n",
      "loss: 1.4586875438690186\n",
      "epoch: 528\n",
      "loss: 1.456486463546753\n",
      "epoch: 529\n",
      "loss: 1.454375982284546\n",
      "epoch: 530\n",
      "loss: 1.4522526264190674\n",
      "epoch: 531\n",
      "loss: 1.450237512588501\n",
      "epoch: 532\n",
      "loss: 1.448075294494629\n",
      "epoch: 533\n",
      "loss: 1.4460411071777344\n",
      "epoch: 534\n",
      "loss: 1.4439404010772705\n",
      "epoch: 535\n",
      "loss: 1.4418866634368896\n",
      "epoch: 536\n",
      "loss: 1.4398715496063232\n",
      "epoch: 537\n",
      "loss: 1.437788486480713\n",
      "epoch: 538\n",
      "loss: 1.435746431350708\n",
      "epoch: 539\n",
      "loss: 1.4337100982666016\n",
      "epoch: 540\n",
      "loss: 1.431670904159546\n",
      "epoch: 541\n",
      "loss: 1.4296386241912842\n",
      "epoch: 542\n",
      "loss: 1.427605390548706\n",
      "epoch: 543\n",
      "loss: 1.4255962371826172\n",
      "epoch: 544\n",
      "loss: 1.423555850982666\n",
      "epoch: 545\n",
      "loss: 1.421668529510498\n",
      "epoch: 546\n",
      "loss: 1.4196370840072632\n",
      "epoch: 547\n",
      "loss: 1.4177117347717285\n",
      "epoch: 548\n",
      "loss: 1.4157631397247314\n",
      "epoch: 549\n",
      "loss: 1.4138245582580566\n",
      "epoch: 550\n",
      "loss: 1.4118765592575073\n",
      "epoch: 551\n",
      "loss: 1.4099268913269043\n",
      "epoch: 552\n",
      "loss: 1.4080110788345337\n",
      "epoch: 553\n",
      "loss: 1.406067132949829\n",
      "epoch: 554\n",
      "loss: 1.4041409492492676\n",
      "epoch: 555\n",
      "loss: 1.4022111892700195\n",
      "epoch: 556\n",
      "loss: 1.400285005569458\n",
      "epoch: 557\n",
      "loss: 1.3984050750732422\n",
      "epoch: 558\n",
      "loss: 1.39646577835083\n",
      "epoch: 559\n",
      "loss: 1.3945679664611816\n",
      "epoch: 560\n",
      "loss: 1.392681360244751\n",
      "epoch: 561\n",
      "loss: 1.390825867652893\n",
      "epoch: 562\n",
      "loss: 1.388946533203125\n",
      "epoch: 563\n",
      "loss: 1.3871253728866577\n",
      "epoch: 564\n",
      "loss: 1.3852636814117432\n",
      "epoch: 565\n",
      "loss: 1.3834446668624878\n",
      "epoch: 566\n",
      "loss: 1.3816299438476562\n",
      "epoch: 567\n",
      "loss: 1.3798253536224365\n",
      "epoch: 568\n",
      "loss: 1.3780102729797363\n",
      "epoch: 569\n",
      "loss: 1.3761827945709229\n",
      "epoch: 570\n",
      "loss: 1.3743788003921509\n",
      "epoch: 571\n",
      "loss: 1.3725725412368774\n",
      "epoch: 572\n",
      "loss: 1.3707587718963623\n",
      "epoch: 573\n",
      "loss: 1.36893892288208\n",
      "epoch: 574\n",
      "loss: 1.3672153949737549\n",
      "epoch: 575\n",
      "loss: 1.3653860092163086\n",
      "epoch: 576\n",
      "loss: 1.3636116981506348\n",
      "epoch: 577\n",
      "loss: 1.3618977069854736\n",
      "epoch: 578\n",
      "loss: 1.3601267337799072\n",
      "epoch: 579\n",
      "loss: 1.3584126234054565\n",
      "epoch: 580\n",
      "loss: 1.3566784858703613\n",
      "epoch: 581\n",
      "loss: 1.3549500703811646\n",
      "epoch: 582\n",
      "loss: 1.3532285690307617\n",
      "epoch: 583\n",
      "loss: 1.3514913320541382\n",
      "epoch: 584\n",
      "loss: 1.3497600555419922\n",
      "epoch: 585\n",
      "loss: 1.3480371236801147\n",
      "epoch: 586\n",
      "loss: 1.3463144302368164\n",
      "epoch: 587\n",
      "loss: 1.344598412513733\n",
      "epoch: 588\n",
      "loss: 1.342879056930542\n",
      "epoch: 589\n",
      "loss: 1.341162919998169\n",
      "epoch: 590\n",
      "loss: 1.3394492864608765\n",
      "epoch: 591\n",
      "loss: 1.3377419710159302\n",
      "epoch: 592\n",
      "loss: 1.3360695838928223\n",
      "epoch: 593\n",
      "loss: 1.3343346118927002\n",
      "epoch: 594\n",
      "loss: 1.3326406478881836\n",
      "epoch: 595\n",
      "loss: 1.3310047388076782\n",
      "epoch: 596\n",
      "loss: 1.3293051719665527\n",
      "epoch: 597\n",
      "loss: 1.3276993036270142\n",
      "epoch: 598\n",
      "loss: 1.3260002136230469\n",
      "epoch: 599\n",
      "loss: 1.3243803977966309\n",
      "epoch: 600\n",
      "loss: 1.322756290435791\n",
      "epoch: 601\n",
      "loss: 1.3211305141448975\n",
      "epoch: 602\n",
      "loss: 1.3195009231567383\n",
      "epoch: 603\n",
      "loss: 1.317866563796997\n",
      "epoch: 604\n",
      "loss: 1.3162293434143066\n",
      "epoch: 605\n",
      "loss: 1.3146092891693115\n",
      "epoch: 606\n",
      "loss: 1.3129799365997314\n",
      "epoch: 607\n",
      "loss: 1.311342477798462\n",
      "epoch: 608\n",
      "loss: 1.309718132019043\n",
      "epoch: 609\n",
      "loss: 1.3081104755401611\n",
      "epoch: 610\n",
      "loss: 1.3065119981765747\n",
      "epoch: 611\n",
      "loss: 1.3049038648605347\n",
      "epoch: 612\n",
      "loss: 1.303291916847229\n",
      "epoch: 613\n",
      "loss: 1.3017349243164062\n",
      "epoch: 614\n",
      "loss: 1.3001117706298828\n",
      "epoch: 615\n",
      "loss: 1.2985954284667969\n",
      "epoch: 616\n",
      "loss: 1.2969820499420166\n",
      "epoch: 617\n",
      "loss: 1.2954027652740479\n",
      "epoch: 618\n",
      "loss: 1.2938393354415894\n",
      "epoch: 619\n",
      "loss: 1.2922687530517578\n",
      "epoch: 620\n",
      "loss: 1.2907025814056396\n",
      "epoch: 621\n",
      "loss: 1.2891950607299805\n",
      "epoch: 622\n",
      "loss: 1.2876088619232178\n",
      "epoch: 623\n",
      "loss: 1.2860615253448486\n",
      "epoch: 624\n",
      "loss: 1.2845447063446045\n",
      "epoch: 625\n",
      "loss: 1.2830193042755127\n",
      "epoch: 626\n",
      "loss: 1.2814829349517822\n",
      "epoch: 627\n",
      "loss: 1.2799338102340698\n",
      "epoch: 628\n",
      "loss: 1.278397560119629\n",
      "epoch: 629\n",
      "loss: 1.2768408060073853\n",
      "epoch: 630\n",
      "loss: 1.2753454446792603\n",
      "epoch: 631\n",
      "loss: 1.2739243507385254\n",
      "epoch: 632\n",
      "loss: 1.2723052501678467\n",
      "epoch: 633\n",
      "loss: 1.2708218097686768\n",
      "epoch: 634\n",
      "loss: 1.2693384885787964\n",
      "epoch: 635\n",
      "loss: 1.2678390741348267\n",
      "epoch: 636\n",
      "loss: 1.266352891921997\n",
      "epoch: 637\n",
      "loss: 1.2648603916168213\n",
      "epoch: 638\n",
      "loss: 1.2633781433105469\n",
      "epoch: 639\n",
      "loss: 1.2618916034698486\n",
      "epoch: 640\n",
      "loss: 1.2604014873504639\n",
      "epoch: 641\n",
      "loss: 1.2589361667633057\n",
      "epoch: 642\n",
      "loss: 1.2574390172958374\n",
      "epoch: 643\n",
      "loss: 1.2559566497802734\n",
      "epoch: 644\n",
      "loss: 1.2544901371002197\n",
      "epoch: 645\n",
      "loss: 1.253066062927246\n",
      "epoch: 646\n",
      "loss: 1.2515535354614258\n",
      "epoch: 647\n",
      "loss: 1.2500890493392944\n",
      "epoch: 648\n",
      "loss: 1.2486238479614258\n",
      "epoch: 649\n",
      "loss: 1.2471706867218018\n",
      "epoch: 650\n",
      "loss: 1.2457058429718018\n",
      "epoch: 651\n",
      "loss: 1.2442491054534912\n",
      "epoch: 652\n",
      "loss: 1.2428005933761597\n",
      "epoch: 653\n",
      "loss: 1.2413663864135742\n",
      "epoch: 654\n",
      "loss: 1.2399513721466064\n",
      "epoch: 655\n",
      "loss: 1.2384803295135498\n",
      "epoch: 656\n",
      "loss: 1.237051248550415\n",
      "epoch: 657\n",
      "loss: 1.2356693744659424\n",
      "epoch: 658\n",
      "loss: 1.2342369556427002\n",
      "epoch: 659\n",
      "loss: 1.2328053712844849\n",
      "epoch: 660\n",
      "loss: 1.2314081192016602\n",
      "epoch: 661\n",
      "loss: 1.230006456375122\n",
      "epoch: 662\n",
      "loss: 1.2286005020141602\n",
      "epoch: 663\n",
      "loss: 1.2271993160247803\n",
      "epoch: 664\n",
      "loss: 1.2257936000823975\n",
      "epoch: 665\n",
      "loss: 1.2244024276733398\n",
      "epoch: 666\n",
      "loss: 1.2229971885681152\n",
      "epoch: 667\n",
      "loss: 1.221604585647583\n",
      "epoch: 668\n",
      "loss: 1.2202104330062866\n",
      "epoch: 669\n",
      "loss: 1.2188160419464111\n",
      "epoch: 670\n",
      "loss: 1.2174240350723267\n",
      "epoch: 671\n",
      "loss: 1.2160353660583496\n",
      "epoch: 672\n",
      "loss: 1.2146879434585571\n",
      "epoch: 673\n",
      "loss: 1.213310956954956\n",
      "epoch: 674\n",
      "loss: 1.2119085788726807\n",
      "epoch: 675\n",
      "loss: 1.210540771484375\n",
      "epoch: 676\n",
      "loss: 1.2092264890670776\n",
      "epoch: 677\n",
      "loss: 1.207801103591919\n",
      "epoch: 678\n",
      "loss: 1.2064564228057861\n",
      "epoch: 679\n",
      "loss: 1.2051000595092773\n",
      "epoch: 680\n",
      "loss: 1.2037580013275146\n",
      "epoch: 681\n",
      "loss: 1.2024481296539307\n",
      "epoch: 682\n",
      "loss: 1.2010917663574219\n",
      "epoch: 683\n",
      "loss: 1.1997491121292114\n",
      "epoch: 684\n",
      "loss: 1.1984105110168457\n",
      "epoch: 685\n",
      "loss: 1.1970751285552979\n",
      "epoch: 686\n",
      "loss: 1.1957426071166992\n",
      "epoch: 687\n",
      "loss: 1.1944084167480469\n",
      "epoch: 688\n",
      "loss: 1.193086862564087\n",
      "epoch: 689\n",
      "loss: 1.1917716264724731\n",
      "epoch: 690\n",
      "loss: 1.1904513835906982\n",
      "epoch: 691\n",
      "loss: 1.1891236305236816\n",
      "epoch: 692\n",
      "loss: 1.1877954006195068\n",
      "epoch: 693\n",
      "loss: 1.186469316482544\n",
      "epoch: 694\n",
      "loss: 1.185166358947754\n",
      "epoch: 695\n",
      "loss: 1.183844804763794\n",
      "epoch: 696\n",
      "loss: 1.1825286149978638\n",
      "epoch: 697\n",
      "loss: 1.1812286376953125\n",
      "epoch: 698\n",
      "loss: 1.179927945137024\n",
      "epoch: 699\n",
      "loss: 1.178626537322998\n",
      "epoch: 700\n",
      "loss: 1.1773607730865479\n",
      "epoch: 701\n",
      "loss: 1.1760995388031006\n",
      "epoch: 702\n",
      "loss: 1.17477285861969\n",
      "epoch: 703\n",
      "loss: 1.1735634803771973\n",
      "epoch: 704\n",
      "loss: 1.172255039215088\n",
      "epoch: 705\n",
      "loss: 1.1709831953048706\n",
      "epoch: 706\n",
      "loss: 1.1697150468826294\n",
      "epoch: 707\n",
      "loss: 1.1684482097625732\n",
      "epoch: 708\n",
      "loss: 1.1671772003173828\n",
      "epoch: 709\n",
      "loss: 1.1659066677093506\n",
      "epoch: 710\n",
      "loss: 1.1646366119384766\n",
      "epoch: 711\n",
      "loss: 1.1633827686309814\n",
      "epoch: 712\n",
      "loss: 1.1621111631393433\n",
      "epoch: 713\n",
      "loss: 1.1608459949493408\n",
      "epoch: 714\n",
      "loss: 1.1595847606658936\n",
      "epoch: 715\n",
      "loss: 1.1583287715911865\n",
      "epoch: 716\n",
      "loss: 1.1570708751678467\n",
      "epoch: 717\n",
      "loss: 1.155835747718811\n",
      "epoch: 718\n",
      "loss: 1.1545770168304443\n",
      "epoch: 719\n",
      "loss: 1.1533349752426147\n",
      "epoch: 720\n",
      "loss: 1.1520891189575195\n",
      "epoch: 721\n",
      "loss: 1.1508369445800781\n",
      "epoch: 722\n",
      "loss: 1.1495826244354248\n",
      "epoch: 723\n",
      "loss: 1.1483755111694336\n",
      "epoch: 724\n",
      "loss: 1.1471089124679565\n",
      "epoch: 725\n",
      "loss: 1.145880937576294\n",
      "epoch: 726\n",
      "loss: 1.1446449756622314\n",
      "epoch: 727\n",
      "loss: 1.1434388160705566\n",
      "epoch: 728\n",
      "loss: 1.142221212387085\n",
      "epoch: 729\n",
      "loss: 1.140993595123291\n",
      "epoch: 730\n",
      "loss: 1.1397817134857178\n",
      "epoch: 731\n",
      "loss: 1.1385369300842285\n",
      "epoch: 732\n",
      "loss: 1.137366771697998\n",
      "epoch: 733\n",
      "loss: 1.1361273527145386\n",
      "epoch: 734\n",
      "loss: 1.1349220275878906\n",
      "epoch: 735\n",
      "loss: 1.1337305307388306\n",
      "epoch: 736\n",
      "loss: 1.1325316429138184\n",
      "epoch: 737\n",
      "loss: 1.1313343048095703\n",
      "epoch: 738\n",
      "loss: 1.1301345825195312\n",
      "epoch: 739\n",
      "loss: 1.1289472579956055\n",
      "epoch: 740\n",
      "loss: 1.1277413368225098\n",
      "epoch: 741\n",
      "loss: 1.1265604496002197\n",
      "epoch: 742\n",
      "loss: 1.1253700256347656\n",
      "epoch: 743\n",
      "loss: 1.1241796016693115\n",
      "epoch: 744\n",
      "loss: 1.1229935884475708\n",
      "epoch: 745\n",
      "loss: 1.1218167543411255\n",
      "epoch: 746\n",
      "loss: 1.120627999305725\n",
      "epoch: 747\n",
      "loss: 1.1194539070129395\n",
      "epoch: 748\n",
      "loss: 1.1182794570922852\n",
      "epoch: 749\n",
      "loss: 1.1171084642410278\n",
      "epoch: 750\n",
      "loss: 1.1159281730651855\n",
      "epoch: 751\n",
      "loss: 1.1148171424865723\n",
      "epoch: 752\n",
      "loss: 1.1136146783828735\n",
      "epoch: 753\n",
      "loss: 1.1125164031982422\n",
      "epoch: 754\n",
      "loss: 1.1113413572311401\n",
      "epoch: 755\n",
      "loss: 1.1102206707000732\n",
      "epoch: 756\n",
      "loss: 1.1090669631958008\n",
      "epoch: 757\n",
      "loss: 1.1079245805740356\n",
      "epoch: 758\n",
      "loss: 1.1067880392074585\n",
      "epoch: 759\n",
      "loss: 1.105642318725586\n",
      "epoch: 760\n",
      "loss: 1.1045241355895996\n",
      "epoch: 761\n",
      "loss: 1.1033624410629272\n",
      "epoch: 762\n",
      "loss: 1.1022320985794067\n",
      "epoch: 763\n",
      "loss: 1.1010987758636475\n",
      "epoch: 764\n",
      "loss: 1.0999616384506226\n",
      "epoch: 765\n",
      "loss: 1.0988268852233887\n",
      "epoch: 766\n",
      "loss: 1.097707748413086\n",
      "epoch: 767\n",
      "loss: 1.096573829650879\n",
      "epoch: 768\n",
      "loss: 1.0954830646514893\n",
      "epoch: 769\n",
      "loss: 1.0943336486816406\n",
      "epoch: 770\n",
      "loss: 1.0932340621948242\n",
      "epoch: 771\n",
      "loss: 1.0921317338943481\n",
      "epoch: 772\n",
      "loss: 1.0910265445709229\n",
      "epoch: 773\n",
      "loss: 1.0899176597595215\n",
      "epoch: 774\n",
      "loss: 1.0888290405273438\n",
      "epoch: 775\n",
      "loss: 1.0877310037612915\n",
      "epoch: 776\n",
      "loss: 1.0866360664367676\n",
      "epoch: 777\n",
      "loss: 1.0855356454849243\n",
      "epoch: 778\n",
      "loss: 1.084436058998108\n",
      "epoch: 779\n",
      "loss: 1.0833395719528198\n",
      "epoch: 780\n",
      "loss: 1.0822426080703735\n",
      "epoch: 781\n",
      "loss: 1.081148624420166\n",
      "epoch: 782\n",
      "loss: 1.080051302909851\n",
      "epoch: 783\n",
      "loss: 1.078967809677124\n",
      "epoch: 784\n",
      "loss: 1.0778858661651611\n",
      "epoch: 785\n",
      "loss: 1.0768181085586548\n",
      "epoch: 786\n",
      "loss: 1.075757622718811\n",
      "epoch: 787\n",
      "loss: 1.0747284889221191\n",
      "epoch: 788\n",
      "loss: 1.0736522674560547\n",
      "epoch: 789\n",
      "loss: 1.0725820064544678\n",
      "epoch: 790\n",
      "loss: 1.071530818939209\n",
      "epoch: 791\n",
      "loss: 1.070454716682434\n",
      "epoch: 792\n",
      "loss: 1.0694085359573364\n",
      "epoch: 793\n",
      "loss: 1.0683746337890625\n",
      "epoch: 794\n",
      "loss: 1.0673052072525024\n",
      "epoch: 795\n",
      "loss: 1.0662678480148315\n",
      "epoch: 796\n",
      "loss: 1.0652306079864502\n",
      "epoch: 797\n",
      "loss: 1.0641825199127197\n",
      "epoch: 798\n",
      "loss: 1.0631288290023804\n",
      "epoch: 799\n",
      "loss: 1.0620821714401245\n",
      "epoch: 800\n",
      "loss: 1.0610382556915283\n",
      "epoch: 801\n",
      "loss: 1.059991717338562\n",
      "epoch: 802\n",
      "loss: 1.0589690208435059\n",
      "epoch: 803\n",
      "loss: 1.057908535003662\n",
      "epoch: 804\n",
      "loss: 1.0568760633468628\n",
      "epoch: 805\n",
      "loss: 1.0558409690856934\n",
      "epoch: 806\n",
      "loss: 1.0548075437545776\n",
      "epoch: 807\n",
      "loss: 1.0537738800048828\n",
      "epoch: 808\n",
      "loss: 1.052737832069397\n",
      "epoch: 809\n",
      "loss: 1.051700472831726\n",
      "epoch: 810\n",
      "loss: 1.0507044792175293\n",
      "epoch: 811\n",
      "loss: 1.049649715423584\n",
      "epoch: 812\n",
      "loss: 1.048656940460205\n",
      "epoch: 813\n",
      "loss: 1.0476175546646118\n",
      "epoch: 814\n",
      "loss: 1.0466110706329346\n",
      "epoch: 815\n",
      "loss: 1.0456037521362305\n",
      "epoch: 816\n",
      "loss: 1.0445891618728638\n",
      "epoch: 817\n",
      "loss: 1.0435733795166016\n",
      "epoch: 818\n",
      "loss: 1.0425626039505005\n",
      "epoch: 819\n",
      "loss: 1.04158616065979\n",
      "epoch: 820\n",
      "loss: 1.0405961275100708\n",
      "epoch: 821\n",
      "loss: 1.039597511291504\n",
      "epoch: 822\n",
      "loss: 1.0386016368865967\n",
      "epoch: 823\n",
      "loss: 1.0376133918762207\n",
      "epoch: 824\n",
      "loss: 1.0366361141204834\n",
      "epoch: 825\n",
      "loss: 1.0356525182724\n",
      "epoch: 826\n",
      "loss: 1.0346546173095703\n",
      "epoch: 827\n",
      "loss: 1.0336655378341675\n",
      "epoch: 828\n",
      "loss: 1.0326918363571167\n",
      "epoch: 829\n",
      "loss: 1.031693935394287\n",
      "epoch: 830\n",
      "loss: 1.0307159423828125\n",
      "epoch: 831\n",
      "loss: 1.029740810394287\n",
      "epoch: 832\n",
      "loss: 1.0287657976150513\n",
      "epoch: 833\n",
      "loss: 1.0277886390686035\n",
      "epoch: 834\n",
      "loss: 1.0268149375915527\n",
      "epoch: 835\n",
      "loss: 1.0258398056030273\n",
      "epoch: 836\n",
      "loss: 1.024869441986084\n",
      "epoch: 837\n",
      "loss: 1.0239156484603882\n",
      "epoch: 838\n",
      "loss: 1.0229370594024658\n",
      "epoch: 839\n",
      "loss: 1.021975040435791\n",
      "epoch: 840\n",
      "loss: 1.021011233329773\n",
      "epoch: 841\n",
      "loss: 1.0200462341308594\n",
      "epoch: 842\n",
      "loss: 1.0190823078155518\n",
      "epoch: 843\n",
      "loss: 1.0181256532669067\n",
      "epoch: 844\n",
      "loss: 1.0171648263931274\n",
      "epoch: 845\n",
      "loss: 1.0162168741226196\n",
      "epoch: 846\n",
      "loss: 1.0152866840362549\n",
      "epoch: 847\n",
      "loss: 1.0143284797668457\n",
      "epoch: 848\n",
      "loss: 1.0133787393569946\n",
      "epoch: 849\n",
      "loss: 1.0124480724334717\n",
      "epoch: 850\n",
      "loss: 1.0115771293640137\n",
      "epoch: 851\n",
      "loss: 1.0105911493301392\n",
      "epoch: 852\n",
      "loss: 1.0096964836120605\n",
      "epoch: 853\n",
      "loss: 1.008751630783081\n",
      "epoch: 854\n",
      "loss: 1.0078275203704834\n",
      "epoch: 855\n",
      "loss: 1.0069093704223633\n",
      "epoch: 856\n",
      "loss: 1.0059863328933716\n",
      "epoch: 857\n",
      "loss: 1.0050549507141113\n",
      "epoch: 858\n",
      "loss: 1.0041308403015137\n",
      "epoch: 859\n",
      "loss: 1.0032012462615967\n",
      "epoch: 860\n",
      "loss: 1.0022729635238647\n",
      "epoch: 861\n",
      "loss: 1.001347541809082\n",
      "epoch: 862\n",
      "loss: 1.0004240274429321\n",
      "epoch: 863\n",
      "loss: 0.9995036125183105\n",
      "epoch: 864\n",
      "loss: 0.9985835552215576\n",
      "epoch: 865\n",
      "loss: 0.9976831078529358\n",
      "epoch: 866\n",
      "loss: 0.9967585802078247\n",
      "epoch: 867\n",
      "loss: 0.9958524107933044\n",
      "epoch: 868\n",
      "loss: 0.9949342012405396\n",
      "epoch: 869\n",
      "loss: 0.9940299987792969\n",
      "epoch: 870\n",
      "loss: 0.99313884973526\n",
      "epoch: 871\n",
      "loss: 0.992218554019928\n",
      "epoch: 872\n",
      "loss: 0.9913199543952942\n",
      "epoch: 873\n",
      "loss: 0.9904193878173828\n",
      "epoch: 874\n",
      "loss: 0.9895340800285339\n",
      "epoch: 875\n",
      "loss: 0.9886300563812256\n",
      "epoch: 876\n",
      "loss: 0.987738847732544\n",
      "epoch: 877\n",
      "loss: 0.9868435859680176\n",
      "epoch: 878\n",
      "loss: 0.9859520196914673\n",
      "epoch: 879\n",
      "loss: 0.9850585460662842\n",
      "epoch: 880\n",
      "loss: 0.9841651916503906\n",
      "epoch: 881\n",
      "loss: 0.9832758903503418\n",
      "epoch: 882\n",
      "loss: 0.9823852181434631\n",
      "epoch: 883\n",
      "loss: 0.9814956188201904\n",
      "epoch: 884\n",
      "loss: 0.980617105960846\n",
      "epoch: 885\n",
      "loss: 0.9797599911689758\n",
      "epoch: 886\n",
      "loss: 0.9788838624954224\n",
      "epoch: 887\n",
      "loss: 0.9779981970787048\n",
      "epoch: 888\n",
      "loss: 0.9771195650100708\n",
      "epoch: 889\n",
      "loss: 0.976270318031311\n",
      "epoch: 890\n",
      "loss: 0.9754065275192261\n",
      "epoch: 891\n",
      "loss: 0.9745534658432007\n",
      "epoch: 892\n",
      "loss: 0.9736953377723694\n",
      "epoch: 893\n",
      "loss: 0.9728351831436157\n",
      "epoch: 894\n",
      "loss: 0.9719778299331665\n",
      "epoch: 895\n",
      "loss: 0.9711093902587891\n",
      "epoch: 896\n",
      "loss: 0.9702516794204712\n",
      "epoch: 897\n",
      "loss: 0.9693957567214966\n",
      "epoch: 898\n",
      "loss: 0.9685412049293518\n",
      "epoch: 899\n",
      "loss: 0.9676903486251831\n",
      "epoch: 900\n",
      "loss: 0.9668291807174683\n",
      "epoch: 901\n",
      "loss: 0.9659877419471741\n",
      "epoch: 902\n",
      "loss: 0.9651246070861816\n",
      "epoch: 903\n",
      "loss: 0.9642775654792786\n",
      "epoch: 904\n",
      "loss: 0.9634300470352173\n",
      "epoch: 905\n",
      "loss: 0.962600827217102\n",
      "epoch: 906\n",
      "loss: 0.9617379307746887\n",
      "epoch: 907\n",
      "loss: 0.9608997702598572\n",
      "epoch: 908\n",
      "loss: 0.9600511193275452\n",
      "epoch: 909\n",
      "loss: 0.9592090249061584\n",
      "epoch: 910\n",
      "loss: 0.9583665728569031\n",
      "epoch: 911\n",
      "loss: 0.9575217962265015\n",
      "epoch: 912\n",
      "loss: 0.9566940069198608\n",
      "epoch: 913\n",
      "loss: 0.955839216709137\n",
      "epoch: 914\n",
      "loss: 0.9550056457519531\n",
      "epoch: 915\n",
      "loss: 0.9542000889778137\n",
      "epoch: 916\n",
      "loss: 0.9533376693725586\n",
      "epoch: 917\n",
      "loss: 0.9525089263916016\n",
      "epoch: 918\n",
      "loss: 0.951711893081665\n",
      "epoch: 919\n",
      "loss: 0.9509438872337341\n",
      "epoch: 920\n",
      "loss: 0.9500561952590942\n",
      "epoch: 921\n",
      "loss: 0.9492571949958801\n",
      "epoch: 922\n",
      "loss: 0.9484500288963318\n",
      "epoch: 923\n",
      "loss: 0.9476372003555298\n",
      "epoch: 924\n",
      "loss: 0.9468294382095337\n",
      "epoch: 925\n",
      "loss: 0.9460136294364929\n",
      "epoch: 926\n",
      "loss: 0.9452009797096252\n",
      "epoch: 927\n",
      "loss: 0.9443873763084412\n",
      "epoch: 928\n",
      "loss: 0.9435750246047974\n",
      "epoch: 929\n",
      "loss: 0.9427609443664551\n",
      "epoch: 930\n",
      "loss: 0.9419471025466919\n",
      "epoch: 931\n",
      "loss: 0.9411345720291138\n",
      "epoch: 932\n",
      "loss: 0.940331220626831\n",
      "epoch: 933\n",
      "loss: 0.9395152926445007\n",
      "epoch: 934\n",
      "loss: 0.9387245178222656\n",
      "epoch: 935\n",
      "loss: 0.9379111528396606\n",
      "epoch: 936\n",
      "loss: 0.9371440410614014\n",
      "epoch: 937\n",
      "loss: 0.9363349080085754\n",
      "epoch: 938\n",
      "loss: 0.9355524778366089\n",
      "epoch: 939\n",
      "loss: 0.934766948223114\n",
      "epoch: 940\n",
      "loss: 0.9339829683303833\n",
      "epoch: 941\n",
      "loss: 0.9331903457641602\n",
      "epoch: 942\n",
      "loss: 0.9324021935462952\n",
      "epoch: 943\n",
      "loss: 0.931617259979248\n",
      "epoch: 944\n",
      "loss: 0.9308657646179199\n",
      "epoch: 945\n",
      "loss: 0.9300540089607239\n",
      "epoch: 946\n",
      "loss: 0.9292733073234558\n",
      "epoch: 947\n",
      "loss: 0.9285003542900085\n",
      "epoch: 948\n",
      "loss: 0.9277210235595703\n",
      "epoch: 949\n",
      "loss: 0.9269492626190186\n",
      "epoch: 950\n",
      "loss: 0.9261757731437683\n",
      "epoch: 951\n",
      "loss: 0.9254043102264404\n",
      "epoch: 952\n",
      "loss: 0.924627423286438\n",
      "epoch: 953\n",
      "loss: 0.9238545298576355\n",
      "epoch: 954\n",
      "loss: 0.9230928421020508\n",
      "epoch: 955\n",
      "loss: 0.9223171472549438\n",
      "epoch: 956\n",
      "loss: 0.9215525984764099\n",
      "epoch: 957\n",
      "loss: 0.9207862615585327\n",
      "epoch: 958\n",
      "loss: 0.9200242757797241\n",
      "epoch: 959\n",
      "loss: 0.9192560911178589\n",
      "epoch: 960\n",
      "loss: 0.9184927940368652\n",
      "epoch: 961\n",
      "loss: 0.9177279472351074\n",
      "epoch: 962\n",
      "loss: 0.9169958829879761\n",
      "epoch: 963\n",
      "loss: 0.9162241816520691\n",
      "epoch: 964\n",
      "loss: 0.9154784083366394\n",
      "epoch: 965\n",
      "loss: 0.9147247076034546\n",
      "epoch: 966\n",
      "loss: 0.9139753580093384\n",
      "epoch: 967\n",
      "loss: 0.9132345914840698\n",
      "epoch: 968\n",
      "loss: 0.9124776721000671\n",
      "epoch: 969\n",
      "loss: 0.9117318987846375\n",
      "epoch: 970\n",
      "loss: 0.9109998345375061\n",
      "epoch: 971\n",
      "loss: 0.9102510213851929\n",
      "epoch: 972\n",
      "loss: 0.9095035791397095\n",
      "epoch: 973\n",
      "loss: 0.9087646007537842\n",
      "epoch: 974\n",
      "loss: 0.9080447554588318\n",
      "epoch: 975\n",
      "loss: 0.9072892665863037\n",
      "epoch: 976\n",
      "loss: 0.906552791595459\n",
      "epoch: 977\n",
      "loss: 0.9058184027671814\n",
      "epoch: 978\n",
      "loss: 0.9050905108451843\n",
      "epoch: 979\n",
      "loss: 0.904346227645874\n",
      "epoch: 980\n",
      "loss: 0.903610110282898\n",
      "epoch: 981\n",
      "loss: 0.9028733372688293\n",
      "epoch: 982\n",
      "loss: 0.9021357893943787\n",
      "epoch: 983\n",
      "loss: 0.9014007449150085\n",
      "epoch: 984\n",
      "loss: 0.9006742238998413\n",
      "epoch: 985\n",
      "loss: 0.8999341726303101\n",
      "epoch: 986\n",
      "loss: 0.8992021679878235\n",
      "epoch: 987\n",
      "loss: 0.8984711170196533\n",
      "epoch: 988\n",
      "loss: 0.8977542519569397\n",
      "epoch: 989\n",
      "loss: 0.8970149755477905\n",
      "epoch: 990\n",
      "loss: 0.8962902426719666\n",
      "epoch: 991\n",
      "loss: 0.8955803513526917\n",
      "epoch: 992\n",
      "loss: 0.8948457837104797\n",
      "epoch: 993\n",
      "loss: 0.8941243886947632\n",
      "epoch: 994\n",
      "loss: 0.8934046626091003\n",
      "epoch: 995\n",
      "loss: 0.8926836848258972\n",
      "epoch: 996\n",
      "loss: 0.8919687271118164\n",
      "epoch: 997\n",
      "loss: 0.8912521600723267\n",
      "epoch: 998\n",
      "loss: 0.8905382752418518\n",
      "epoch: 999\n",
      "loss: 0.8898295760154724\n",
      "Initialized dense model\n",
      "1000 epochs, val_objective 8.89e-01, val_loss 8.89e-01, regularization 2.28e+01, l2_regularization 1.14e+01\n",
      "epoch: 0\n",
      "loss: 0.8892061710357666\n",
      "epoch: 1\n",
      "loss: 0.8891027569770813\n",
      "epoch: 2\n",
      "loss: 0.8889690637588501\n",
      "epoch: 3\n",
      "loss: 0.8888252377510071\n",
      "epoch: 4\n",
      "loss: 0.8886405229568481\n",
      "epoch: 5\n",
      "loss: 0.888418972492218\n",
      "epoch: 6\n",
      "loss: 0.8881669044494629\n",
      "epoch: 7\n",
      "loss: 0.8878849744796753\n",
      "epoch: 8\n",
      "loss: 0.8875777125358582\n",
      "epoch: 9\n",
      "loss: 0.8872489929199219\n",
      "Lambda = 1.00e-03, selected 84 features \n",
      "10 epochs, val_objective 9.10e-01, val_loss 8.87e-01, regularization 2.29e+01, l2_regularization 1.15e+01\n",
      "epoch: 0\n",
      "loss: 0.8869021534919739\n",
      "epoch: 1\n",
      "loss: 0.8865375518798828\n",
      "epoch: 2\n",
      "loss: 0.8861569762229919\n",
      "epoch: 3\n",
      "loss: 0.8857614398002625\n",
      "epoch: 4\n",
      "loss: 0.8853532671928406\n",
      "epoch: 5\n",
      "loss: 0.8849341869354248\n",
      "epoch: 6\n",
      "loss: 0.8845041990280151\n",
      "epoch: 7\n",
      "loss: 0.8840638995170593\n",
      "epoch: 8\n",
      "loss: 0.8836175203323364\n",
      "epoch: 9\n",
      "loss: 0.8831631541252136\n",
      "Lambda = 1.02e-03, selected 84 features \n",
      "10 epochs, val_objective 9.06e-01, val_loss 8.83e-01, regularization 2.29e+01, l2_regularization 1.15e+01\n",
      "epoch: 0\n",
      "loss: 0.8827021718025208\n",
      "epoch: 1\n",
      "loss: 0.8822352290153503\n",
      "epoch: 2\n",
      "loss: 0.8817635178565979\n",
      "epoch: 3\n",
      "loss: 0.881288468837738\n",
      "epoch: 4\n",
      "loss: 0.8808093667030334\n",
      "epoch: 5\n",
      "loss: 0.8803268074989319\n",
      "epoch: 6\n",
      "loss: 0.8798421025276184\n",
      "epoch: 7\n",
      "loss: 0.8793546557426453\n",
      "epoch: 8\n",
      "loss: 0.8788667321205139\n",
      "epoch: 9\n",
      "loss: 0.8783742189407349\n",
      "Lambda = 1.05e-03, selected 84 features \n",
      "10 epochs, val_objective 9.02e-01, val_loss 8.78e-01, regularization 2.30e+01, l2_regularization 1.15e+01\n",
      "epoch: 0\n",
      "loss: 0.8778842687606812\n",
      "epoch: 1\n",
      "loss: 0.8773900270462036\n",
      "epoch: 2\n",
      "loss: 0.8768954873085022\n",
      "epoch: 3\n",
      "loss: 0.8764011859893799\n",
      "epoch: 4\n",
      "loss: 0.8759060502052307\n",
      "epoch: 5\n",
      "loss: 0.8754099607467651\n",
      "epoch: 6\n",
      "loss: 0.8749139904975891\n",
      "epoch: 7\n",
      "loss: 0.8744179010391235\n",
      "epoch: 8\n",
      "loss: 0.8739219903945923\n",
      "epoch: 9\n",
      "loss: 0.8734254240989685\n",
      "Lambda = 1.08e-03, selected 84 features \n",
      "10 epochs, val_objective 8.98e-01, val_loss 8.73e-01, regularization 2.31e+01, l2_regularization 1.16e+01\n",
      "epoch: 0\n",
      "loss: 0.8729292750358582\n",
      "epoch: 1\n",
      "loss: 0.872433066368103\n",
      "epoch: 2\n",
      "loss: 0.8719366788864136\n",
      "epoch: 3\n",
      "loss: 0.8714412450790405\n",
      "epoch: 4\n",
      "loss: 0.8709455728530884\n",
      "epoch: 5\n",
      "loss: 0.8704499006271362\n",
      "epoch: 6\n",
      "loss: 0.8699572682380676\n",
      "epoch: 7\n",
      "loss: 0.8694615960121155\n",
      "epoch: 8\n",
      "loss: 0.868968665599823\n",
      "epoch: 9\n",
      "loss: 0.868476152420044\n",
      "Lambda = 1.10e-03, selected 84 features \n",
      "10 epochs, val_objective 8.94e-01, val_loss 8.68e-01, regularization 2.32e+01, l2_regularization 1.16e+01\n",
      "epoch: 0\n",
      "loss: 0.8679837584495544\n",
      "epoch: 1\n",
      "loss: 0.8674923777580261\n",
      "epoch: 2\n",
      "loss: 0.8670185208320618\n",
      "epoch: 3\n",
      "loss: 0.8665120601654053\n",
      "epoch: 4\n",
      "loss: 0.8660265207290649\n",
      "epoch: 5\n",
      "loss: 0.8655393123626709\n",
      "epoch: 6\n",
      "loss: 0.8650524616241455\n",
      "epoch: 7\n",
      "loss: 0.864566445350647\n",
      "epoch: 8\n",
      "loss: 0.8640807867050171\n",
      "epoch: 9\n",
      "loss: 0.8635964393615723\n",
      "Lambda = 1.13e-03, selected 84 features \n",
      "10 epochs, val_objective 8.90e-01, val_loss 8.63e-01, regularization 2.33e+01, l2_regularization 1.17e+01\n",
      "epoch: 0\n",
      "loss: 0.8631119132041931\n",
      "epoch: 1\n",
      "loss: 0.8626272082328796\n",
      "epoch: 2\n",
      "loss: 0.8621437549591064\n",
      "epoch: 3\n",
      "loss: 0.8616666793823242\n",
      "epoch: 4\n",
      "loss: 0.861180305480957\n",
      "epoch: 5\n",
      "loss: 0.8606998920440674\n",
      "epoch: 6\n",
      "loss: 0.8602200150489807\n",
      "epoch: 7\n",
      "loss: 0.8597403764724731\n",
      "epoch: 8\n",
      "loss: 0.8592677116394043\n",
      "epoch: 9\n",
      "loss: 0.8587843179702759\n",
      "Lambda = 1.16e-03, selected 84 features \n",
      "10 epochs, val_objective 8.85e-01, val_loss 8.58e-01, regularization 2.34e+01, l2_regularization 1.17e+01\n",
      "epoch: 0\n",
      "loss: 0.8583102822303772\n",
      "epoch: 1\n",
      "loss: 0.8578345775604248\n",
      "epoch: 2\n",
      "loss: 0.8573607802391052\n",
      "epoch: 3\n",
      "loss: 0.8568893074989319\n",
      "epoch: 4\n",
      "loss: 0.8564178347587585\n",
      "epoch: 5\n",
      "loss: 0.8559459447860718\n",
      "epoch: 6\n",
      "loss: 0.8554738163948059\n",
      "epoch: 7\n",
      "loss: 0.855006217956543\n",
      "epoch: 8\n",
      "loss: 0.8545351028442383\n",
      "epoch: 9\n",
      "loss: 0.8540644645690918\n",
      "Lambda = 1.19e-03, selected 84 features \n",
      "10 epochs, val_objective 8.82e-01, val_loss 8.54e-01, regularization 2.35e+01, l2_regularization 1.18e+01\n",
      "epoch: 0\n",
      "loss: 0.8535950183868408\n",
      "epoch: 1\n",
      "loss: 0.8531281352043152\n",
      "epoch: 2\n",
      "loss: 0.8526612520217896\n",
      "epoch: 3\n",
      "loss: 0.8521946668624878\n",
      "epoch: 4\n",
      "loss: 0.8517281413078308\n",
      "epoch: 5\n",
      "loss: 0.8512617349624634\n",
      "epoch: 6\n",
      "loss: 0.8507964611053467\n",
      "epoch: 7\n",
      "loss: 0.8503347039222717\n",
      "epoch: 8\n",
      "loss: 0.8498716354370117\n",
      "epoch: 9\n",
      "loss: 0.8494138717651367\n",
      "Lambda = 1.22e-03, selected 84 features \n",
      "10 epochs, val_objective 8.78e-01, val_loss 8.49e-01, regularization 2.36e+01, l2_regularization 1.18e+01\n",
      "epoch: 0\n",
      "loss: 0.8489527702331543\n",
      "epoch: 1\n",
      "loss: 0.8484938144683838\n",
      "epoch: 2\n",
      "loss: 0.8480351567268372\n",
      "epoch: 3\n",
      "loss: 0.8475764989852905\n",
      "epoch: 4\n",
      "loss: 0.8471186757087708\n",
      "epoch: 5\n",
      "loss: 0.8466620445251465\n",
      "epoch: 6\n",
      "loss: 0.8462046384811401\n",
      "epoch: 7\n",
      "loss: 0.8457494378089905\n",
      "epoch: 8\n",
      "loss: 0.8452935814857483\n",
      "epoch: 9\n",
      "loss: 0.8448441624641418\n",
      "Lambda = 1.25e-03, selected 84 features \n",
      "10 epochs, val_objective 8.74e-01, val_loss 8.44e-01, regularization 2.37e+01, l2_regularization 1.19e+01\n",
      "epoch: 0\n",
      "loss: 0.8443882465362549\n",
      "epoch: 1\n",
      "loss: 0.8439360857009888\n",
      "epoch: 2\n",
      "loss: 0.8434851765632629\n",
      "epoch: 3\n",
      "loss: 0.8430350422859192\n",
      "epoch: 4\n",
      "loss: 0.8425855040550232\n",
      "epoch: 5\n",
      "loss: 0.8421375155448914\n",
      "epoch: 6\n",
      "loss: 0.8416895866394043\n",
      "epoch: 7\n",
      "loss: 0.8412442207336426\n",
      "epoch: 8\n",
      "loss: 0.8407968282699585\n",
      "epoch: 9\n",
      "loss: 0.8403526544570923\n",
      "Lambda = 1.28e-03, selected 84 features \n",
      "10 epochs, val_objective 8.70e-01, val_loss 8.40e-01, regularization 2.38e+01, l2_regularization 1.19e+01\n",
      "epoch: 0\n",
      "loss: 0.8399072885513306\n",
      "epoch: 1\n",
      "loss: 0.83946293592453\n",
      "epoch: 2\n",
      "loss: 0.8390179872512817\n",
      "epoch: 3\n",
      "loss: 0.8385730981826782\n",
      "epoch: 4\n",
      "loss: 0.8381348252296448\n",
      "epoch: 5\n",
      "loss: 0.8376927375793457\n",
      "epoch: 6\n",
      "loss: 0.8372475504875183\n",
      "epoch: 7\n",
      "loss: 0.8368052840232849\n",
      "epoch: 8\n",
      "loss: 0.8363661170005798\n",
      "epoch: 9\n",
      "loss: 0.8359267115592957\n",
      "Lambda = 1.31e-03, selected 84 features \n",
      "10 epochs, val_objective 8.67e-01, val_loss 8.35e-01, regularization 2.39e+01, l2_regularization 1.20e+01\n",
      "epoch: 0\n",
      "loss: 0.8354883790016174\n",
      "epoch: 1\n",
      "loss: 0.8350521922111511\n",
      "epoch: 2\n",
      "loss: 0.8346120715141296\n",
      "epoch: 3\n",
      "loss: 0.8341755270957947\n",
      "epoch: 4\n",
      "loss: 0.8337427377700806\n",
      "epoch: 5\n",
      "loss: 0.8333038687705994\n",
      "epoch: 6\n",
      "loss: 0.8328719735145569\n",
      "epoch: 7\n",
      "loss: 0.8324401378631592\n",
      "epoch: 8\n",
      "loss: 0.8320085406303406\n",
      "epoch: 9\n",
      "loss: 0.8315773010253906\n",
      "Lambda = 1.34e-03, selected 84 features \n",
      "10 epochs, val_objective 8.63e-01, val_loss 8.31e-01, regularization 2.40e+01, l2_regularization 1.20e+01\n",
      "epoch: 0\n",
      "loss: 0.8311457633972168\n",
      "epoch: 1\n",
      "loss: 0.8307132720947266\n",
      "epoch: 2\n",
      "loss: 0.830282986164093\n",
      "epoch: 3\n",
      "loss: 0.8298534750938416\n",
      "epoch: 4\n",
      "loss: 0.8294243812561035\n",
      "epoch: 5\n",
      "loss: 0.8289949297904968\n",
      "epoch: 6\n",
      "loss: 0.8285669088363647\n",
      "epoch: 7\n",
      "loss: 0.8281394839286804\n",
      "epoch: 8\n",
      "loss: 0.827713668346405\n",
      "epoch: 9\n",
      "loss: 0.8272896409034729\n",
      "Lambda = 1.38e-03, selected 84 features \n",
      "10 epochs, val_objective 8.60e-01, val_loss 8.27e-01, regularization 2.41e+01, l2_regularization 1.20e+01\n",
      "epoch: 0\n",
      "loss: 0.8268632292747498\n",
      "epoch: 1\n",
      "loss: 0.8264425992965698\n",
      "epoch: 2\n",
      "loss: 0.826017439365387\n",
      "epoch: 3\n",
      "loss: 0.8255938291549683\n",
      "epoch: 4\n",
      "loss: 0.8251715302467346\n",
      "epoch: 5\n",
      "loss: 0.8247536420822144\n",
      "epoch: 6\n",
      "loss: 0.8243297934532166\n",
      "epoch: 7\n",
      "loss: 0.8239102959632874\n",
      "epoch: 8\n",
      "loss: 0.8234904408454895\n",
      "epoch: 9\n",
      "loss: 0.8230734467506409\n",
      "Lambda = 1.41e-03, selected 84 features \n",
      "10 epochs, val_objective 8.57e-01, val_loss 8.23e-01, regularization 2.42e+01, l2_regularization 1.21e+01\n",
      "epoch: 0\n",
      "loss: 0.822655975818634\n",
      "epoch: 1\n",
      "loss: 0.8222389817237854\n",
      "epoch: 2\n",
      "loss: 0.8218219876289368\n",
      "epoch: 3\n",
      "loss: 0.8214045763015747\n",
      "epoch: 4\n",
      "loss: 0.8209896087646484\n",
      "epoch: 5\n",
      "loss: 0.8205729722976685\n",
      "epoch: 6\n",
      "loss: 0.8201557397842407\n",
      "epoch: 7\n",
      "loss: 0.8197488784790039\n",
      "epoch: 8\n",
      "loss: 0.8193286061286926\n",
      "epoch: 9\n",
      "loss: 0.8189167380332947\n",
      "Lambda = 1.45e-03, selected 84 features \n",
      "10 epochs, val_objective 8.54e-01, val_loss 8.19e-01, regularization 2.43e+01, l2_regularization 1.21e+01\n",
      "epoch: 0\n",
      "loss: 0.8185053467750549\n",
      "epoch: 1\n",
      "loss: 0.8180931210517883\n",
      "epoch: 2\n",
      "loss: 0.8176808953285217\n",
      "epoch: 3\n",
      "loss: 0.8172711730003357\n",
      "epoch: 4\n",
      "loss: 0.8168601989746094\n",
      "epoch: 5\n",
      "loss: 0.8164497017860413\n",
      "epoch: 6\n",
      "loss: 0.8160427212715149\n",
      "epoch: 7\n",
      "loss: 0.8156319260597229\n",
      "epoch: 8\n",
      "loss: 0.8152289390563965\n",
      "epoch: 9\n",
      "loss: 0.8148229122161865\n",
      "Lambda = 1.48e-03, selected 84 features \n",
      "10 epochs, val_objective 8.51e-01, val_loss 8.14e-01, regularization 2.44e+01, l2_regularization 1.22e+01\n",
      "epoch: 0\n",
      "loss: 0.8144150376319885\n",
      "epoch: 1\n",
      "loss: 0.81401127576828\n",
      "epoch: 2\n",
      "loss: 0.8136072158813477\n",
      "epoch: 3\n",
      "loss: 0.8132032752037048\n",
      "epoch: 4\n",
      "loss: 0.812798798084259\n",
      "epoch: 5\n",
      "loss: 0.8123989701271057\n",
      "epoch: 6\n",
      "loss: 0.8119939565658569\n",
      "epoch: 7\n",
      "loss: 0.8115931749343872\n",
      "epoch: 8\n",
      "loss: 0.8111922740936279\n",
      "epoch: 9\n",
      "loss: 0.8107913136482239\n",
      "Lambda = 1.52e-03, selected 84 features \n",
      "10 epochs, val_objective 8.48e-01, val_loss 8.10e-01, regularization 2.45e+01, l2_regularization 1.22e+01\n",
      "epoch: 0\n",
      "loss: 0.8103904128074646\n",
      "epoch: 1\n",
      "loss: 0.8099892735481262\n",
      "epoch: 2\n",
      "loss: 0.8095906376838684\n",
      "epoch: 3\n",
      "loss: 0.8091935515403748\n",
      "epoch: 4\n",
      "loss: 0.8087937831878662\n",
      "epoch: 5\n",
      "loss: 0.8083973526954651\n",
      "epoch: 6\n",
      "loss: 0.808000922203064\n",
      "epoch: 7\n",
      "loss: 0.8076053261756897\n",
      "epoch: 8\n",
      "loss: 0.8072104454040527\n",
      "epoch: 9\n",
      "loss: 0.8068152666091919\n",
      "Lambda = 1.56e-03, selected 84 features \n",
      "10 epochs, val_objective 8.45e-01, val_loss 8.06e-01, regularization 2.46e+01, l2_regularization 1.23e+01\n",
      "epoch: 0\n",
      "loss: 0.8064208626747131\n",
      "epoch: 1\n",
      "loss: 0.8060261607170105\n",
      "epoch: 2\n",
      "loss: 0.8056367635726929\n",
      "epoch: 3\n",
      "loss: 0.8052406311035156\n",
      "epoch: 4\n",
      "loss: 0.8048474788665771\n",
      "epoch: 5\n",
      "loss: 0.8044556975364685\n",
      "epoch: 6\n",
      "loss: 0.8040650486946106\n",
      "epoch: 7\n",
      "loss: 0.8036745190620422\n",
      "epoch: 8\n",
      "loss: 0.8032853007316589\n",
      "epoch: 9\n",
      "loss: 0.8028969764709473\n",
      "Lambda = 1.60e-03, selected 84 features \n",
      "10 epochs, val_objective 8.42e-01, val_loss 8.03e-01, regularization 2.46e+01, l2_regularization 1.23e+01\n",
      "epoch: 0\n",
      "loss: 0.8025051355361938\n",
      "epoch: 1\n",
      "loss: 0.8021159768104553\n",
      "epoch: 2\n",
      "loss: 0.8017300963401794\n",
      "epoch: 3\n",
      "loss: 0.8013421297073364\n",
      "epoch: 4\n",
      "loss: 0.8009567856788635\n",
      "epoch: 5\n",
      "loss: 0.800571620464325\n",
      "epoch: 6\n",
      "loss: 0.8001864552497864\n",
      "epoch: 7\n",
      "loss: 0.7998031377792358\n",
      "epoch: 8\n",
      "loss: 0.7994190454483032\n",
      "epoch: 9\n",
      "loss: 0.7990362048149109\n",
      "Lambda = 1.64e-03, selected 84 features \n",
      "10 epochs, val_objective 8.39e-01, val_loss 7.99e-01, regularization 2.47e+01, l2_regularization 1.23e+01\n",
      "epoch: 0\n",
      "loss: 0.7986525297164917\n",
      "epoch: 1\n",
      "loss: 0.7982704043388367\n",
      "epoch: 2\n",
      "loss: 0.7978883981704712\n",
      "epoch: 3\n",
      "loss: 0.7975071668624878\n",
      "epoch: 4\n",
      "loss: 0.7971251010894775\n",
      "epoch: 5\n",
      "loss: 0.7967440485954285\n",
      "epoch: 6\n",
      "loss: 0.7963635921478271\n",
      "epoch: 7\n",
      "loss: 0.7959836721420288\n",
      "epoch: 8\n",
      "loss: 0.7956047654151917\n",
      "epoch: 9\n",
      "loss: 0.7952257990837097\n",
      "Lambda = 1.68e-03, selected 84 features \n",
      "10 epochs, val_objective 8.37e-01, val_loss 7.95e-01, regularization 2.48e+01, l2_regularization 1.24e+01\n",
      "epoch: 0\n",
      "loss: 0.7948462963104248\n",
      "epoch: 1\n",
      "loss: 0.7944685220718384\n",
      "epoch: 2\n",
      "loss: 0.7940902709960938\n",
      "epoch: 3\n",
      "loss: 0.793712854385376\n",
      "epoch: 4\n",
      "loss: 0.7933453321456909\n",
      "epoch: 5\n",
      "loss: 0.7929641008377075\n",
      "epoch: 6\n",
      "loss: 0.7925893068313599\n",
      "epoch: 7\n",
      "loss: 0.7922168374061584\n",
      "epoch: 8\n",
      "loss: 0.7918447256088257\n",
      "epoch: 9\n",
      "loss: 0.7914724349975586\n",
      "Lambda = 1.72e-03, selected 84 features \n",
      "10 epochs, val_objective 8.34e-01, val_loss 7.91e-01, regularization 2.49e+01, l2_regularization 1.24e+01\n",
      "epoch: 0\n",
      "loss: 0.7910988926887512\n",
      "epoch: 1\n",
      "loss: 0.7907282114028931\n",
      "epoch: 2\n",
      "loss: 0.7903532981872559\n",
      "epoch: 3\n",
      "loss: 0.7899812459945679\n",
      "epoch: 4\n",
      "loss: 0.7896091938018799\n",
      "epoch: 5\n",
      "loss: 0.7892383337020874\n",
      "epoch: 6\n",
      "loss: 0.7888683080673218\n",
      "epoch: 7\n",
      "loss: 0.7885022163391113\n",
      "epoch: 8\n",
      "loss: 0.7881309986114502\n",
      "epoch: 9\n",
      "loss: 0.7877619862556458\n",
      "Lambda = 1.76e-03, selected 84 features \n",
      "10 epochs, val_objective 8.32e-01, val_loss 7.87e-01, regularization 2.50e+01, l2_regularization 1.25e+01\n",
      "epoch: 0\n",
      "loss: 0.7873940467834473\n",
      "epoch: 1\n",
      "loss: 0.7870275378227234\n",
      "epoch: 2\n",
      "loss: 0.7866606712341309\n",
      "epoch: 3\n",
      "loss: 0.7862932682037354\n",
      "epoch: 4\n",
      "loss: 0.7859251499176025\n",
      "epoch: 5\n",
      "loss: 0.7855597734451294\n",
      "epoch: 6\n",
      "loss: 0.7851982712745667\n",
      "epoch: 7\n",
      "loss: 0.7848299741744995\n",
      "epoch: 8\n",
      "loss: 0.7844669818878174\n",
      "epoch: 9\n",
      "loss: 0.7841054797172546\n",
      "Lambda = 1.81e-03, selected 84 features \n",
      "10 epochs, val_objective 8.29e-01, val_loss 7.84e-01, regularization 2.51e+01, l2_regularization 1.25e+01\n",
      "epoch: 0\n",
      "loss: 0.7837424278259277\n",
      "epoch: 1\n",
      "loss: 0.7833820581436157\n",
      "epoch: 2\n",
      "loss: 0.7830191254615784\n",
      "epoch: 3\n",
      "loss: 0.7826578617095947\n",
      "epoch: 4\n",
      "loss: 0.7822989821434021\n",
      "epoch: 5\n",
      "loss: 0.7819363474845886\n",
      "epoch: 6\n",
      "loss: 0.781575620174408\n",
      "epoch: 7\n",
      "loss: 0.781215488910675\n",
      "epoch: 8\n",
      "loss: 0.7808558940887451\n",
      "epoch: 9\n",
      "loss: 0.7804962396621704\n",
      "Lambda = 1.85e-03, selected 84 features \n",
      "10 epochs, val_objective 8.27e-01, val_loss 7.80e-01, regularization 2.52e+01, l2_regularization 1.25e+01\n",
      "epoch: 0\n",
      "loss: 0.7801381349563599\n",
      "epoch: 1\n",
      "loss: 0.7797812819480896\n",
      "epoch: 2\n",
      "loss: 0.7794223427772522\n",
      "epoch: 3\n",
      "loss: 0.7790665626525879\n",
      "epoch: 4\n",
      "loss: 0.7787099480628967\n",
      "epoch: 5\n",
      "loss: 0.7783539891242981\n",
      "epoch: 6\n",
      "loss: 0.7779983878135681\n",
      "epoch: 7\n",
      "loss: 0.7776427268981934\n",
      "epoch: 8\n",
      "loss: 0.7772890329360962\n",
      "epoch: 9\n",
      "loss: 0.7769359350204468\n",
      "Lambda = 1.90e-03, selected 84 features \n",
      "10 epochs, val_objective 8.25e-01, val_loss 7.77e-01, regularization 2.53e+01, l2_regularization 1.26e+01\n",
      "epoch: 0\n",
      "loss: 0.7765843272209167\n",
      "epoch: 1\n",
      "loss: 0.7762331962585449\n",
      "epoch: 2\n",
      "loss: 0.7758820652961731\n",
      "epoch: 3\n",
      "loss: 0.7755305171012878\n",
      "epoch: 4\n",
      "loss: 0.7751801609992981\n",
      "epoch: 5\n",
      "loss: 0.7748264074325562\n",
      "epoch: 6\n",
      "loss: 0.7744744420051575\n",
      "epoch: 7\n",
      "loss: 0.7741227746009827\n",
      "epoch: 8\n",
      "loss: 0.773777961730957\n",
      "epoch: 9\n",
      "loss: 0.7734225988388062\n",
      "Lambda = 1.95e-03, selected 84 features \n",
      "10 epochs, val_objective 8.22e-01, val_loss 7.73e-01, regularization 2.53e+01, l2_regularization 1.26e+01\n",
      "epoch: 0\n",
      "loss: 0.7730733752250671\n",
      "epoch: 1\n",
      "loss: 0.7727267742156982\n",
      "epoch: 2\n",
      "loss: 0.7723793983459473\n",
      "epoch: 3\n",
      "loss: 0.7720323801040649\n",
      "epoch: 4\n",
      "loss: 0.771685004234314\n",
      "epoch: 5\n",
      "loss: 0.7713370323181152\n",
      "epoch: 6\n",
      "loss: 0.7709906697273254\n",
      "epoch: 7\n",
      "loss: 0.7706449031829834\n",
      "epoch: 8\n",
      "loss: 0.7702996134757996\n",
      "epoch: 9\n",
      "loss: 0.7699525952339172\n",
      "Lambda = 2.00e-03, selected 84 features \n",
      "10 epochs, val_objective 8.20e-01, val_loss 7.70e-01, regularization 2.54e+01, l2_regularization 1.27e+01\n",
      "epoch: 0\n",
      "loss: 0.7696099281311035\n",
      "epoch: 1\n",
      "loss: 0.7692647576332092\n",
      "epoch: 2\n",
      "loss: 0.7689225077629089\n",
      "epoch: 3\n",
      "loss: 0.7685807347297668\n",
      "epoch: 4\n",
      "loss: 0.7682368755340576\n",
      "epoch: 5\n",
      "loss: 0.767894983291626\n",
      "epoch: 6\n",
      "loss: 0.7675524353981018\n",
      "epoch: 7\n",
      "loss: 0.7672137022018433\n",
      "epoch: 8\n",
      "loss: 0.766869843006134\n",
      "epoch: 9\n",
      "loss: 0.7665322422981262\n",
      "Lambda = 2.05e-03, selected 84 features \n",
      "10 epochs, val_objective 8.18e-01, val_loss 7.66e-01, regularization 2.55e+01, l2_regularization 1.27e+01\n",
      "epoch: 0\n",
      "loss: 0.766189455986023\n",
      "epoch: 1\n",
      "loss: 0.7658494710922241\n",
      "epoch: 2\n",
      "loss: 0.7655109167098999\n",
      "epoch: 3\n",
      "loss: 0.7651726007461548\n",
      "epoch: 4\n",
      "loss: 0.7648345232009888\n",
      "epoch: 5\n",
      "loss: 0.7644968032836914\n",
      "epoch: 6\n",
      "loss: 0.7641593217849731\n",
      "epoch: 7\n",
      "loss: 0.7638231515884399\n",
      "epoch: 8\n",
      "loss: 0.763485848903656\n",
      "epoch: 9\n",
      "loss: 0.7631493210792542\n",
      "Lambda = 2.10e-03, selected 84 features \n",
      "10 epochs, val_objective 8.17e-01, val_loss 7.63e-01, regularization 2.56e+01, l2_regularization 1.27e+01\n",
      "epoch: 0\n",
      "loss: 0.762813150882721\n",
      "epoch: 1\n",
      "loss: 0.7624766230583191\n",
      "epoch: 2\n",
      "loss: 0.762140691280365\n",
      "epoch: 3\n",
      "loss: 0.7618109583854675\n",
      "epoch: 4\n",
      "loss: 0.7614739537239075\n",
      "epoch: 5\n",
      "loss: 0.7611393928527832\n",
      "epoch: 6\n",
      "loss: 0.7608066201210022\n",
      "epoch: 7\n",
      "loss: 0.760474681854248\n",
      "epoch: 8\n",
      "loss: 0.7601432800292969\n",
      "epoch: 9\n",
      "loss: 0.7598102688789368\n",
      "Lambda = 2.15e-03, selected 84 features \n",
      "10 epochs, val_objective 8.15e-01, val_loss 7.59e-01, regularization 2.57e+01, l2_regularization 1.28e+01\n",
      "epoch: 0\n",
      "loss: 0.7594825029373169\n",
      "epoch: 1\n",
      "loss: 0.7591489553451538\n",
      "epoch: 2\n",
      "loss: 0.7588195204734802\n",
      "epoch: 3\n",
      "loss: 0.7584899663925171\n",
      "epoch: 4\n",
      "loss: 0.7581596970558167\n",
      "epoch: 5\n",
      "loss: 0.7578295469284058\n",
      "epoch: 6\n",
      "loss: 0.7574993371963501\n",
      "epoch: 7\n",
      "loss: 0.757169783115387\n",
      "epoch: 8\n",
      "loss: 0.7568397521972656\n",
      "epoch: 9\n",
      "loss: 0.7565141916275024\n",
      "Lambda = 2.20e-03, selected 84 features \n",
      "10 epochs, val_objective 8.13e-01, val_loss 7.56e-01, regularization 2.58e+01, l2_regularization 1.28e+01\n",
      "epoch: 0\n",
      "loss: 0.7561845779418945\n",
      "epoch: 1\n",
      "loss: 0.7558596730232239\n",
      "epoch: 2\n",
      "loss: 0.7555299997329712\n",
      "epoch: 3\n",
      "loss: 0.7552027106285095\n",
      "epoch: 4\n",
      "loss: 0.754876971244812\n",
      "epoch: 5\n",
      "loss: 0.7545515298843384\n",
      "epoch: 6\n",
      "loss: 0.7542262077331543\n",
      "epoch: 7\n",
      "loss: 0.7539011836051941\n",
      "epoch: 8\n",
      "loss: 0.7535820603370667\n",
      "epoch: 9\n",
      "loss: 0.7532532215118408\n",
      "Lambda = 2.26e-03, selected 84 features \n",
      "10 epochs, val_objective 8.11e-01, val_loss 7.53e-01, regularization 2.59e+01, l2_regularization 1.28e+01\n",
      "epoch: 0\n",
      "loss: 0.7529309391975403\n",
      "epoch: 1\n",
      "loss: 0.7526108622550964\n",
      "epoch: 2\n",
      "loss: 0.7522861957550049\n",
      "epoch: 3\n",
      "loss: 0.7519656419754028\n",
      "epoch: 4\n",
      "loss: 0.7516452074050903\n",
      "epoch: 5\n",
      "loss: 0.7513237595558167\n",
      "epoch: 6\n",
      "loss: 0.7510031461715698\n",
      "epoch: 7\n",
      "loss: 0.7506822943687439\n",
      "epoch: 8\n",
      "loss: 0.7503611445426941\n",
      "epoch: 9\n",
      "loss: 0.7500418424606323\n",
      "Lambda = 2.32e-03, selected 84 features \n",
      "10 epochs, val_objective 8.10e-01, val_loss 7.50e-01, regularization 2.59e+01, l2_regularization 1.29e+01\n",
      "epoch: 0\n",
      "loss: 0.7497198581695557\n",
      "epoch: 1\n",
      "loss: 0.7493997812271118\n",
      "epoch: 2\n",
      "loss: 0.7490803599357605\n",
      "epoch: 3\n",
      "loss: 0.748762309551239\n",
      "epoch: 4\n",
      "loss: 0.7484444379806519\n",
      "epoch: 5\n",
      "loss: 0.7481257319450378\n",
      "epoch: 6\n",
      "loss: 0.7478084564208984\n",
      "epoch: 7\n",
      "loss: 0.7474909424781799\n",
      "epoch: 8\n",
      "loss: 0.7471739053726196\n",
      "epoch: 9\n",
      "loss: 0.7468565702438354\n",
      "Lambda = 2.37e-03, selected 84 features \n",
      "10 epochs, val_objective 8.08e-01, val_loss 7.47e-01, regularization 2.60e+01, l2_regularization 1.29e+01\n",
      "epoch: 0\n",
      "loss: 0.7465399503707886\n",
      "epoch: 1\n",
      "loss: 0.7462236881256104\n",
      "epoch: 2\n",
      "loss: 0.7459073066711426\n",
      "epoch: 3\n",
      "loss: 0.7455949783325195\n",
      "epoch: 4\n",
      "loss: 0.7452887892723083\n",
      "epoch: 5\n",
      "loss: 0.7449662685394287\n",
      "epoch: 6\n",
      "loss: 0.7446561455726624\n",
      "epoch: 7\n",
      "loss: 0.7443456053733826\n",
      "epoch: 8\n",
      "loss: 0.7440343499183655\n",
      "epoch: 9\n",
      "loss: 0.743722677230835\n",
      "Lambda = 2.43e-03, selected 84 features \n",
      "10 epochs, val_objective 8.07e-01, val_loss 7.43e-01, regularization 2.61e+01, l2_regularization 1.30e+01\n",
      "epoch: 0\n",
      "loss: 0.7434138655662537\n",
      "epoch: 1\n",
      "loss: 0.7430992126464844\n",
      "epoch: 2\n",
      "loss: 0.742787778377533\n",
      "epoch: 3\n",
      "loss: 0.7424761652946472\n",
      "epoch: 4\n",
      "loss: 0.7421647310256958\n",
      "epoch: 5\n",
      "loss: 0.7418529391288757\n",
      "epoch: 6\n",
      "loss: 0.7415422797203064\n",
      "epoch: 7\n",
      "loss: 0.7412314414978027\n",
      "epoch: 8\n",
      "loss: 0.7409234046936035\n",
      "epoch: 9\n",
      "loss: 0.7406171560287476\n",
      "Lambda = 2.49e-03, selected 84 features \n",
      "10 epochs, val_objective 8.06e-01, val_loss 7.40e-01, regularization 2.62e+01, l2_regularization 1.30e+01\n",
      "epoch: 0\n",
      "loss: 0.7403060793876648\n",
      "epoch: 1\n",
      "loss: 0.7399969696998596\n",
      "epoch: 2\n",
      "loss: 0.7396893501281738\n",
      "epoch: 3\n",
      "loss: 0.7393818497657776\n",
      "epoch: 4\n",
      "loss: 0.739074170589447\n",
      "epoch: 5\n",
      "loss: 0.738767147064209\n",
      "epoch: 6\n",
      "loss: 0.7384592294692993\n",
      "epoch: 7\n",
      "loss: 0.7381540536880493\n",
      "epoch: 8\n",
      "loss: 0.7378467917442322\n",
      "epoch: 9\n",
      "loss: 0.737540602684021\n",
      "Lambda = 2.56e-03, selected 84 features \n",
      "10 epochs, val_objective 8.04e-01, val_loss 7.37e-01, regularization 2.63e+01, l2_regularization 1.30e+01\n",
      "epoch: 0\n",
      "loss: 0.7372365593910217\n",
      "epoch: 1\n",
      "loss: 0.7369357347488403\n",
      "epoch: 2\n",
      "loss: 0.7366292476654053\n",
      "epoch: 3\n",
      "loss: 0.7363268733024597\n",
      "epoch: 4\n",
      "loss: 0.7360260486602783\n",
      "epoch: 5\n",
      "loss: 0.7357226610183716\n",
      "epoch: 6\n",
      "loss: 0.7354199290275574\n",
      "epoch: 7\n",
      "loss: 0.7351173162460327\n",
      "epoch: 8\n",
      "loss: 0.7348146438598633\n",
      "epoch: 9\n",
      "loss: 0.7345131635665894\n",
      "Lambda = 2.62e-03, selected 84 features \n",
      "10 epochs, val_objective 8.03e-01, val_loss 7.34e-01, regularization 2.63e+01, l2_regularization 1.31e+01\n",
      "epoch: 0\n",
      "loss: 0.7342102527618408\n",
      "epoch: 1\n",
      "loss: 0.7339078783988953\n",
      "epoch: 2\n",
      "loss: 0.7336072325706482\n",
      "epoch: 3\n",
      "loss: 0.7333070039749146\n",
      "epoch: 4\n",
      "loss: 0.7330062389373779\n",
      "epoch: 5\n",
      "loss: 0.7327064275741577\n",
      "epoch: 6\n",
      "loss: 0.7324070930480957\n",
      "epoch: 7\n",
      "loss: 0.7321077585220337\n",
      "epoch: 8\n",
      "loss: 0.7318089008331299\n",
      "epoch: 9\n",
      "loss: 0.7315144538879395\n",
      "Lambda = 2.69e-03, selected 84 features \n",
      "10 epochs, val_objective 8.02e-01, val_loss 7.31e-01, regularization 2.64e+01, l2_regularization 1.31e+01\n",
      "epoch: 0\n",
      "loss: 0.7312185764312744\n",
      "epoch: 1\n",
      "loss: 0.7309183478355408\n",
      "epoch: 2\n",
      "loss: 0.7306221723556519\n",
      "epoch: 3\n",
      "loss: 0.7303257584571838\n",
      "epoch: 4\n",
      "loss: 0.7300292253494263\n",
      "epoch: 5\n",
      "loss: 0.7297350764274597\n",
      "epoch: 6\n",
      "loss: 0.7294387817382812\n",
      "epoch: 7\n",
      "loss: 0.7291446924209595\n",
      "epoch: 8\n",
      "loss: 0.7288474440574646\n",
      "epoch: 9\n",
      "loss: 0.7285511493682861\n",
      "Lambda = 2.75e-03, selected 84 features \n",
      "10 epochs, val_objective 8.01e-01, val_loss 7.28e-01, regularization 2.65e+01, l2_regularization 1.31e+01\n",
      "epoch: 0\n",
      "loss: 0.7282568216323853\n",
      "epoch: 1\n",
      "loss: 0.7279640436172485\n",
      "epoch: 2\n",
      "loss: 0.7276688814163208\n",
      "epoch: 3\n",
      "loss: 0.7273758053779602\n",
      "epoch: 4\n",
      "loss: 0.7270836234092712\n",
      "epoch: 5\n",
      "loss: 0.7267912030220032\n",
      "epoch: 6\n",
      "loss: 0.7264984846115112\n",
      "epoch: 7\n",
      "loss: 0.7262064218521118\n",
      "epoch: 8\n",
      "loss: 0.7259150147438049\n",
      "epoch: 9\n",
      "loss: 0.7256217002868652\n",
      "Lambda = 2.82e-03, selected 84 features \n",
      "10 epochs, val_objective 8.00e-01, val_loss 7.25e-01, regularization 2.66e+01, l2_regularization 1.32e+01\n",
      "epoch: 0\n",
      "loss: 0.7253304719924927\n",
      "epoch: 1\n",
      "loss: 0.7250391840934753\n",
      "epoch: 2\n",
      "loss: 0.7247480154037476\n",
      "epoch: 3\n",
      "loss: 0.7244568467140198\n",
      "epoch: 4\n",
      "loss: 0.7241673469543457\n",
      "epoch: 5\n",
      "loss: 0.7238773107528687\n",
      "epoch: 6\n",
      "loss: 0.7235884666442871\n",
      "epoch: 7\n",
      "loss: 0.7232993841171265\n",
      "epoch: 8\n",
      "loss: 0.7230100631713867\n",
      "epoch: 9\n",
      "loss: 0.7227206230163574\n",
      "Lambda = 2.89e-03, selected 84 features \n",
      "10 epochs, val_objective 8.00e-01, val_loss 7.22e-01, regularization 2.67e+01, l2_regularization 1.32e+01\n",
      "epoch: 0\n",
      "loss: 0.7224344611167908\n",
      "epoch: 1\n",
      "loss: 0.7221453189849854\n",
      "epoch: 2\n",
      "loss: 0.721862256526947\n",
      "epoch: 3\n",
      "loss: 0.7215684652328491\n",
      "epoch: 4\n",
      "loss: 0.7212825417518616\n",
      "epoch: 5\n",
      "loss: 0.7209964990615845\n",
      "epoch: 6\n",
      "loss: 0.7207096815109253\n",
      "epoch: 7\n",
      "loss: 0.7204243540763855\n",
      "epoch: 8\n",
      "loss: 0.7201380729675293\n",
      "epoch: 9\n",
      "loss: 0.7198526263237\n",
      "Lambda = 2.96e-03, selected 84 features \n",
      "10 epochs, val_objective 7.99e-01, val_loss 7.20e-01, regularization 2.67e+01, l2_regularization 1.32e+01\n",
      "epoch: 0\n",
      "loss: 0.7195672988891602\n",
      "epoch: 1\n",
      "loss: 0.719283401966095\n",
      "epoch: 2\n",
      "loss: 0.7189977169036865\n",
      "epoch: 3\n",
      "loss: 0.7187145948410034\n",
      "epoch: 4\n",
      "loss: 0.7184315919876099\n",
      "epoch: 5\n",
      "loss: 0.7181492447853088\n",
      "epoch: 6\n",
      "loss: 0.7178663015365601\n",
      "epoch: 7\n",
      "loss: 0.7175830006599426\n",
      "epoch: 8\n",
      "loss: 0.717300295829773\n",
      "epoch: 9\n",
      "loss: 0.7170190811157227\n",
      "Lambda = 3.04e-03, selected 84 features \n",
      "10 epochs, val_objective 7.98e-01, val_loss 7.17e-01, regularization 2.68e+01, l2_regularization 1.33e+01\n",
      "epoch: 0\n",
      "loss: 0.7167372703552246\n",
      "epoch: 1\n",
      "loss: 0.7164543867111206\n",
      "epoch: 2\n",
      "loss: 0.716174840927124\n",
      "epoch: 3\n",
      "loss: 0.7158933281898499\n",
      "epoch: 4\n",
      "loss: 0.7156135439872742\n",
      "epoch: 5\n",
      "loss: 0.7153319120407104\n",
      "epoch: 6\n",
      "loss: 0.7150536775588989\n",
      "epoch: 7\n",
      "loss: 0.7147732973098755\n",
      "epoch: 8\n",
      "loss: 0.7144927978515625\n",
      "epoch: 9\n",
      "loss: 0.714215099811554\n",
      "Lambda = 3.11e-03, selected 84 features \n",
      "10 epochs, val_objective 7.98e-01, val_loss 7.14e-01, regularization 2.69e+01, l2_regularization 1.33e+01\n",
      "epoch: 0\n",
      "loss: 0.7139347791671753\n",
      "epoch: 1\n",
      "loss: 0.7136565446853638\n",
      "epoch: 2\n",
      "loss: 0.7133786082267761\n",
      "epoch: 3\n",
      "loss: 0.7131026983261108\n",
      "epoch: 4\n",
      "loss: 0.7128257751464844\n",
      "epoch: 5\n",
      "loss: 0.7125491499900818\n",
      "epoch: 6\n",
      "loss: 0.7122726440429688\n",
      "epoch: 7\n",
      "loss: 0.7119959592819214\n",
      "epoch: 8\n",
      "loss: 0.7117189168930054\n",
      "epoch: 9\n",
      "loss: 0.7114440202713013\n",
      "Lambda = 3.19e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 7.11e-01, regularization 2.70e+01, l2_regularization 1.33e+01\n",
      "epoch: 0\n",
      "loss: 0.7111678123474121\n",
      "epoch: 1\n",
      "loss: 0.7108938694000244\n",
      "epoch: 2\n",
      "loss: 0.7106196284294128\n",
      "epoch: 3\n",
      "loss: 0.7103447914123535\n",
      "epoch: 4\n",
      "loss: 0.7100695967674255\n",
      "epoch: 5\n",
      "loss: 0.7097951173782349\n",
      "epoch: 6\n",
      "loss: 0.7095199823379517\n",
      "epoch: 7\n",
      "loss: 0.7092469334602356\n",
      "epoch: 8\n",
      "loss: 0.7089738845825195\n",
      "epoch: 9\n",
      "loss: 0.7087000012397766\n",
      "Lambda = 3.27e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 7.08e-01, regularization 2.70e+01, l2_regularization 1.34e+01\n",
      "epoch: 0\n",
      "loss: 0.7084283828735352\n",
      "epoch: 1\n",
      "loss: 0.7081552743911743\n",
      "epoch: 2\n",
      "loss: 0.7078832387924194\n",
      "epoch: 3\n",
      "loss: 0.7076122164726257\n",
      "epoch: 4\n",
      "loss: 0.7073392271995544\n",
      "epoch: 5\n",
      "loss: 0.7070692777633667\n",
      "epoch: 6\n",
      "loss: 0.7067966461181641\n",
      "epoch: 7\n",
      "loss: 0.7065274715423584\n",
      "epoch: 8\n",
      "loss: 0.7062578201293945\n",
      "epoch: 9\n",
      "loss: 0.7059863805770874\n",
      "Lambda = 3.35e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 7.06e-01, regularization 2.71e+01, l2_regularization 1.34e+01\n",
      "epoch: 0\n",
      "loss: 0.7057163715362549\n",
      "epoch: 1\n",
      "loss: 0.705447256565094\n",
      "epoch: 2\n",
      "loss: 0.7051774263381958\n",
      "epoch: 3\n",
      "loss: 0.7049078345298767\n",
      "epoch: 4\n",
      "loss: 0.7046383619308472\n",
      "epoch: 5\n",
      "loss: 0.7043708562850952\n",
      "epoch: 6\n",
      "loss: 0.7041025161743164\n",
      "epoch: 7\n",
      "loss: 0.7038322687149048\n",
      "epoch: 8\n",
      "loss: 0.7035649418830872\n",
      "epoch: 9\n",
      "loss: 0.7032976150512695\n",
      "Lambda = 3.44e-03, selected 84 features \n",
      "10 epochs, val_objective 7.96e-01, val_loss 7.03e-01, regularization 2.72e+01, l2_regularization 1.34e+01\n",
      "epoch: 0\n",
      "loss: 0.7030320167541504\n",
      "epoch: 1\n",
      "loss: 0.7027630805969238\n",
      "epoch: 2\n",
      "loss: 0.7024977803230286\n",
      "epoch: 3\n",
      "loss: 0.7022315859794617\n",
      "epoch: 4\n",
      "loss: 0.701966404914856\n",
      "epoch: 5\n",
      "loss: 0.7017006874084473\n",
      "epoch: 6\n",
      "loss: 0.7014350295066833\n",
      "epoch: 7\n",
      "loss: 0.7011725902557373\n",
      "epoch: 8\n",
      "loss: 0.7009046077728271\n",
      "epoch: 9\n",
      "loss: 0.7006399035453796\n",
      "Lambda = 3.52e-03, selected 84 features \n",
      "10 epochs, val_objective 7.96e-01, val_loss 7.00e-01, regularization 2.73e+01, l2_regularization 1.35e+01\n",
      "epoch: 0\n",
      "loss: 0.7003757953643799\n",
      "epoch: 1\n",
      "loss: 0.7001123428344727\n",
      "epoch: 2\n",
      "loss: 0.6998515129089355\n",
      "epoch: 3\n",
      "loss: 0.6995852589607239\n",
      "epoch: 4\n",
      "loss: 0.6993237733840942\n",
      "epoch: 5\n",
      "loss: 0.6990618705749512\n",
      "epoch: 6\n",
      "loss: 0.6987972259521484\n",
      "epoch: 7\n",
      "loss: 0.69853675365448\n",
      "epoch: 8\n",
      "loss: 0.6982749104499817\n",
      "epoch: 9\n",
      "loss: 0.698013424873352\n",
      "Lambda = 3.61e-03, selected 84 features \n",
      "10 epochs, val_objective 7.96e-01, val_loss 6.98e-01, regularization 2.73e+01, l2_regularization 1.35e+01\n",
      "epoch: 0\n",
      "loss: 0.6977524161338806\n",
      "epoch: 1\n",
      "loss: 0.6974910497665405\n",
      "epoch: 2\n",
      "loss: 0.697230875492096\n",
      "epoch: 3\n",
      "loss: 0.6969684362411499\n",
      "epoch: 4\n",
      "loss: 0.6967076063156128\n",
      "epoch: 5\n",
      "loss: 0.6964468955993652\n",
      "epoch: 6\n",
      "loss: 0.6961880922317505\n",
      "epoch: 7\n",
      "loss: 0.6959272027015686\n",
      "epoch: 8\n",
      "loss: 0.6956684589385986\n",
      "epoch: 9\n",
      "loss: 0.6954095363616943\n",
      "Lambda = 3.70e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 6.95e-01, regularization 2.74e+01, l2_regularization 1.35e+01\n",
      "epoch: 0\n",
      "loss: 0.6951509118080139\n",
      "epoch: 1\n",
      "loss: 0.6948919892311096\n",
      "epoch: 2\n",
      "loss: 0.6946341395378113\n",
      "epoch: 3\n",
      "loss: 0.6943753957748413\n",
      "epoch: 4\n",
      "loss: 0.6941172480583191\n",
      "epoch: 5\n",
      "loss: 0.6938599348068237\n",
      "epoch: 6\n",
      "loss: 0.6936025023460388\n",
      "epoch: 7\n",
      "loss: 0.6933451890945435\n",
      "epoch: 8\n",
      "loss: 0.6930880546569824\n",
      "epoch: 9\n",
      "loss: 0.6928315162658691\n",
      "Lambda = 3.79e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 6.93e-01, regularization 2.75e+01, l2_regularization 1.36e+01\n",
      "epoch: 0\n",
      "loss: 0.6925751566886902\n",
      "epoch: 1\n",
      "loss: 0.6923187971115112\n",
      "epoch: 2\n",
      "loss: 0.6920674443244934\n",
      "epoch: 3\n",
      "loss: 0.6918091773986816\n",
      "epoch: 4\n",
      "loss: 0.6915536522865295\n",
      "epoch: 5\n",
      "loss: 0.691299319267273\n",
      "epoch: 6\n",
      "loss: 0.6910476088523865\n",
      "epoch: 7\n",
      "loss: 0.6907934546470642\n",
      "epoch: 8\n",
      "loss: 0.6905381679534912\n",
      "epoch: 9\n",
      "loss: 0.6902849078178406\n",
      "Lambda = 3.89e-03, selected 84 features \n",
      "10 epochs, val_objective 7.97e-01, val_loss 6.90e-01, regularization 2.76e+01, l2_regularization 1.36e+01\n",
      "epoch: 0\n",
      "loss: 0.6900314092636108\n",
      "epoch: 1\n",
      "loss: 0.6897785067558289\n",
      "epoch: 2\n",
      "loss: 0.6895259618759155\n",
      "epoch: 3\n",
      "loss: 0.6892735362052917\n",
      "epoch: 4\n",
      "loss: 0.6890211701393127\n",
      "epoch: 5\n",
      "loss: 0.6887684464454651\n",
      "epoch: 6\n",
      "loss: 0.688515305519104\n",
      "epoch: 7\n",
      "loss: 0.6882632374763489\n",
      "epoch: 8\n",
      "loss: 0.6880115270614624\n",
      "epoch: 9\n",
      "loss: 0.6877585053443909\n",
      "Lambda = 3.99e-03, selected 84 features \n",
      "10 epochs, val_objective 7.98e-01, val_loss 6.88e-01, regularization 2.76e+01, l2_regularization 1.36e+01\n",
      "epoch: 0\n",
      "loss: 0.6875074505805969\n",
      "epoch: 1\n",
      "loss: 0.6872568130493164\n",
      "epoch: 2\n",
      "loss: 0.6870059370994568\n",
      "epoch: 3\n",
      "loss: 0.6867551207542419\n",
      "epoch: 4\n",
      "loss: 0.6865062713623047\n",
      "epoch: 5\n",
      "loss: 0.6862557530403137\n",
      "epoch: 6\n",
      "loss: 0.6860085129737854\n",
      "epoch: 7\n",
      "loss: 0.6857569813728333\n",
      "epoch: 8\n",
      "loss: 0.6855083107948303\n",
      "epoch: 9\n",
      "loss: 0.6852598190307617\n",
      "Lambda = 4.09e-03, selected 84 features \n",
      "10 epochs, val_objective 7.98e-01, val_loss 6.85e-01, regularization 2.77e+01, l2_regularization 1.37e+01\n",
      "epoch: 0\n",
      "loss: 0.685010552406311\n",
      "epoch: 1\n",
      "loss: 0.6847671270370483\n",
      "epoch: 2\n",
      "loss: 0.6845154762268066\n",
      "epoch: 3\n",
      "loss: 0.6842688322067261\n",
      "epoch: 4\n",
      "loss: 0.6840221881866455\n",
      "epoch: 5\n",
      "loss: 0.6837759017944336\n",
      "epoch: 6\n",
      "loss: 0.6835291385650635\n",
      "epoch: 7\n",
      "loss: 0.6832820177078247\n",
      "epoch: 8\n",
      "loss: 0.6830366849899292\n",
      "epoch: 9\n",
      "loss: 0.68278968334198\n",
      "Lambda = 4.19e-03, selected 84 features \n",
      "10 epochs, val_objective 7.99e-01, val_loss 6.83e-01, regularization 2.78e+01, l2_regularization 1.37e+01\n",
      "epoch: 0\n",
      "loss: 0.682543158531189\n",
      "epoch: 1\n",
      "loss: 0.6822974681854248\n",
      "epoch: 2\n",
      "loss: 0.6820518374443054\n",
      "epoch: 3\n",
      "loss: 0.6818080544471741\n",
      "epoch: 4\n",
      "loss: 0.6815609931945801\n",
      "epoch: 5\n",
      "loss: 0.6813169717788696\n",
      "epoch: 6\n",
      "loss: 0.6810715198516846\n",
      "epoch: 7\n",
      "loss: 0.6808269023895264\n",
      "epoch: 8\n",
      "loss: 0.6805822253227234\n",
      "epoch: 9\n",
      "loss: 0.6803404092788696\n",
      "Lambda = 4.29e-03, selected 84 features \n",
      "10 epochs, val_objective 8.00e-01, val_loss 6.80e-01, regularization 2.79e+01, l2_regularization 1.37e+01\n",
      "epoch: 0\n",
      "loss: 0.6800954341888428\n",
      "epoch: 1\n",
      "loss: 0.6798518896102905\n",
      "epoch: 2\n",
      "loss: 0.6796091198921204\n",
      "epoch: 3\n",
      "loss: 0.6793661713600159\n",
      "epoch: 4\n",
      "loss: 0.6791235208511353\n",
      "epoch: 5\n",
      "loss: 0.6788812875747681\n",
      "epoch: 6\n",
      "loss: 0.6786386370658875\n",
      "epoch: 7\n",
      "loss: 0.6784007549285889\n",
      "epoch: 8\n",
      "loss: 0.6781565546989441\n",
      "epoch: 9\n",
      "loss: 0.6779152154922485\n",
      "Lambda = 4.40e-03, selected 84 features \n",
      "10 epochs, val_objective 8.01e-01, val_loss 6.78e-01, regularization 2.79e+01, l2_regularization 1.38e+01\n",
      "epoch: 0\n",
      "loss: 0.6776735186576843\n",
      "epoch: 1\n",
      "loss: 0.6774333715438843\n",
      "epoch: 2\n",
      "loss: 0.6771920919418335\n",
      "epoch: 3\n",
      "loss: 0.6769521236419678\n",
      "epoch: 4\n",
      "loss: 0.6767123937606812\n",
      "epoch: 5\n",
      "loss: 0.6764733791351318\n",
      "epoch: 6\n",
      "loss: 0.6762331128120422\n",
      "epoch: 7\n",
      "loss: 0.6759935617446899\n",
      "epoch: 8\n",
      "loss: 0.6757549047470093\n",
      "epoch: 9\n",
      "loss: 0.6755161881446838\n",
      "Lambda = 4.51e-03, selected 84 features \n",
      "10 epochs, val_objective 8.02e-01, val_loss 6.75e-01, regularization 2.80e+01, l2_regularization 1.38e+01\n",
      "epoch: 0\n",
      "loss: 0.6752774119377136\n",
      "epoch: 1\n",
      "loss: 0.6750384569168091\n",
      "epoch: 2\n",
      "loss: 0.674801766872406\n",
      "epoch: 3\n",
      "loss: 0.6745625138282776\n",
      "epoch: 4\n",
      "loss: 0.6743259429931641\n",
      "epoch: 5\n",
      "loss: 0.6740884780883789\n",
      "epoch: 6\n",
      "loss: 0.6738520264625549\n",
      "epoch: 7\n",
      "loss: 0.6736136674880981\n",
      "epoch: 8\n",
      "loss: 0.6733763217926025\n",
      "epoch: 9\n",
      "loss: 0.6731389760971069\n",
      "Lambda = 4.62e-03, selected 84 features \n",
      "10 epochs, val_objective 8.03e-01, val_loss 6.73e-01, regularization 2.81e+01, l2_regularization 1.38e+01\n",
      "epoch: 0\n",
      "loss: 0.6729028820991516\n",
      "epoch: 1\n",
      "loss: 0.6726655960083008\n",
      "epoch: 2\n",
      "loss: 0.6724295020103455\n",
      "epoch: 3\n",
      "loss: 0.6721944212913513\n",
      "epoch: 4\n",
      "loss: 0.6719576716423035\n",
      "epoch: 5\n",
      "loss: 0.6717225313186646\n",
      "epoch: 6\n",
      "loss: 0.6714879870414734\n",
      "epoch: 7\n",
      "loss: 0.6712530851364136\n",
      "epoch: 8\n",
      "loss: 0.6710189580917358\n",
      "epoch: 9\n",
      "loss: 0.6707859635353088\n",
      "Lambda = 4.74e-03, selected 84 features \n",
      "10 epochs, val_objective 8.04e-01, val_loss 6.71e-01, regularization 2.81e+01, l2_regularization 1.39e+01\n",
      "epoch: 0\n",
      "loss: 0.6705508232116699\n",
      "epoch: 1\n",
      "loss: 0.6703172922134399\n",
      "epoch: 2\n",
      "loss: 0.6700841784477234\n",
      "epoch: 3\n",
      "loss: 0.6698516011238098\n",
      "epoch: 4\n",
      "loss: 0.669618546962738\n",
      "epoch: 5\n",
      "loss: 0.6693857908248901\n",
      "epoch: 6\n",
      "loss: 0.6691527962684631\n",
      "epoch: 7\n",
      "loss: 0.6689198017120361\n",
      "epoch: 8\n",
      "loss: 0.6686865091323853\n",
      "epoch: 9\n",
      "loss: 0.6684542298316956\n",
      "Lambda = 4.86e-03, selected 84 features \n",
      "10 epochs, val_objective 8.05e-01, val_loss 6.68e-01, regularization 2.82e+01, l2_regularization 1.39e+01\n",
      "epoch: 0\n",
      "loss: 0.6682226061820984\n",
      "epoch: 1\n",
      "loss: 0.6679902672767639\n",
      "epoch: 2\n",
      "loss: 0.6677587628364563\n",
      "epoch: 3\n",
      "loss: 0.6675271987915039\n",
      "epoch: 4\n",
      "loss: 0.667299747467041\n",
      "epoch: 5\n",
      "loss: 0.6670658588409424\n",
      "epoch: 6\n",
      "loss: 0.6668360829353333\n",
      "epoch: 7\n",
      "loss: 0.6666060090065002\n",
      "epoch: 8\n",
      "loss: 0.666375994682312\n",
      "epoch: 9\n",
      "loss: 0.6661475896835327\n",
      "Lambda = 4.98e-03, selected 84 features \n",
      "10 epochs, val_objective 8.07e-01, val_loss 6.66e-01, regularization 2.83e+01, l2_regularization 1.39e+01\n",
      "epoch: 0\n",
      "loss: 0.6659163236618042\n",
      "epoch: 1\n",
      "loss: 0.6656866073608398\n",
      "epoch: 2\n",
      "loss: 0.6654583215713501\n",
      "epoch: 3\n",
      "loss: 0.6652287244796753\n",
      "epoch: 4\n",
      "loss: 0.6650000810623169\n",
      "epoch: 5\n",
      "loss: 0.6647718548774719\n",
      "epoch: 6\n",
      "loss: 0.6645453572273254\n",
      "epoch: 7\n",
      "loss: 0.6643155813217163\n",
      "epoch: 8\n",
      "loss: 0.6640905141830444\n",
      "epoch: 9\n",
      "loss: 0.6638606786727905\n",
      "Lambda = 5.10e-03, selected 84 features \n",
      "10 epochs, val_objective 8.08e-01, val_loss 6.64e-01, regularization 2.83e+01, l2_regularization 1.40e+01\n",
      "epoch: 0\n",
      "loss: 0.6636331081390381\n",
      "epoch: 1\n",
      "loss: 0.6634059548377991\n",
      "epoch: 2\n",
      "loss: 0.6631792783737183\n",
      "epoch: 3\n",
      "loss: 0.6629537343978882\n",
      "epoch: 4\n",
      "loss: 0.6627259254455566\n",
      "epoch: 5\n",
      "loss: 0.6625034213066101\n",
      "epoch: 6\n",
      "loss: 0.6622740030288696\n",
      "epoch: 7\n",
      "loss: 0.6620493531227112\n",
      "epoch: 8\n",
      "loss: 0.6618244647979736\n",
      "epoch: 9\n",
      "loss: 0.6616009473800659\n",
      "Lambda = 5.23e-03, selected 84 features \n",
      "10 epochs, val_objective 8.10e-01, val_loss 6.61e-01, regularization 2.84e+01, l2_regularization 1.40e+01\n",
      "epoch: 0\n",
      "loss: 0.6613753437995911\n",
      "epoch: 1\n",
      "loss: 0.6611492037773132\n",
      "epoch: 2\n",
      "loss: 0.6609253883361816\n",
      "epoch: 3\n",
      "loss: 0.6607002019882202\n",
      "epoch: 4\n",
      "loss: 0.6604748964309692\n",
      "epoch: 5\n",
      "loss: 0.6602498292922974\n",
      "epoch: 6\n",
      "loss: 0.660024881362915\n",
      "epoch: 7\n",
      "loss: 0.6598002910614014\n",
      "epoch: 8\n",
      "loss: 0.6595748662948608\n",
      "epoch: 9\n",
      "loss: 0.6593532562255859\n",
      "Lambda = 5.36e-03, selected 84 features \n",
      "10 epochs, val_objective 8.12e-01, val_loss 6.59e-01, regularization 2.85e+01, l2_regularization 1.40e+01\n",
      "epoch: 0\n",
      "loss: 0.6591295003890991\n",
      "epoch: 1\n",
      "loss: 0.6589070558547974\n",
      "epoch: 2\n",
      "loss: 0.6586834192276001\n",
      "epoch: 3\n",
      "loss: 0.6584607362747192\n",
      "epoch: 4\n",
      "loss: 0.6582399010658264\n",
      "epoch: 5\n",
      "loss: 0.6580184102058411\n",
      "epoch: 6\n",
      "loss: 0.657797634601593\n",
      "epoch: 7\n",
      "loss: 0.657575786113739\n",
      "epoch: 8\n",
      "loss: 0.6573547124862671\n",
      "epoch: 9\n",
      "loss: 0.6571335196495056\n",
      "Lambda = 5.49e-03, selected 84 features \n",
      "10 epochs, val_objective 8.14e-01, val_loss 6.57e-01, regularization 2.86e+01, l2_regularization 1.41e+01\n",
      "epoch: 0\n",
      "loss: 0.6569121479988098\n",
      "epoch: 1\n",
      "loss: 0.6566910743713379\n",
      "epoch: 2\n",
      "loss: 0.6564694046974182\n",
      "epoch: 3\n",
      "loss: 0.656248152256012\n",
      "epoch: 4\n",
      "loss: 0.6560286283493042\n",
      "epoch: 5\n",
      "loss: 0.6558077931404114\n",
      "epoch: 6\n",
      "loss: 0.6555886268615723\n",
      "epoch: 7\n",
      "loss: 0.6553701162338257\n",
      "epoch: 8\n",
      "loss: 0.6551499366760254\n",
      "epoch: 9\n",
      "loss: 0.6549306511878967\n",
      "Lambda = 5.63e-03, selected 84 features \n",
      "10 epochs, val_objective 8.16e-01, val_loss 6.55e-01, regularization 2.86e+01, l2_regularization 1.41e+01\n",
      "epoch: 0\n",
      "loss: 0.6547111868858337\n",
      "epoch: 1\n",
      "loss: 0.6544933319091797\n",
      "epoch: 2\n",
      "loss: 0.6542725563049316\n",
      "epoch: 3\n",
      "loss: 0.6540542244911194\n",
      "epoch: 4\n",
      "loss: 0.653836190700531\n",
      "epoch: 5\n",
      "loss: 0.6536178588867188\n",
      "epoch: 6\n",
      "loss: 0.653401255607605\n",
      "epoch: 7\n",
      "loss: 0.6531821489334106\n",
      "epoch: 8\n",
      "loss: 0.6529651284217834\n",
      "epoch: 9\n",
      "loss: 0.6527472138404846\n",
      "Lambda = 5.77e-03, selected 84 features \n",
      "10 epochs, val_objective 8.18e-01, val_loss 6.53e-01, regularization 2.87e+01, l2_regularization 1.41e+01\n",
      "epoch: 0\n",
      "loss: 0.6525299549102783\n",
      "epoch: 1\n",
      "loss: 0.6523125171661377\n",
      "epoch: 2\n",
      "loss: 0.6520991325378418\n",
      "epoch: 3\n",
      "loss: 0.6518803238868713\n",
      "epoch: 4\n",
      "loss: 0.6516643762588501\n",
      "epoch: 5\n",
      "loss: 0.6514480710029602\n",
      "epoch: 6\n",
      "loss: 0.6512336730957031\n",
      "epoch: 7\n",
      "loss: 0.6510215997695923\n",
      "epoch: 8\n",
      "loss: 0.6508037447929382\n",
      "epoch: 9\n",
      "loss: 0.6505907773971558\n",
      "Lambda = 5.92e-03, selected 84 features \n",
      "10 epochs, val_objective 8.21e-01, val_loss 6.50e-01, regularization 2.88e+01, l2_regularization 1.41e+01\n",
      "epoch: 0\n",
      "loss: 0.6503769159317017\n",
      "epoch: 1\n",
      "loss: 0.6501628160476685\n",
      "epoch: 2\n",
      "loss: 0.6499491930007935\n",
      "epoch: 3\n",
      "loss: 0.649734616279602\n",
      "epoch: 4\n",
      "loss: 0.6495203971862793\n",
      "epoch: 5\n",
      "loss: 0.6493061780929565\n",
      "epoch: 6\n",
      "loss: 0.6490916013717651\n",
      "epoch: 7\n",
      "loss: 0.6488772034645081\n",
      "epoch: 8\n",
      "loss: 0.6486627459526062\n",
      "epoch: 9\n",
      "loss: 0.6484499573707581\n",
      "Lambda = 6.07e-03, selected 84 features \n",
      "10 epochs, val_objective 8.23e-01, val_loss 6.48e-01, regularization 2.88e+01, l2_regularization 1.42e+01\n",
      "epoch: 0\n",
      "loss: 0.6482359766960144\n",
      "epoch: 1\n",
      "loss: 0.6480225920677185\n",
      "epoch: 2\n",
      "loss: 0.6478104591369629\n",
      "epoch: 3\n",
      "loss: 0.647598385810852\n",
      "epoch: 4\n",
      "loss: 0.647385835647583\n",
      "epoch: 5\n",
      "loss: 0.6471730470657349\n",
      "epoch: 6\n",
      "loss: 0.646960973739624\n",
      "epoch: 7\n",
      "loss: 0.6467495560646057\n",
      "epoch: 8\n",
      "loss: 0.6465376615524292\n",
      "epoch: 9\n",
      "loss: 0.646325945854187\n",
      "Lambda = 6.22e-03, selected 84 features \n",
      "10 epochs, val_objective 8.26e-01, val_loss 6.46e-01, regularization 2.89e+01, l2_regularization 1.42e+01\n",
      "epoch: 0\n",
      "loss: 0.6461163759231567\n",
      "epoch: 1\n",
      "loss: 0.6459037065505981\n",
      "epoch: 2\n",
      "loss: 0.6456941366195679\n",
      "epoch: 3\n",
      "loss: 0.6454843878746033\n",
      "epoch: 4\n",
      "loss: 0.64527428150177\n",
      "epoch: 5\n",
      "loss: 0.6450640559196472\n",
      "epoch: 6\n",
      "loss: 0.6448534727096558\n",
      "epoch: 7\n",
      "loss: 0.6446435451507568\n",
      "epoch: 8\n",
      "loss: 0.6444356441497803\n",
      "epoch: 9\n",
      "loss: 0.644224226474762\n",
      "Lambda = 6.37e-03, selected 84 features \n",
      "10 epochs, val_objective 8.29e-01, val_loss 6.44e-01, regularization 2.90e+01, l2_regularization 1.42e+01\n",
      "epoch: 0\n",
      "loss: 0.6440154314041138\n",
      "epoch: 1\n",
      "loss: 0.643806517124176\n",
      "epoch: 2\n",
      "loss: 0.6435980200767517\n",
      "epoch: 3\n",
      "loss: 0.6433898210525513\n",
      "epoch: 4\n",
      "loss: 0.6431803703308105\n",
      "epoch: 5\n",
      "loss: 0.642971396446228\n",
      "epoch: 6\n",
      "loss: 0.6427648663520813\n",
      "epoch: 7\n",
      "loss: 0.6425576210021973\n",
      "epoch: 8\n",
      "loss: 0.6423468589782715\n",
      "epoch: 9\n",
      "loss: 0.6421400308609009\n",
      "Lambda = 6.53e-03, selected 84 features \n",
      "10 epochs, val_objective 8.32e-01, val_loss 6.42e-01, regularization 2.90e+01, l2_regularization 1.43e+01\n",
      "epoch: 0\n",
      "loss: 0.6419323682785034\n",
      "epoch: 1\n",
      "loss: 0.6417261362075806\n",
      "epoch: 2\n",
      "loss: 0.6415179371833801\n",
      "epoch: 3\n",
      "loss: 0.6413110494613647\n",
      "epoch: 4\n",
      "loss: 0.6411045789718628\n",
      "epoch: 5\n",
      "loss: 0.6408982872962952\n",
      "epoch: 6\n",
      "loss: 0.6406922936439514\n",
      "epoch: 7\n",
      "loss: 0.6404852867126465\n",
      "epoch: 8\n",
      "loss: 0.64028000831604\n",
      "epoch: 9\n",
      "loss: 0.6400752067565918\n",
      "Lambda = 6.69e-03, selected 84 features \n",
      "10 epochs, val_objective 8.35e-01, val_loss 6.40e-01, regularization 2.91e+01, l2_regularization 1.43e+01\n",
      "epoch: 0\n",
      "loss: 0.6398705244064331\n",
      "epoch: 1\n",
      "loss: 0.6396693587303162\n",
      "epoch: 2\n",
      "loss: 0.6394603252410889\n",
      "epoch: 3\n",
      "loss: 0.6392555236816406\n",
      "epoch: 4\n",
      "loss: 0.6390508413314819\n",
      "epoch: 5\n",
      "loss: 0.6388461589813232\n",
      "epoch: 6\n",
      "loss: 0.6386410593986511\n",
      "epoch: 7\n",
      "loss: 0.6384377479553223\n",
      "epoch: 8\n",
      "loss: 0.6382333040237427\n",
      "epoch: 9\n",
      "loss: 0.6380305290222168\n",
      "Lambda = 6.86e-03, selected 84 features \n",
      "10 epochs, val_objective 8.38e-01, val_loss 6.38e-01, regularization 2.92e+01, l2_regularization 1.43e+01\n",
      "epoch: 0\n",
      "loss: 0.6378269791603088\n",
      "epoch: 1\n",
      "loss: 0.6376239061355591\n",
      "epoch: 2\n",
      "loss: 0.6374242305755615\n",
      "epoch: 3\n",
      "loss: 0.6372183561325073\n",
      "epoch: 4\n",
      "loss: 0.6370158195495605\n",
      "epoch: 5\n",
      "loss: 0.6368130445480347\n",
      "epoch: 6\n",
      "loss: 0.6366100311279297\n",
      "epoch: 7\n",
      "loss: 0.6364072561264038\n",
      "epoch: 8\n",
      "loss: 0.6362062692642212\n",
      "epoch: 9\n",
      "loss: 0.6360023021697998\n",
      "Lambda = 7.03e-03, selected 84 features \n",
      "10 epochs, val_objective 8.41e-01, val_loss 6.36e-01, regularization 2.92e+01, l2_regularization 1.44e+01\n",
      "epoch: 0\n",
      "loss: 0.635800838470459\n",
      "epoch: 1\n",
      "loss: 0.6355999112129211\n",
      "epoch: 2\n",
      "loss: 0.6353977918624878\n",
      "epoch: 3\n",
      "loss: 0.6351967453956604\n",
      "epoch: 4\n",
      "loss: 0.6349951028823853\n",
      "epoch: 5\n",
      "loss: 0.6347939968109131\n",
      "epoch: 6\n",
      "loss: 0.6345923542976379\n",
      "epoch: 7\n",
      "loss: 0.634393572807312\n",
      "epoch: 8\n",
      "loss: 0.63419109582901\n",
      "epoch: 9\n",
      "loss: 0.6339906454086304\n",
      "Lambda = 7.21e-03, selected 84 features \n",
      "10 epochs, val_objective 8.45e-01, val_loss 6.34e-01, regularization 2.93e+01, l2_regularization 1.44e+01\n",
      "epoch: 0\n",
      "loss: 0.6337909698486328\n",
      "epoch: 1\n",
      "loss: 0.6335912942886353\n",
      "epoch: 2\n",
      "loss: 0.6333922743797302\n",
      "epoch: 3\n",
      "loss: 0.6331931948661804\n",
      "epoch: 4\n",
      "loss: 0.6329941153526306\n",
      "epoch: 5\n",
      "loss: 0.6327953934669495\n",
      "epoch: 6\n",
      "loss: 0.6326007843017578\n",
      "epoch: 7\n",
      "loss: 0.6323983669281006\n",
      "epoch: 8\n",
      "loss: 0.6322001814842224\n",
      "epoch: 9\n",
      "loss: 0.6320018768310547\n",
      "Lambda = 7.39e-03, selected 84 features \n",
      "10 epochs, val_objective 8.49e-01, val_loss 6.32e-01, regularization 2.93e+01, l2_regularization 1.44e+01\n",
      "epoch: 0\n",
      "loss: 0.6318055391311646\n",
      "epoch: 1\n",
      "loss: 0.6316068768501282\n",
      "epoch: 2\n",
      "loss: 0.6314085721969604\n",
      "epoch: 3\n",
      "loss: 0.6312112212181091\n",
      "epoch: 4\n",
      "loss: 0.6310136914253235\n",
      "epoch: 5\n",
      "loss: 0.630816638469696\n",
      "epoch: 6\n",
      "loss: 0.6306194067001343\n",
      "epoch: 7\n",
      "loss: 0.630422055721283\n",
      "epoch: 8\n",
      "loss: 0.6302248239517212\n",
      "epoch: 9\n",
      "loss: 0.6300276517868042\n",
      "Lambda = 7.57e-03, selected 84 features \n",
      "10 epochs, val_objective 8.53e-01, val_loss 6.30e-01, regularization 2.94e+01, l2_regularization 1.44e+01\n",
      "epoch: 0\n",
      "loss: 0.6298305988311768\n",
      "epoch: 1\n",
      "loss: 0.629635751247406\n",
      "epoch: 2\n",
      "loss: 0.6294395923614502\n",
      "epoch: 3\n",
      "loss: 0.6292431354522705\n",
      "epoch: 4\n",
      "loss: 0.6290485858917236\n",
      "epoch: 5\n",
      "loss: 0.628853440284729\n",
      "epoch: 6\n",
      "loss: 0.6286575794219971\n",
      "epoch: 7\n",
      "loss: 0.6284620761871338\n",
      "epoch: 8\n",
      "loss: 0.6282665133476257\n",
      "epoch: 9\n",
      "loss: 0.6280717253684998\n",
      "Lambda = 7.76e-03, selected 84 features \n",
      "10 epochs, val_objective 8.57e-01, val_loss 6.28e-01, regularization 2.95e+01, l2_regularization 1.45e+01\n",
      "epoch: 0\n",
      "loss: 0.6278754472732544\n",
      "epoch: 1\n",
      "loss: 0.6276811361312866\n",
      "epoch: 2\n",
      "loss: 0.627487063407898\n",
      "epoch: 3\n",
      "loss: 0.6272929310798645\n",
      "epoch: 4\n",
      "loss: 0.6271008253097534\n",
      "epoch: 5\n",
      "loss: 0.6269056797027588\n",
      "epoch: 6\n",
      "loss: 0.626712441444397\n",
      "epoch: 7\n",
      "loss: 0.6265191435813904\n",
      "epoch: 8\n",
      "loss: 0.6263260841369629\n",
      "epoch: 9\n",
      "loss: 0.626133382320404\n",
      "Lambda = 7.96e-03, selected 84 features \n",
      "10 epochs, val_objective 8.61e-01, val_loss 6.26e-01, regularization 2.95e+01, l2_regularization 1.45e+01\n",
      "epoch: 0\n",
      "loss: 0.6259397864341736\n",
      "epoch: 1\n",
      "loss: 0.6257485747337341\n",
      "epoch: 2\n",
      "loss: 0.6255544424057007\n",
      "epoch: 3\n",
      "loss: 0.625362753868103\n",
      "epoch: 4\n",
      "loss: 0.6251707077026367\n",
      "epoch: 5\n",
      "loss: 0.6249785423278809\n",
      "epoch: 6\n",
      "loss: 0.6247860193252563\n",
      "epoch: 7\n",
      "loss: 0.62459397315979\n",
      "epoch: 8\n",
      "loss: 0.6244040727615356\n",
      "epoch: 9\n",
      "loss: 0.6242109537124634\n",
      "Lambda = 8.16e-03, selected 84 features \n",
      "10 epochs, val_objective 8.65e-01, val_loss 6.24e-01, regularization 2.96e+01, l2_regularization 1.45e+01\n",
      "epoch: 0\n",
      "loss: 0.6240193843841553\n",
      "epoch: 1\n",
      "loss: 0.6238279342651367\n",
      "epoch: 2\n",
      "loss: 0.6236376762390137\n",
      "epoch: 3\n",
      "loss: 0.6234457492828369\n",
      "epoch: 4\n",
      "loss: 0.6232550144195557\n",
      "epoch: 5\n",
      "loss: 0.623063862323761\n",
      "epoch: 6\n",
      "loss: 0.6228780150413513\n",
      "epoch: 7\n",
      "loss: 0.6226833462715149\n",
      "epoch: 8\n",
      "loss: 0.6224941611289978\n",
      "epoch: 9\n",
      "loss: 0.6223052740097046\n",
      "Lambda = 8.36e-03, selected 84 features \n",
      "10 epochs, val_objective 8.70e-01, val_loss 6.22e-01, regularization 2.97e+01, l2_regularization 1.46e+01\n",
      "epoch: 0\n",
      "loss: 0.6221188306808472\n",
      "epoch: 1\n",
      "loss: 0.6219266653060913\n",
      "epoch: 2\n",
      "loss: 0.6217405200004578\n",
      "epoch: 3\n",
      "loss: 0.6215499043464661\n",
      "epoch: 4\n",
      "loss: 0.6213617920875549\n",
      "epoch: 5\n",
      "loss: 0.6211743950843811\n",
      "epoch: 6\n",
      "loss: 0.6209860444068909\n",
      "epoch: 7\n",
      "loss: 0.6207962036132812\n",
      "epoch: 8\n",
      "loss: 0.620608389377594\n",
      "epoch: 9\n",
      "loss: 0.6204207539558411\n",
      "Lambda = 8.57e-03, selected 84 features \n",
      "10 epochs, val_objective 8.75e-01, val_loss 6.20e-01, regularization 2.97e+01, l2_regularization 1.46e+01\n",
      "epoch: 0\n",
      "loss: 0.6202328205108643\n",
      "epoch: 1\n",
      "loss: 0.6200460195541382\n",
      "epoch: 2\n",
      "loss: 0.619857668876648\n",
      "epoch: 3\n",
      "loss: 0.6196709871292114\n",
      "epoch: 4\n",
      "loss: 0.6194841265678406\n",
      "epoch: 5\n",
      "loss: 0.619296669960022\n",
      "epoch: 6\n",
      "loss: 0.6191092729568481\n",
      "epoch: 7\n",
      "loss: 0.6189216375350952\n",
      "epoch: 8\n",
      "loss: 0.6187341213226318\n",
      "epoch: 9\n",
      "loss: 0.6185466051101685\n",
      "Lambda = 8.78e-03, selected 84 features \n",
      "10 epochs, val_objective 8.80e-01, val_loss 6.18e-01, regularization 2.98e+01, l2_regularization 1.46e+01\n",
      "epoch: 0\n",
      "loss: 0.6183596849441528\n",
      "epoch: 1\n",
      "loss: 0.6181747317314148\n",
      "epoch: 2\n",
      "loss: 0.6179865598678589\n",
      "epoch: 3\n",
      "loss: 0.6178010702133179\n",
      "epoch: 4\n",
      "loss: 0.6176185607910156\n",
      "epoch: 5\n",
      "loss: 0.617429792881012\n",
      "epoch: 6\n",
      "loss: 0.6172456741333008\n",
      "epoch: 7\n",
      "loss: 0.6170624494552612\n",
      "epoch: 8\n",
      "loss: 0.6168760061264038\n",
      "epoch: 9\n",
      "loss: 0.6166913509368896\n",
      "Lambda = 9.00e-03, selected 84 features \n",
      "10 epochs, val_objective 8.85e-01, val_loss 6.17e-01, regularization 2.99e+01, l2_regularization 1.46e+01\n",
      "epoch: 0\n",
      "loss: 0.6165066957473755\n",
      "epoch: 1\n",
      "loss: 0.6163218021392822\n",
      "epoch: 2\n",
      "loss: 0.6161389350891113\n",
      "epoch: 3\n",
      "loss: 0.6159530878067017\n",
      "epoch: 4\n",
      "loss: 0.6157706379890442\n",
      "epoch: 5\n",
      "loss: 0.6155848503112793\n",
      "epoch: 6\n",
      "loss: 0.6154016256332397\n",
      "epoch: 7\n",
      "loss: 0.6152187585830688\n",
      "epoch: 8\n",
      "loss: 0.615034818649292\n",
      "epoch: 9\n",
      "loss: 0.6148507595062256\n",
      "Lambda = 9.23e-03, selected 84 features \n",
      "10 epochs, val_objective 8.91e-01, val_loss 6.15e-01, regularization 2.99e+01, l2_regularization 1.47e+01\n",
      "epoch: 0\n",
      "loss: 0.6146690845489502\n",
      "epoch: 1\n",
      "loss: 0.6144881248474121\n",
      "epoch: 2\n",
      "loss: 0.6143014430999756\n",
      "epoch: 3\n",
      "loss: 0.6141188144683838\n",
      "epoch: 4\n",
      "loss: 0.613938570022583\n",
      "epoch: 5\n",
      "loss: 0.613755464553833\n",
      "epoch: 6\n",
      "loss: 0.6135741472244263\n",
      "epoch: 7\n",
      "loss: 0.6133918762207031\n",
      "epoch: 8\n",
      "loss: 0.6132098436355591\n",
      "epoch: 9\n",
      "loss: 0.6130279302597046\n",
      "Lambda = 9.46e-03, selected 84 features \n",
      "10 epochs, val_objective 8.96e-01, val_loss 6.13e-01, regularization 3.00e+01, l2_regularization 1.47e+01\n",
      "epoch: 0\n",
      "loss: 0.6128455400466919\n",
      "epoch: 1\n",
      "loss: 0.6126633286476135\n",
      "epoch: 2\n",
      "loss: 0.6124837398529053\n",
      "epoch: 3\n",
      "loss: 0.6123014688491821\n",
      "epoch: 4\n",
      "loss: 0.6121209859848022\n",
      "epoch: 5\n",
      "loss: 0.6119400262832642\n",
      "epoch: 6\n",
      "loss: 0.6117590665817261\n",
      "epoch: 7\n",
      "loss: 0.6115790605545044\n",
      "epoch: 8\n",
      "loss: 0.6113977432250977\n",
      "epoch: 9\n",
      "loss: 0.6112208962440491\n",
      "Lambda = 9.70e-03, selected 84 features \n",
      "10 epochs, val_objective 9.02e-01, val_loss 6.11e-01, regularization 3.00e+01, l2_regularization 1.47e+01\n",
      "epoch: 0\n",
      "loss: 0.6110379695892334\n",
      "epoch: 1\n",
      "loss: 0.6108579635620117\n",
      "epoch: 2\n",
      "loss: 0.6106787919998169\n",
      "epoch: 3\n",
      "loss: 0.6104998588562012\n",
      "epoch: 4\n",
      "loss: 0.6103197336196899\n",
      "epoch: 5\n",
      "loss: 0.6101405620574951\n",
      "epoch: 6\n",
      "loss: 0.6099607944488525\n",
      "epoch: 7\n",
      "loss: 0.6097825765609741\n",
      "epoch: 8\n",
      "loss: 0.6096027493476868\n",
      "epoch: 9\n",
      "loss: 0.6094248294830322\n",
      "Lambda = 9.94e-03, selected 84 features \n",
      "10 epochs, val_objective 9.08e-01, val_loss 6.09e-01, regularization 3.01e+01, l2_regularization 1.48e+01\n",
      "epoch: 0\n",
      "loss: 0.609245777130127\n",
      "epoch: 1\n",
      "loss: 0.6090693473815918\n",
      "epoch: 2\n",
      "loss: 0.6088910102844238\n",
      "epoch: 3\n",
      "loss: 0.6087123155593872\n",
      "epoch: 4\n",
      "loss: 0.6085348725318909\n",
      "epoch: 5\n",
      "loss: 0.6083574891090393\n",
      "epoch: 6\n",
      "loss: 0.6081799268722534\n",
      "epoch: 7\n",
      "loss: 0.6080033779144287\n",
      "epoch: 8\n",
      "loss: 0.6078249216079712\n",
      "epoch: 9\n",
      "loss: 0.6076478362083435\n",
      "Lambda = 1.02e-02, selected 84 features \n",
      "10 epochs, val_objective 9.15e-01, val_loss 6.07e-01, regularization 3.02e+01, l2_regularization 1.48e+01\n",
      "epoch: 0\n",
      "loss: 0.6074705123901367\n",
      "epoch: 1\n",
      "loss: 0.607294499874115\n",
      "epoch: 2\n",
      "loss: 0.6071165800094604\n",
      "epoch: 3\n",
      "loss: 0.6069405674934387\n",
      "epoch: 4\n",
      "loss: 0.606765627861023\n",
      "epoch: 5\n",
      "loss: 0.6065887212753296\n",
      "epoch: 6\n",
      "loss: 0.6064134836196899\n",
      "epoch: 7\n",
      "loss: 0.6062377691268921\n",
      "epoch: 8\n",
      "loss: 0.6060622930526733\n",
      "epoch: 9\n",
      "loss: 0.605886697769165\n",
      "Lambda = 1.04e-02, selected 84 features \n",
      "10 epochs, val_objective 9.21e-01, val_loss 6.06e-01, regularization 3.02e+01, l2_regularization 1.48e+01\n",
      "epoch: 0\n",
      "loss: 0.605710506439209\n",
      "epoch: 1\n",
      "loss: 0.6055350303649902\n",
      "epoch: 2\n",
      "loss: 0.6053601503372192\n",
      "epoch: 3\n",
      "loss: 0.6051839590072632\n",
      "epoch: 4\n",
      "loss: 0.6050093770027161\n",
      "epoch: 5\n",
      "loss: 0.6048344373703003\n",
      "epoch: 6\n",
      "loss: 0.6046595573425293\n",
      "epoch: 7\n",
      "loss: 0.6044855713844299\n",
      "epoch: 8\n",
      "loss: 0.6043136119842529\n",
      "epoch: 9\n",
      "loss: 0.6041370630264282\n",
      "Lambda = 1.07e-02, selected 84 features \n",
      "10 epochs, val_objective 9.28e-01, val_loss 6.04e-01, regularization 3.03e+01, l2_regularization 1.48e+01\n",
      "epoch: 0\n",
      "loss: 0.6039631962776184\n",
      "epoch: 1\n",
      "loss: 0.6037912368774414\n",
      "epoch: 2\n",
      "loss: 0.6036179661750793\n",
      "epoch: 3\n",
      "loss: 0.6034467220306396\n",
      "epoch: 4\n",
      "loss: 0.6032761335372925\n",
      "epoch: 5\n",
      "loss: 0.6031031608581543\n",
      "epoch: 6\n",
      "loss: 0.6029314994812012\n",
      "epoch: 7\n",
      "loss: 0.6027572154998779\n",
      "epoch: 8\n",
      "loss: 0.6025835275650024\n",
      "epoch: 9\n",
      "loss: 0.6024104356765747\n",
      "Lambda = 1.10e-02, selected 84 features \n",
      "10 epochs, val_objective 9.35e-01, val_loss 6.02e-01, regularization 3.03e+01, l2_regularization 1.49e+01\n",
      "epoch: 0\n",
      "loss: 0.6022377014160156\n",
      "epoch: 1\n",
      "loss: 0.6020681858062744\n",
      "epoch: 2\n",
      "loss: 0.6018941402435303\n",
      "epoch: 3\n",
      "loss: 0.6017224788665771\n",
      "epoch: 4\n",
      "loss: 0.6015509366989136\n",
      "epoch: 5\n",
      "loss: 0.6013796329498291\n",
      "epoch: 6\n",
      "loss: 0.601206362247467\n",
      "epoch: 7\n",
      "loss: 0.6010358333587646\n",
      "epoch: 8\n",
      "loss: 0.6008625030517578\n",
      "epoch: 9\n",
      "loss: 0.6006917357444763\n",
      "Lambda = 1.12e-02, selected 84 features \n",
      "10 epochs, val_objective 9.42e-01, val_loss 6.01e-01, regularization 3.04e+01, l2_regularization 1.49e+01\n",
      "epoch: 0\n",
      "loss: 0.6005204916000366\n",
      "epoch: 1\n",
      "loss: 0.6003495454788208\n",
      "epoch: 2\n",
      "loss: 0.6001819968223572\n",
      "epoch: 3\n",
      "loss: 0.6000086069107056\n",
      "epoch: 4\n",
      "loss: 0.5998381972312927\n",
      "epoch: 5\n",
      "loss: 0.599668025970459\n",
      "epoch: 6\n",
      "loss: 0.5994988083839417\n",
      "epoch: 7\n",
      "loss: 0.5993270874023438\n",
      "epoch: 8\n",
      "loss: 0.5991576910018921\n",
      "epoch: 9\n",
      "loss: 0.5989874601364136\n",
      "Lambda = 1.15e-02, selected 84 features \n",
      "10 epochs, val_objective 9.50e-01, val_loss 5.99e-01, regularization 3.05e+01, l2_regularization 1.49e+01\n",
      "epoch: 0\n",
      "loss: 0.5988171696662903\n",
      "epoch: 1\n",
      "loss: 0.5986487865447998\n",
      "epoch: 2\n",
      "loss: 0.5984785556793213\n",
      "epoch: 3\n",
      "loss: 0.5983105301856995\n",
      "epoch: 4\n",
      "loss: 0.5981431007385254\n",
      "epoch: 5\n",
      "loss: 0.5979748368263245\n",
      "epoch: 6\n",
      "loss: 0.597806453704834\n",
      "epoch: 7\n",
      "loss: 0.5976371765136719\n",
      "epoch: 8\n",
      "loss: 0.5974684953689575\n",
      "epoch: 9\n",
      "loss: 0.5972999930381775\n",
      "Lambda = 1.18e-02, selected 84 features \n",
      "10 epochs, val_objective 9.58e-01, val_loss 5.97e-01, regularization 3.05e+01, l2_regularization 1.49e+01\n",
      "epoch: 0\n",
      "loss: 0.5971329808235168\n",
      "epoch: 1\n",
      "loss: 0.5969641804695129\n",
      "epoch: 2\n",
      "loss: 0.5967968702316284\n",
      "epoch: 3\n",
      "loss: 0.5966286659240723\n",
      "epoch: 4\n",
      "loss: 0.5964611768722534\n",
      "epoch: 5\n",
      "loss: 0.5962938666343689\n",
      "epoch: 6\n",
      "loss: 0.5961260795593262\n",
      "epoch: 7\n",
      "loss: 0.5959582924842834\n",
      "epoch: 8\n",
      "loss: 0.5957911014556885\n",
      "epoch: 9\n",
      "loss: 0.595623254776001\n",
      "Lambda = 1.21e-02, selected 84 features \n",
      "10 epochs, val_objective 9.66e-01, val_loss 5.95e-01, regularization 3.06e+01, l2_regularization 1.50e+01\n",
      "epoch: 0\n",
      "loss: 0.5954561233520508\n",
      "epoch: 1\n",
      "loss: 0.5952898263931274\n",
      "epoch: 2\n",
      "loss: 0.5951230525970459\n",
      "epoch: 3\n",
      "loss: 0.5949562191963196\n",
      "epoch: 4\n",
      "loss: 0.5947917699813843\n",
      "epoch: 5\n",
      "loss: 0.5946242809295654\n",
      "epoch: 6\n",
      "loss: 0.5944584608078003\n",
      "epoch: 7\n",
      "loss: 0.594292163848877\n",
      "epoch: 8\n",
      "loss: 0.5941263437271118\n",
      "epoch: 9\n",
      "loss: 0.5939601063728333\n",
      "Lambda = 1.24e-02, selected 84 features \n",
      "10 epochs, val_objective 9.74e-01, val_loss 5.94e-01, regularization 3.06e+01, l2_regularization 1.50e+01\n",
      "epoch: 0\n",
      "loss: 0.5937967300415039\n",
      "epoch: 1\n",
      "loss: 0.5936291813850403\n",
      "epoch: 2\n",
      "loss: 0.5934659242630005\n",
      "epoch: 3\n",
      "loss: 0.5933018922805786\n",
      "epoch: 4\n",
      "loss: 0.5931370854377747\n",
      "epoch: 5\n",
      "loss: 0.5929734706878662\n",
      "epoch: 6\n",
      "loss: 0.5928092002868652\n",
      "epoch: 7\n",
      "loss: 0.5926445722579956\n",
      "epoch: 8\n",
      "loss: 0.5924800634384155\n",
      "epoch: 9\n",
      "loss: 0.5923160314559937\n",
      "Lambda = 1.27e-02, selected 84 features \n",
      "10 epochs, val_objective 9.82e-01, val_loss 5.92e-01, regularization 3.07e+01, l2_regularization 1.50e+01\n",
      "epoch: 0\n",
      "loss: 0.5921535491943359\n",
      "epoch: 1\n",
      "loss: 0.5919898748397827\n",
      "epoch: 2\n",
      "loss: 0.5918271541595459\n",
      "epoch: 3\n",
      "loss: 0.5916647911071777\n",
      "epoch: 4\n",
      "loss: 0.5915015935897827\n",
      "epoch: 5\n",
      "loss: 0.5913382768630981\n",
      "epoch: 6\n",
      "loss: 0.5911747813224792\n",
      "epoch: 7\n",
      "loss: 0.5910111665725708\n",
      "epoch: 8\n",
      "loss: 0.5908477902412415\n",
      "epoch: 9\n",
      "loss: 0.5906844139099121\n",
      "Lambda = 1.30e-02, selected 84 features \n",
      "10 epochs, val_objective 9.91e-01, val_loss 5.91e-01, regularization 3.07e+01, l2_regularization 1.50e+01\n",
      "epoch: 0\n",
      "loss: 0.5905212163925171\n",
      "epoch: 1\n",
      "loss: 0.5903628468513489\n",
      "epoch: 2\n",
      "loss: 0.5901981592178345\n",
      "epoch: 3\n",
      "loss: 0.5900347232818604\n",
      "epoch: 4\n",
      "loss: 0.5898728370666504\n",
      "epoch: 5\n",
      "loss: 0.5897111892700195\n",
      "epoch: 6\n",
      "loss: 0.5895493030548096\n",
      "epoch: 7\n",
      "loss: 0.589387834072113\n",
      "epoch: 8\n",
      "loss: 0.5892266035079956\n",
      "epoch: 9\n",
      "loss: 0.5890653133392334\n",
      "Lambda = 1.34e-02, selected 84 features \n",
      "10 epochs, val_objective 1.00e+00, val_loss 5.89e-01, regularization 3.08e+01, l2_regularization 1.51e+01\n",
      "epoch: 0\n",
      "loss: 0.5889028310775757\n",
      "epoch: 1\n",
      "loss: 0.5887426137924194\n",
      "epoch: 2\n",
      "loss: 0.5885815024375916\n",
      "epoch: 3\n",
      "loss: 0.5884202718734741\n",
      "epoch: 4\n",
      "loss: 0.588259220123291\n",
      "epoch: 5\n",
      "loss: 0.5881004333496094\n",
      "epoch: 6\n",
      "loss: 0.5879391431808472\n",
      "epoch: 7\n",
      "loss: 0.5877795219421387\n",
      "epoch: 8\n",
      "loss: 0.5876206159591675\n",
      "epoch: 9\n",
      "loss: 0.5874608755111694\n",
      "Lambda = 1.37e-02, selected 84 features \n",
      "10 epochs, val_objective 1.01e+00, val_loss 5.87e-01, regularization 3.08e+01, l2_regularization 1.51e+01\n",
      "epoch: 0\n",
      "loss: 0.5873016715049744\n",
      "epoch: 1\n",
      "loss: 0.5871411561965942\n",
      "epoch: 2\n",
      "loss: 0.586982250213623\n",
      "epoch: 3\n",
      "loss: 0.5868220329284668\n",
      "epoch: 4\n",
      "loss: 0.586664080619812\n",
      "epoch: 5\n",
      "loss: 0.5865042209625244\n",
      "epoch: 6\n",
      "loss: 0.5863451957702637\n",
      "epoch: 7\n",
      "loss: 0.5861867666244507\n",
      "epoch: 8\n",
      "loss: 0.5860288143157959\n",
      "epoch: 9\n",
      "loss: 0.5858696103096008\n",
      "Lambda = 1.40e-02, selected 84 features \n",
      "10 epochs, val_objective 1.02e+00, val_loss 5.86e-01, regularization 3.09e+01, l2_regularization 1.51e+01\n",
      "epoch: 0\n",
      "loss: 0.5857112407684326\n",
      "epoch: 1\n",
      "loss: 0.5855534076690674\n",
      "epoch: 2\n",
      "loss: 0.585395336151123\n",
      "epoch: 3\n",
      "loss: 0.5852367877960205\n",
      "epoch: 4\n",
      "loss: 0.5850790739059448\n",
      "epoch: 5\n",
      "loss: 0.5849219560623169\n",
      "epoch: 6\n",
      "loss: 0.5847622752189636\n",
      "epoch: 7\n",
      "loss: 0.5846049785614014\n",
      "epoch: 8\n",
      "loss: 0.5844478607177734\n",
      "epoch: 9\n",
      "loss: 0.58428955078125\n",
      "Lambda = 1.44e-02, selected 84 features \n",
      "10 epochs, val_objective 1.03e+00, val_loss 5.84e-01, regularization 3.10e+01, l2_regularization 1.51e+01\n",
      "epoch: 0\n",
      "loss: 0.5841322541236877\n",
      "epoch: 1\n",
      "loss: 0.5839766263961792\n",
      "epoch: 2\n",
      "loss: 0.5838186740875244\n",
      "epoch: 3\n",
      "loss: 0.583662211894989\n",
      "epoch: 4\n",
      "loss: 0.5835058689117432\n",
      "epoch: 5\n",
      "loss: 0.5833501219749451\n",
      "epoch: 6\n",
      "loss: 0.5831940770149231\n",
      "epoch: 7\n",
      "loss: 0.5830374956130981\n",
      "epoch: 8\n",
      "loss: 0.5828808546066284\n",
      "epoch: 9\n",
      "loss: 0.5827258825302124\n",
      "Lambda = 1.48e-02, selected 84 features \n",
      "10 epochs, val_objective 1.04e+00, val_loss 5.83e-01, regularization 3.10e+01, l2_regularization 1.52e+01\n",
      "epoch: 0\n",
      "loss: 0.5825696587562561\n",
      "epoch: 1\n",
      "loss: 0.5824140310287476\n",
      "epoch: 2\n",
      "loss: 0.5822600722312927\n",
      "epoch: 3\n",
      "loss: 0.5821042060852051\n",
      "epoch: 4\n",
      "loss: 0.5819499492645264\n",
      "epoch: 5\n",
      "loss: 0.5817956924438477\n",
      "epoch: 6\n",
      "loss: 0.5816407203674316\n",
      "epoch: 7\n",
      "loss: 0.5814867615699768\n",
      "epoch: 8\n",
      "loss: 0.5813308954238892\n",
      "epoch: 9\n",
      "loss: 0.5811764597892761\n",
      "Lambda = 1.51e-02, selected 84 features \n",
      "10 epochs, val_objective 1.05e+00, val_loss 5.81e-01, regularization 3.11e+01, l2_regularization 1.52e+01\n",
      "epoch: 0\n",
      "loss: 0.5810222625732422\n",
      "epoch: 1\n",
      "loss: 0.5808681845664978\n",
      "epoch: 2\n",
      "loss: 0.5807138681411743\n",
      "epoch: 3\n",
      "loss: 0.5805608034133911\n",
      "epoch: 4\n",
      "loss: 0.5804051756858826\n",
      "epoch: 5\n",
      "loss: 0.5802513360977173\n",
      "epoch: 6\n",
      "loss: 0.5800991654396057\n",
      "epoch: 7\n",
      "loss: 0.5799447298049927\n",
      "epoch: 8\n",
      "loss: 0.5797903537750244\n",
      "epoch: 9\n",
      "loss: 0.5796375274658203\n",
      "Lambda = 1.55e-02, selected 84 features \n",
      "10 epochs, val_objective 1.06e+00, val_loss 5.79e-01, regularization 3.11e+01, l2_regularization 1.52e+01\n",
      "epoch: 0\n",
      "loss: 0.5794841051101685\n",
      "epoch: 1\n",
      "loss: 0.57933109998703\n",
      "epoch: 2\n",
      "loss: 0.5791797637939453\n",
      "epoch: 3\n",
      "loss: 0.5790258646011353\n",
      "epoch: 4\n",
      "loss: 0.5788726210594177\n",
      "epoch: 5\n",
      "loss: 0.5787197351455688\n",
      "epoch: 6\n",
      "loss: 0.5785674452781677\n",
      "epoch: 7\n",
      "loss: 0.5784146189689636\n",
      "epoch: 8\n",
      "loss: 0.57826167345047\n",
      "epoch: 9\n",
      "loss: 0.5781091451644897\n",
      "Lambda = 1.59e-02, selected 84 features \n",
      "10 epochs, val_objective 1.07e+00, val_loss 5.78e-01, regularization 3.12e+01, l2_regularization 1.53e+01\n",
      "epoch: 0\n",
      "loss: 0.5779633522033691\n",
      "epoch: 1\n",
      "loss: 0.5778063535690308\n",
      "epoch: 2\n",
      "loss: 0.5776556134223938\n",
      "epoch: 3\n",
      "loss: 0.5775046348571777\n",
      "epoch: 4\n",
      "loss: 0.5773541331291199\n",
      "epoch: 5\n",
      "loss: 0.5772035717964172\n",
      "epoch: 6\n",
      "loss: 0.5770529508590698\n",
      "epoch: 7\n",
      "loss: 0.5769015550613403\n",
      "epoch: 8\n",
      "loss: 0.5767503976821899\n",
      "epoch: 9\n",
      "loss: 0.5765997171401978\n",
      "Lambda = 1.63e-02, selected 84 features \n",
      "10 epochs, val_objective 1.09e+00, val_loss 5.76e-01, regularization 3.12e+01, l2_regularization 1.53e+01\n",
      "epoch: 0\n",
      "loss: 0.5764486789703369\n",
      "epoch: 1\n",
      "loss: 0.576298177242279\n",
      "epoch: 2\n",
      "loss: 0.5761492252349854\n",
      "epoch: 3\n",
      "loss: 0.5759977102279663\n",
      "epoch: 4\n",
      "loss: 0.5758487582206726\n",
      "epoch: 5\n",
      "loss: 0.5756979584693909\n",
      "epoch: 6\n",
      "loss: 0.5755483508110046\n",
      "epoch: 7\n",
      "loss: 0.5753992795944214\n",
      "epoch: 8\n",
      "loss: 0.575249433517456\n",
      "epoch: 9\n",
      "loss: 0.5751004219055176\n",
      "Lambda = 1.67e-02, selected 84 features \n",
      "10 epochs, val_objective 1.10e+00, val_loss 5.75e-01, regularization 3.13e+01, l2_regularization 1.53e+01\n",
      "epoch: 0\n",
      "loss: 0.5749505162239075\n",
      "epoch: 1\n",
      "loss: 0.5748008489608765\n",
      "epoch: 2\n",
      "loss: 0.5746510624885559\n",
      "epoch: 3\n",
      "loss: 0.5745015740394592\n",
      "epoch: 4\n",
      "loss: 0.574353814125061\n",
      "epoch: 5\n",
      "loss: 0.5742033123970032\n",
      "epoch: 6\n",
      "loss: 0.574054479598999\n",
      "epoch: 7\n",
      "loss: 0.5739076137542725\n",
      "epoch: 8\n",
      "loss: 0.5737582445144653\n",
      "epoch: 9\n",
      "loss: 0.5736104846000671\n",
      "Lambda = 1.71e-02, selected 84 features \n",
      "10 epochs, val_objective 1.11e+00, val_loss 5.73e-01, regularization 3.13e+01, l2_regularization 1.53e+01\n",
      "epoch: 0\n",
      "loss: 0.5734625458717346\n",
      "epoch: 1\n",
      "loss: 0.5733165740966797\n",
      "epoch: 2\n",
      "loss: 0.5731685161590576\n",
      "epoch: 3\n",
      "loss: 0.5730217695236206\n",
      "epoch: 4\n",
      "loss: 0.5728745460510254\n",
      "epoch: 5\n",
      "loss: 0.5727272629737854\n",
      "epoch: 6\n",
      "loss: 0.5725803375244141\n",
      "epoch: 7\n",
      "loss: 0.5724327564239502\n",
      "epoch: 8\n",
      "loss: 0.5722854137420654\n",
      "epoch: 9\n",
      "loss: 0.5721388459205627\n",
      "Lambda = 1.75e-02, selected 84 features \n",
      "10 epochs, val_objective 1.12e+00, val_loss 5.72e-01, regularization 3.14e+01, l2_regularization 1.53e+01\n",
      "epoch: 0\n",
      "loss: 0.5719910860061646\n",
      "epoch: 1\n",
      "loss: 0.571845531463623\n",
      "epoch: 2\n",
      "loss: 0.571698784828186\n",
      "epoch: 3\n",
      "loss: 0.5715528726577759\n",
      "epoch: 4\n",
      "loss: 0.5714057683944702\n",
      "epoch: 5\n",
      "loss: 0.5712602138519287\n",
      "epoch: 6\n",
      "loss: 0.5711134672164917\n",
      "epoch: 7\n",
      "loss: 0.570967435836792\n",
      "epoch: 8\n",
      "loss: 0.5708216428756714\n",
      "epoch: 9\n",
      "loss: 0.5706759691238403\n",
      "Lambda = 1.80e-02, selected 84 features \n",
      "10 epochs, val_objective 1.14e+00, val_loss 5.71e-01, regularization 3.14e+01, l2_regularization 1.54e+01\n",
      "epoch: 0\n",
      "loss: 0.5705305337905884\n",
      "epoch: 1\n",
      "loss: 0.570385217666626\n",
      "epoch: 2\n",
      "loss: 0.5702400207519531\n",
      "epoch: 3\n",
      "loss: 0.5700955390930176\n",
      "epoch: 4\n",
      "loss: 0.5699499249458313\n",
      "epoch: 5\n",
      "loss: 0.5698049068450928\n",
      "epoch: 6\n",
      "loss: 0.5696600675582886\n",
      "epoch: 7\n",
      "loss: 0.56951504945755\n",
      "epoch: 8\n",
      "loss: 0.5693701505661011\n",
      "epoch: 9\n",
      "loss: 0.5692248940467834\n",
      "Lambda = 1.84e-02, selected 84 features \n",
      "10 epochs, val_objective 1.15e+00, val_loss 5.69e-01, regularization 3.15e+01, l2_regularization 1.54e+01\n",
      "epoch: 0\n",
      "loss: 0.5690833926200867\n",
      "epoch: 1\n",
      "loss: 0.568937361240387\n",
      "epoch: 2\n",
      "loss: 0.5687934756278992\n",
      "epoch: 3\n",
      "loss: 0.568650484085083\n",
      "epoch: 4\n",
      "loss: 0.5685070157051086\n",
      "epoch: 5\n",
      "loss: 0.5683633089065552\n",
      "epoch: 6\n",
      "loss: 0.5682196617126465\n",
      "epoch: 7\n",
      "loss: 0.568076491355896\n",
      "epoch: 8\n",
      "loss: 0.5679336786270142\n",
      "epoch: 9\n",
      "loss: 0.5677911043167114\n",
      "Lambda = 1.89e-02, selected 84 features \n",
      "10 epochs, val_objective 1.16e+00, val_loss 5.68e-01, regularization 3.15e+01, l2_regularization 1.54e+01\n",
      "epoch: 0\n",
      "loss: 0.5676486492156982\n",
      "epoch: 1\n",
      "loss: 0.567504346370697\n",
      "epoch: 2\n",
      "loss: 0.5673615336418152\n",
      "epoch: 3\n",
      "loss: 0.5672187805175781\n",
      "epoch: 4\n",
      "loss: 0.5670763850212097\n",
      "epoch: 5\n",
      "loss: 0.566933810710907\n",
      "epoch: 6\n",
      "loss: 0.5667920112609863\n",
      "epoch: 7\n",
      "loss: 0.5666493773460388\n",
      "epoch: 8\n",
      "loss: 0.5665073394775391\n",
      "epoch: 9\n",
      "loss: 0.5663652420043945\n",
      "Lambda = 1.94e-02, selected 84 features \n",
      "10 epochs, val_objective 1.18e+00, val_loss 5.66e-01, regularization 3.16e+01, l2_regularization 1.54e+01\n",
      "epoch: 0\n",
      "loss: 0.5662235617637634\n",
      "epoch: 1\n",
      "loss: 0.5660825967788696\n",
      "epoch: 2\n",
      "loss: 0.5659396052360535\n",
      "epoch: 3\n",
      "loss: 0.5657984614372253\n",
      "epoch: 4\n",
      "loss: 0.5656565427780151\n",
      "epoch: 5\n",
      "loss: 0.5655152797698975\n",
      "epoch: 6\n",
      "loss: 0.5653734803199768\n",
      "epoch: 7\n",
      "loss: 0.5652323365211487\n",
      "epoch: 8\n",
      "loss: 0.5650917291641235\n",
      "epoch: 9\n",
      "loss: 0.5649529695510864\n",
      "Lambda = 1.98e-02, selected 84 features \n",
      "10 epochs, val_objective 1.19e+00, val_loss 5.65e-01, regularization 3.16e+01, l2_regularization 1.55e+01\n",
      "epoch: 0\n",
      "loss: 0.5648107528686523\n",
      "epoch: 1\n",
      "loss: 0.564673125743866\n",
      "epoch: 2\n",
      "loss: 0.5645314455032349\n",
      "epoch: 3\n",
      "loss: 0.5643920302391052\n",
      "epoch: 4\n",
      "loss: 0.5642525553703308\n",
      "epoch: 5\n",
      "loss: 0.5641128420829773\n",
      "epoch: 6\n",
      "loss: 0.5639724731445312\n",
      "epoch: 7\n",
      "loss: 0.5638332962989807\n",
      "epoch: 8\n",
      "loss: 0.563693106174469\n",
      "epoch: 9\n",
      "loss: 0.5635532736778259\n",
      "Lambda = 2.03e-02, selected 84 features \n",
      "10 epochs, val_objective 1.21e+00, val_loss 5.63e-01, regularization 3.17e+01, l2_regularization 1.55e+01\n",
      "epoch: 0\n",
      "loss: 0.563413679599762\n",
      "epoch: 1\n",
      "loss: 0.5632742643356323\n",
      "epoch: 2\n",
      "loss: 0.5631344318389893\n",
      "epoch: 3\n",
      "loss: 0.5629948973655701\n",
      "epoch: 4\n",
      "loss: 0.5628559589385986\n",
      "epoch: 5\n",
      "loss: 0.5627198219299316\n",
      "epoch: 6\n",
      "loss: 0.5625786781311035\n",
      "epoch: 7\n",
      "loss: 0.5624412894248962\n",
      "epoch: 8\n",
      "loss: 0.5623030662536621\n",
      "epoch: 9\n",
      "loss: 0.5621651411056519\n",
      "Lambda = 2.08e-02, selected 84 features \n",
      "10 epochs, val_objective 1.22e+00, val_loss 5.62e-01, regularization 3.17e+01, l2_regularization 1.55e+01\n",
      "epoch: 0\n",
      "loss: 0.5620267391204834\n",
      "epoch: 1\n",
      "loss: 0.5618883371353149\n",
      "epoch: 2\n",
      "loss: 0.5617517232894897\n",
      "epoch: 3\n",
      "loss: 0.5616129636764526\n",
      "epoch: 4\n",
      "loss: 0.5614748597145081\n",
      "epoch: 5\n",
      "loss: 0.5613383054733276\n",
      "epoch: 6\n",
      "loss: 0.5611993074417114\n",
      "epoch: 7\n",
      "loss: 0.5610624551773071\n",
      "epoch: 8\n",
      "loss: 0.5609251856803894\n",
      "epoch: 9\n",
      "loss: 0.5607889294624329\n",
      "Lambda = 2.14e-02, selected 84 features \n",
      "10 epochs, val_objective 1.24e+00, val_loss 5.61e-01, regularization 3.18e+01, l2_regularization 1.55e+01\n",
      "epoch: 0\n",
      "loss: 0.5606509447097778\n",
      "epoch: 1\n",
      "loss: 0.5605143308639526\n",
      "epoch: 2\n",
      "loss: 0.5603775978088379\n",
      "epoch: 3\n",
      "loss: 0.5602408051490784\n",
      "epoch: 4\n",
      "loss: 0.5601046085357666\n",
      "epoch: 5\n",
      "loss: 0.5599676370620728\n",
      "epoch: 6\n",
      "loss: 0.5598308444023132\n",
      "epoch: 7\n",
      "loss: 0.5596967935562134\n",
      "epoch: 8\n",
      "loss: 0.5595589876174927\n",
      "epoch: 9\n",
      "loss: 0.5594232082366943\n",
      "Lambda = 2.19e-02, selected 84 features \n",
      "10 epochs, val_objective 1.26e+00, val_loss 5.59e-01, regularization 3.18e+01, l2_regularization 1.56e+01\n",
      "epoch: 0\n",
      "loss: 0.559287428855896\n",
      "epoch: 1\n",
      "loss: 0.5591534376144409\n",
      "epoch: 2\n",
      "loss: 0.5590161085128784\n",
      "epoch: 3\n",
      "loss: 0.5588810443878174\n",
      "epoch: 4\n",
      "loss: 0.5587451457977295\n",
      "epoch: 5\n",
      "loss: 0.5586128830909729\n",
      "epoch: 6\n",
      "loss: 0.5584752559661865\n",
      "epoch: 7\n",
      "loss: 0.5583399534225464\n",
      "epoch: 8\n",
      "loss: 0.5582048296928406\n",
      "epoch: 9\n",
      "loss: 0.5580706000328064\n",
      "Lambda = 2.24e-02, selected 84 features \n",
      "10 epochs, val_objective 1.27e+00, val_loss 5.58e-01, regularization 3.19e+01, l2_regularization 1.56e+01\n",
      "epoch: 0\n",
      "loss: 0.557938814163208\n",
      "epoch: 1\n",
      "loss: 0.5578017830848694\n",
      "epoch: 2\n",
      "loss: 0.5576680898666382\n",
      "epoch: 3\n",
      "loss: 0.5575342178344727\n",
      "epoch: 4\n",
      "loss: 0.5574004650115967\n",
      "epoch: 5\n",
      "loss: 0.5572665929794312\n",
      "epoch: 6\n",
      "loss: 0.5571333765983582\n",
      "epoch: 7\n",
      "loss: 0.5569984912872314\n",
      "epoch: 8\n",
      "loss: 0.5568653345108032\n",
      "epoch: 9\n",
      "loss: 0.5567313432693481\n",
      "Lambda = 2.30e-02, selected 84 features \n",
      "10 epochs, val_objective 1.29e+00, val_loss 5.57e-01, regularization 3.19e+01, l2_regularization 1.56e+01\n",
      "epoch: 0\n",
      "loss: 0.5565974116325378\n",
      "epoch: 1\n",
      "loss: 0.5564647912979126\n",
      "epoch: 2\n",
      "loss: 0.5563322901725769\n",
      "epoch: 3\n",
      "loss: 0.556198239326477\n",
      "epoch: 4\n",
      "loss: 0.5560659170150757\n",
      "epoch: 5\n",
      "loss: 0.555932879447937\n",
      "epoch: 6\n",
      "loss: 0.5558000802993774\n",
      "epoch: 7\n",
      "loss: 0.555667519569397\n",
      "epoch: 8\n",
      "loss: 0.5555341839790344\n",
      "epoch: 9\n",
      "loss: 0.5554054975509644\n",
      "Lambda = 2.36e-02, selected 84 features \n",
      "10 epochs, val_objective 1.31e+00, val_loss 5.55e-01, regularization 3.20e+01, l2_regularization 1.56e+01\n",
      "epoch: 0\n",
      "loss: 0.5552700757980347\n",
      "epoch: 1\n",
      "loss: 0.5551379323005676\n",
      "epoch: 2\n",
      "loss: 0.5550064444541931\n",
      "epoch: 3\n",
      "loss: 0.554875373840332\n",
      "epoch: 4\n",
      "loss: 0.5547431707382202\n",
      "epoch: 5\n",
      "loss: 0.554611325263977\n",
      "epoch: 6\n",
      "loss: 0.5544800162315369\n",
      "epoch: 7\n",
      "loss: 0.5543481111526489\n",
      "epoch: 8\n",
      "loss: 0.5542189478874207\n",
      "epoch: 9\n",
      "loss: 0.5540860891342163\n",
      "Lambda = 2.42e-02, selected 84 features \n",
      "10 epochs, val_objective 1.33e+00, val_loss 5.54e-01, regularization 3.20e+01, l2_regularization 1.57e+01\n",
      "epoch: 0\n",
      "loss: 0.5539543032646179\n",
      "epoch: 1\n",
      "loss: 0.5538245439529419\n",
      "epoch: 2\n",
      "loss: 0.5536941885948181\n",
      "epoch: 3\n",
      "loss: 0.5535632371902466\n",
      "epoch: 4\n",
      "loss: 0.5534327626228333\n",
      "epoch: 5\n",
      "loss: 0.5533027648925781\n",
      "epoch: 6\n",
      "loss: 0.5531723499298096\n",
      "epoch: 7\n",
      "loss: 0.5530422925949097\n",
      "epoch: 8\n",
      "loss: 0.552911639213562\n",
      "epoch: 9\n",
      "loss: 0.552781343460083\n",
      "Lambda = 2.48e-02, selected 84 features \n",
      "10 epochs, val_objective 1.35e+00, val_loss 5.53e-01, regularization 3.21e+01, l2_regularization 1.57e+01\n",
      "epoch: 0\n",
      "loss: 0.5526516437530518\n",
      "epoch: 1\n",
      "loss: 0.5525217056274414\n",
      "epoch: 2\n",
      "loss: 0.5523927807807922\n",
      "epoch: 3\n",
      "loss: 0.5522632598876953\n",
      "epoch: 4\n",
      "loss: 0.5521343350410461\n",
      "epoch: 5\n",
      "loss: 0.5520055294036865\n",
      "epoch: 6\n",
      "loss: 0.5518770217895508\n",
      "epoch: 7\n",
      "loss: 0.5517479777336121\n",
      "epoch: 8\n",
      "loss: 0.5516186952590942\n",
      "epoch: 9\n",
      "loss: 0.5514894127845764\n",
      "Lambda = 2.54e-02, selected 84 features \n",
      "10 epochs, val_objective 1.37e+00, val_loss 5.51e-01, regularization 3.21e+01, l2_regularization 1.57e+01\n",
      "epoch: 0\n",
      "loss: 0.5513602495193481\n",
      "epoch: 1\n",
      "loss: 0.5512310862541199\n",
      "epoch: 2\n",
      "loss: 0.5511038303375244\n",
      "epoch: 3\n",
      "loss: 0.5509744882583618\n",
      "epoch: 4\n",
      "loss: 0.5508463978767395\n",
      "epoch: 5\n",
      "loss: 0.5507187843322754\n",
      "epoch: 6\n",
      "loss: 0.5505911111831665\n",
      "epoch: 7\n",
      "loss: 0.5504631996154785\n",
      "epoch: 8\n",
      "loss: 0.5503351092338562\n",
      "epoch: 9\n",
      "loss: 0.5502071380615234\n",
      "Lambda = 2.60e-02, selected 84 features \n",
      "10 epochs, val_objective 1.39e+00, val_loss 5.50e-01, regularization 3.22e+01, l2_regularization 1.57e+01\n",
      "epoch: 0\n",
      "loss: 0.5500790476799011\n",
      "epoch: 1\n",
      "loss: 0.5499520301818848\n",
      "epoch: 2\n",
      "loss: 0.5498235821723938\n",
      "epoch: 3\n",
      "loss: 0.5496970415115356\n",
      "epoch: 4\n",
      "loss: 0.5495691299438477\n",
      "epoch: 5\n",
      "loss: 0.5494422912597656\n",
      "epoch: 6\n",
      "loss: 0.5493183135986328\n",
      "epoch: 7\n",
      "loss: 0.5491901636123657\n",
      "epoch: 8\n",
      "loss: 0.5490634441375732\n",
      "epoch: 9\n",
      "loss: 0.5489369630813599\n",
      "Lambda = 2.67e-02, selected 84 features \n",
      "10 epochs, val_objective 1.41e+00, val_loss 5.49e-01, regularization 3.22e+01, l2_regularization 1.58e+01\n",
      "epoch: 0\n",
      "loss: 0.548810601234436\n",
      "epoch: 1\n",
      "loss: 0.5486848950386047\n",
      "epoch: 2\n",
      "loss: 0.5485589504241943\n",
      "epoch: 3\n",
      "loss: 0.5484333038330078\n",
      "epoch: 4\n",
      "loss: 0.5483074188232422\n",
      "epoch: 5\n",
      "loss: 0.5481821298599243\n",
      "epoch: 6\n",
      "loss: 0.5480560064315796\n",
      "epoch: 7\n",
      "loss: 0.5479303598403931\n",
      "epoch: 8\n",
      "loss: 0.5478056073188782\n",
      "epoch: 9\n",
      "loss: 0.5476795434951782\n",
      "Lambda = 2.74e-02, selected 84 features \n",
      "10 epochs, val_objective 1.43e+00, val_loss 5.48e-01, regularization 3.23e+01, l2_regularization 1.58e+01\n",
      "epoch: 0\n",
      "loss: 0.5475543737411499\n",
      "epoch: 1\n",
      "loss: 0.547429621219635\n",
      "epoch: 2\n",
      "loss: 0.5473043918609619\n",
      "epoch: 3\n",
      "loss: 0.5471793413162231\n",
      "epoch: 4\n",
      "loss: 0.5470553040504456\n",
      "epoch: 5\n",
      "loss: 0.5469298958778381\n",
      "epoch: 6\n",
      "loss: 0.5468055009841919\n",
      "epoch: 7\n",
      "loss: 0.5466828346252441\n",
      "epoch: 8\n",
      "loss: 0.5465565919876099\n",
      "epoch: 9\n",
      "loss: 0.5464321970939636\n",
      "Lambda = 2.80e-02, selected 84 features \n",
      "10 epochs, val_objective 1.45e+00, val_loss 5.46e-01, regularization 3.23e+01, l2_regularization 1.58e+01\n",
      "epoch: 0\n",
      "loss: 0.5463093519210815\n",
      "epoch: 1\n",
      "loss: 0.5461845397949219\n",
      "epoch: 2\n",
      "loss: 0.5460606217384338\n",
      "epoch: 3\n",
      "loss: 0.5459370017051697\n",
      "epoch: 4\n",
      "loss: 0.5458133220672607\n",
      "epoch: 5\n",
      "loss: 0.545690655708313\n",
      "epoch: 6\n",
      "loss: 0.5455662608146667\n",
      "epoch: 7\n",
      "loss: 0.5454440712928772\n",
      "epoch: 8\n",
      "loss: 0.545319676399231\n",
      "epoch: 9\n",
      "loss: 0.5451966524124146\n",
      "Lambda = 2.87e-02, selected 84 features \n",
      "10 epochs, val_objective 1.47e+00, val_loss 5.45e-01, regularization 3.24e+01, l2_regularization 1.58e+01\n",
      "epoch: 0\n",
      "loss: 0.5450738668441772\n",
      "epoch: 1\n",
      "loss: 0.5449502468109131\n",
      "epoch: 2\n",
      "loss: 0.5448279976844788\n",
      "epoch: 3\n",
      "loss: 0.5447056293487549\n",
      "epoch: 4\n",
      "loss: 0.5445843935012817\n",
      "epoch: 5\n",
      "loss: 0.544460654258728\n",
      "epoch: 6\n",
      "loss: 0.5443389415740967\n",
      "epoch: 7\n",
      "loss: 0.5442173480987549\n",
      "epoch: 8\n",
      "loss: 0.5440957546234131\n",
      "epoch: 9\n",
      "loss: 0.5439733266830444\n",
      "Lambda = 2.95e-02, selected 84 features \n",
      "10 epochs, val_objective 1.50e+00, val_loss 5.44e-01, regularization 3.24e+01, l2_regularization 1.58e+01\n",
      "epoch: 0\n",
      "loss: 0.5438521504402161\n",
      "epoch: 1\n",
      "loss: 0.5437307357788086\n",
      "epoch: 2\n",
      "loss: 0.5436112284660339\n",
      "epoch: 3\n",
      "loss: 0.5434885025024414\n",
      "epoch: 4\n",
      "loss: 0.5433676242828369\n",
      "epoch: 5\n",
      "loss: 0.5432471036911011\n",
      "epoch: 6\n",
      "loss: 0.5431261658668518\n",
      "epoch: 7\n",
      "loss: 0.5430048704147339\n",
      "epoch: 8\n",
      "loss: 0.5428838729858398\n",
      "epoch: 9\n",
      "loss: 0.5427635908126831\n",
      "Lambda = 3.02e-02, selected 84 features \n",
      "10 epochs, val_objective 1.52e+00, val_loss 5.43e-01, regularization 3.24e+01, l2_regularization 1.59e+01\n",
      "epoch: 0\n",
      "loss: 0.5426415205001831\n",
      "epoch: 1\n",
      "loss: 0.5425217747688293\n",
      "epoch: 2\n",
      "loss: 0.5424010753631592\n",
      "epoch: 3\n",
      "loss: 0.5422810912132263\n",
      "epoch: 4\n",
      "loss: 0.5421608686447144\n",
      "epoch: 5\n",
      "loss: 0.5420413017272949\n",
      "epoch: 6\n",
      "loss: 0.5419206619262695\n",
      "epoch: 7\n",
      "loss: 0.5418006181716919\n",
      "epoch: 8\n",
      "loss: 0.5416814088821411\n",
      "epoch: 9\n",
      "loss: 0.5415607690811157\n",
      "Lambda = 3.09e-02, selected 84 features \n",
      "10 epochs, val_objective 1.55e+00, val_loss 5.41e-01, regularization 3.25e+01, l2_regularization 1.59e+01\n",
      "epoch: 0\n",
      "loss: 0.5414413213729858\n",
      "epoch: 1\n",
      "loss: 0.5413224101066589\n",
      "epoch: 2\n",
      "loss: 0.5412033796310425\n",
      "epoch: 3\n",
      "loss: 0.5410841107368469\n",
      "epoch: 4\n",
      "loss: 0.5409654974937439\n",
      "epoch: 5\n",
      "loss: 0.5408464670181274\n",
      "epoch: 6\n",
      "loss: 0.5407286882400513\n",
      "epoch: 7\n",
      "loss: 0.5406094193458557\n",
      "epoch: 8\n",
      "loss: 0.5404906272888184\n",
      "epoch: 9\n",
      "loss: 0.540372371673584\n",
      "Lambda = 3.17e-02, selected 84 features \n",
      "10 epochs, val_objective 1.57e+00, val_loss 5.40e-01, regularization 3.25e+01, l2_regularization 1.59e+01\n",
      "epoch: 0\n",
      "loss: 0.5402534008026123\n",
      "epoch: 1\n",
      "loss: 0.5401347279548645\n",
      "epoch: 2\n",
      "loss: 0.5400189161300659\n",
      "epoch: 3\n",
      "loss: 0.5399009585380554\n",
      "epoch: 4\n",
      "loss: 0.539782702922821\n",
      "epoch: 5\n",
      "loss: 0.5396645665168762\n",
      "epoch: 6\n",
      "loss: 0.5395476222038269\n",
      "epoch: 7\n",
      "loss: 0.5394302606582642\n",
      "epoch: 8\n",
      "loss: 0.539312481880188\n",
      "epoch: 9\n",
      "loss: 0.5391955375671387\n",
      "Lambda = 3.25e-02, selected 84 features \n",
      "10 epochs, val_objective 1.60e+00, val_loss 5.39e-01, regularization 3.26e+01, l2_regularization 1.59e+01\n",
      "epoch: 0\n",
      "loss: 0.5390779972076416\n",
      "epoch: 1\n",
      "loss: 0.5389607548713684\n",
      "epoch: 2\n",
      "loss: 0.5388437509536743\n",
      "epoch: 3\n",
      "loss: 0.5387277603149414\n",
      "epoch: 4\n",
      "loss: 0.5386101603507996\n",
      "epoch: 5\n",
      "loss: 0.5384935736656189\n",
      "epoch: 6\n",
      "loss: 0.5383772850036621\n",
      "epoch: 7\n",
      "loss: 0.5382607579231262\n",
      "epoch: 8\n",
      "loss: 0.5381454229354858\n",
      "epoch: 9\n",
      "loss: 0.538027822971344\n",
      "Lambda = 3.33e-02, selected 84 features \n",
      "10 epochs, val_objective 1.62e+00, val_loss 5.38e-01, regularization 3.26e+01, l2_regularization 1.59e+01\n",
      "epoch: 0\n",
      "loss: 0.5379120111465454\n",
      "epoch: 1\n",
      "loss: 0.537796139717102\n",
      "epoch: 2\n",
      "loss: 0.5376808047294617\n",
      "epoch: 3\n",
      "loss: 0.5375649333000183\n",
      "epoch: 4\n",
      "loss: 0.5374503135681152\n",
      "epoch: 5\n",
      "loss: 0.5373339056968689\n",
      "epoch: 6\n",
      "loss: 0.537219762802124\n",
      "epoch: 7\n",
      "loss: 0.5371041297912598\n",
      "epoch: 8\n",
      "loss: 0.5369895696640015\n",
      "epoch: 9\n",
      "loss: 0.5368742346763611\n",
      "Lambda = 3.42e-02, selected 84 features \n",
      "10 epochs, val_objective 1.65e+00, val_loss 5.37e-01, regularization 3.27e+01, l2_regularization 1.60e+01\n",
      "epoch: 0\n",
      "loss: 0.5367594957351685\n",
      "epoch: 1\n",
      "loss: 0.5366448760032654\n",
      "epoch: 2\n",
      "loss: 0.5365306735038757\n",
      "epoch: 3\n",
      "loss: 0.5364154577255249\n",
      "epoch: 4\n",
      "loss: 0.5363008975982666\n",
      "epoch: 5\n",
      "loss: 0.5361862778663635\n",
      "epoch: 6\n",
      "loss: 0.5360724925994873\n",
      "epoch: 7\n",
      "loss: 0.5359582304954529\n",
      "epoch: 8\n",
      "loss: 0.535846471786499\n",
      "epoch: 9\n",
      "loss: 0.5357317328453064\n",
      "Lambda = 3.50e-02, selected 84 features \n",
      "10 epochs, val_objective 1.68e+00, val_loss 5.36e-01, regularization 3.27e+01, l2_regularization 1.60e+01\n",
      "epoch: 0\n",
      "loss: 0.5356175303459167\n",
      "epoch: 1\n",
      "loss: 0.5355043411254883\n",
      "epoch: 2\n",
      "loss: 0.5353930592536926\n",
      "epoch: 3\n",
      "loss: 0.5352781414985657\n",
      "epoch: 4\n",
      "loss: 0.5351656079292297\n",
      "epoch: 5\n",
      "loss: 0.5350528955459595\n",
      "epoch: 6\n",
      "loss: 0.5349397659301758\n",
      "epoch: 7\n",
      "loss: 0.5348266959190369\n",
      "epoch: 8\n",
      "loss: 0.5347157120704651\n",
      "epoch: 9\n",
      "loss: 0.5346015095710754\n",
      "Lambda = 3.59e-02, selected 84 features \n",
      "10 epochs, val_objective 1.71e+00, val_loss 5.34e-01, regularization 3.27e+01, l2_regularization 1.60e+01\n",
      "epoch: 0\n",
      "loss: 0.5344882607460022\n",
      "epoch: 1\n",
      "loss: 0.5343760848045349\n",
      "epoch: 2\n",
      "loss: 0.5342637300491333\n",
      "epoch: 3\n",
      "loss: 0.5341514348983765\n",
      "epoch: 4\n",
      "loss: 0.5340394377708435\n",
      "epoch: 5\n",
      "loss: 0.5339272022247314\n",
      "epoch: 6\n",
      "loss: 0.5338156819343567\n",
      "epoch: 7\n",
      "loss: 0.5337039828300476\n",
      "epoch: 8\n",
      "loss: 0.5335917472839355\n",
      "epoch: 9\n",
      "loss: 0.5334798097610474\n",
      "Lambda = 3.68e-02, selected 84 features \n",
      "10 epochs, val_objective 1.74e+00, val_loss 5.33e-01, regularization 3.28e+01, l2_regularization 1.60e+01\n",
      "epoch: 0\n",
      "loss: 0.533367395401001\n",
      "epoch: 1\n",
      "loss: 0.533255934715271\n",
      "epoch: 2\n",
      "loss: 0.5331450700759888\n",
      "epoch: 3\n",
      "loss: 0.5330342650413513\n",
      "epoch: 4\n",
      "loss: 0.5329236388206482\n",
      "epoch: 5\n",
      "loss: 0.532813310623169\n",
      "epoch: 6\n",
      "loss: 0.5327043533325195\n",
      "epoch: 7\n",
      "loss: 0.5325919389724731\n",
      "epoch: 8\n",
      "loss: 0.5324810743331909\n",
      "epoch: 9\n",
      "loss: 0.5323703289031982\n",
      "Lambda = 3.77e-02, selected 84 features \n",
      "10 epochs, val_objective 1.77e+00, val_loss 5.32e-01, regularization 3.28e+01, l2_regularization 1.61e+01\n",
      "epoch: 0\n",
      "loss: 0.5322597622871399\n",
      "epoch: 1\n",
      "loss: 0.5321508646011353\n",
      "epoch: 2\n",
      "loss: 0.5320413112640381\n",
      "epoch: 3\n",
      "loss: 0.5319311022758484\n",
      "epoch: 4\n",
      "loss: 0.5318227410316467\n",
      "epoch: 5\n",
      "loss: 0.53171306848526\n",
      "epoch: 6\n",
      "loss: 0.531603217124939\n",
      "epoch: 7\n",
      "loss: 0.5314945578575134\n",
      "epoch: 8\n",
      "loss: 0.5313851833343506\n",
      "epoch: 9\n",
      "loss: 0.5312758088111877\n",
      "Lambda = 3.86e-02, selected 84 features \n",
      "10 epochs, val_objective 1.80e+00, val_loss 5.31e-01, regularization 3.28e+01, l2_regularization 1.61e+01\n",
      "epoch: 0\n",
      "loss: 0.5311667919158936\n",
      "epoch: 1\n",
      "loss: 0.5310572981834412\n",
      "epoch: 2\n",
      "loss: 0.5309491157531738\n",
      "epoch: 3\n",
      "loss: 0.5308393239974976\n",
      "epoch: 4\n",
      "loss: 0.5307308435440063\n",
      "epoch: 5\n",
      "loss: 0.5306220650672913\n",
      "epoch: 6\n",
      "loss: 0.5305131077766418\n",
      "epoch: 7\n",
      "loss: 0.5304045677185059\n",
      "epoch: 8\n",
      "loss: 0.5302960276603699\n",
      "epoch: 9\n",
      "loss: 0.5301879644393921\n",
      "Lambda = 3.96e-02, selected 84 features \n",
      "10 epochs, val_objective 1.83e+00, val_loss 5.30e-01, regularization 3.29e+01, l2_regularization 1.61e+01\n",
      "epoch: 0\n",
      "loss: 0.5300792455673218\n",
      "epoch: 1\n",
      "loss: 0.5299720764160156\n",
      "epoch: 2\n",
      "loss: 0.5298648476600647\n",
      "epoch: 3\n",
      "loss: 0.5297569036483765\n",
      "epoch: 4\n",
      "loss: 0.5296496748924255\n",
      "epoch: 5\n",
      "loss: 0.5295425057411194\n",
      "epoch: 6\n",
      "loss: 0.5294373035430908\n",
      "epoch: 7\n",
      "loss: 0.5293280482292175\n",
      "epoch: 8\n",
      "loss: 0.5292215943336487\n",
      "epoch: 9\n",
      "loss: 0.5291146039962769\n",
      "Lambda = 4.06e-02, selected 84 features \n",
      "10 epochs, val_objective 1.87e+00, val_loss 5.29e-01, regularization 3.29e+01, l2_regularization 1.61e+01\n",
      "epoch: 0\n",
      "loss: 0.5290073156356812\n",
      "epoch: 1\n",
      "loss: 0.5289010405540466\n",
      "epoch: 2\n",
      "loss: 0.5287943482398987\n",
      "epoch: 3\n",
      "loss: 0.5286877155303955\n",
      "epoch: 4\n",
      "loss: 0.5285811424255371\n",
      "epoch: 5\n",
      "loss: 0.5284745097160339\n",
      "epoch: 6\n",
      "loss: 0.5283691883087158\n",
      "epoch: 7\n",
      "loss: 0.5282620787620544\n",
      "epoch: 8\n",
      "loss: 0.5281562805175781\n",
      "epoch: 9\n",
      "loss: 0.5280506610870361\n",
      "Lambda = 4.16e-02, selected 84 features \n",
      "10 epochs, val_objective 1.90e+00, val_loss 5.28e-01, regularization 3.30e+01, l2_regularization 1.61e+01\n",
      "epoch: 0\n",
      "loss: 0.5279462933540344\n",
      "epoch: 1\n",
      "loss: 0.5278393626213074\n",
      "epoch: 2\n",
      "loss: 0.5277345180511475\n",
      "epoch: 3\n",
      "loss: 0.5276296734809875\n",
      "epoch: 4\n",
      "loss: 0.5275248289108276\n",
      "epoch: 5\n",
      "loss: 0.527419924736023\n",
      "epoch: 6\n",
      "loss: 0.5273146033287048\n",
      "epoch: 7\n",
      "loss: 0.5272125005722046\n",
      "epoch: 8\n",
      "loss: 0.5271052718162537\n",
      "epoch: 9\n",
      "loss: 0.5270006656646729\n",
      "Lambda = 4.27e-02, selected 84 features \n",
      "10 epochs, val_objective 1.93e+00, val_loss 5.27e-01, regularization 3.30e+01, l2_regularization 1.62e+01\n",
      "epoch: 0\n",
      "loss: 0.5268963575363159\n",
      "epoch: 1\n",
      "loss: 0.5267925262451172\n",
      "epoch: 2\n",
      "loss: 0.526688277721405\n",
      "epoch: 3\n",
      "loss: 0.5265845060348511\n",
      "epoch: 4\n",
      "loss: 0.5264806151390076\n",
      "epoch: 5\n",
      "loss: 0.5263766646385193\n",
      "epoch: 6\n",
      "loss: 0.5262730717658997\n",
      "epoch: 7\n",
      "loss: 0.5261690020561218\n",
      "epoch: 8\n",
      "loss: 0.5260661840438843\n",
      "epoch: 9\n",
      "loss: 0.5259623527526855\n",
      "Lambda = 4.37e-02, selected 84 features \n",
      "10 epochs, val_objective 1.97e+00, val_loss 5.26e-01, regularization 3.30e+01, l2_regularization 1.62e+01\n",
      "epoch: 0\n",
      "loss: 0.5258589386940002\n",
      "epoch: 1\n",
      "loss: 0.5257571935653687\n",
      "epoch: 2\n",
      "loss: 0.525653600692749\n",
      "epoch: 3\n",
      "loss: 0.5255511403083801\n",
      "epoch: 4\n",
      "loss: 0.5254482626914978\n",
      "epoch: 5\n",
      "loss: 0.5253453850746155\n",
      "epoch: 6\n",
      "loss: 0.5252426862716675\n",
      "epoch: 7\n",
      "loss: 0.525139331817627\n",
      "epoch: 8\n",
      "loss: 0.5250372290611267\n",
      "epoch: 9\n",
      "loss: 0.5249338746070862\n",
      "Lambda = 4.48e-02, selected 84 features \n",
      "10 epochs, val_objective 2.01e+00, val_loss 5.25e-01, regularization 3.31e+01, l2_regularization 1.62e+01\n",
      "epoch: 0\n",
      "loss: 0.5248333811759949\n",
      "epoch: 1\n",
      "loss: 0.5247295498847961\n",
      "epoch: 2\n",
      "loss: 0.524627685546875\n",
      "epoch: 3\n",
      "loss: 0.5245263576507568\n",
      "epoch: 4\n",
      "loss: 0.5244253873825073\n",
      "epoch: 5\n",
      "loss: 0.5243240594863892\n",
      "epoch: 6\n",
      "loss: 0.524223268032074\n",
      "epoch: 7\n",
      "loss: 0.5241213440895081\n",
      "epoch: 8\n",
      "loss: 0.5240200757980347\n",
      "epoch: 9\n",
      "loss: 0.523918867111206\n",
      "Lambda = 4.59e-02, selected 84 features \n",
      "10 epochs, val_objective 2.04e+00, val_loss 5.24e-01, regularization 3.31e+01, l2_regularization 1.62e+01\n",
      "epoch: 0\n",
      "loss: 0.523817241191864\n",
      "epoch: 1\n",
      "loss: 0.5237162113189697\n",
      "epoch: 2\n",
      "loss: 0.5236150026321411\n",
      "epoch: 3\n",
      "loss: 0.523514986038208\n",
      "epoch: 4\n",
      "loss: 0.5234140157699585\n",
      "epoch: 5\n",
      "loss: 0.523314893245697\n",
      "epoch: 6\n",
      "loss: 0.5232141613960266\n",
      "epoch: 7\n",
      "loss: 0.5231142640113831\n",
      "epoch: 8\n",
      "loss: 0.5230151414871216\n",
      "epoch: 9\n",
      "loss: 0.5229140520095825\n",
      "Lambda = 4.71e-02, selected 84 features \n",
      "10 epochs, val_objective 2.08e+00, val_loss 5.23e-01, regularization 3.31e+01, l2_regularization 1.62e+01\n",
      "epoch: 0\n",
      "loss: 0.5228145122528076\n",
      "epoch: 1\n",
      "loss: 0.5227154493331909\n",
      "epoch: 2\n",
      "loss: 0.5226165056228638\n",
      "epoch: 3\n",
      "loss: 0.5225169658660889\n",
      "epoch: 4\n",
      "loss: 0.5224177837371826\n",
      "epoch: 5\n",
      "loss: 0.5223191976547241\n",
      "epoch: 6\n",
      "loss: 0.5222189426422119\n",
      "epoch: 7\n",
      "loss: 0.5221219658851624\n",
      "epoch: 8\n",
      "loss: 0.5220213532447815\n",
      "epoch: 9\n",
      "loss: 0.5219222903251648\n",
      "Lambda = 4.83e-02, selected 84 features \n",
      "10 epochs, val_objective 2.12e+00, val_loss 5.22e-01, regularization 3.32e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5218238234519958\n",
      "epoch: 1\n",
      "loss: 0.5217262506484985\n",
      "epoch: 2\n",
      "loss: 0.5216282606124878\n",
      "epoch: 3\n",
      "loss: 0.5215314030647278\n",
      "epoch: 4\n",
      "loss: 0.5214325189590454\n",
      "epoch: 5\n",
      "loss: 0.5213344097137451\n",
      "epoch: 6\n",
      "loss: 0.5212363004684448\n",
      "epoch: 7\n",
      "loss: 0.5211384296417236\n",
      "epoch: 8\n",
      "loss: 0.521040678024292\n",
      "epoch: 9\n",
      "loss: 0.520942211151123\n",
      "Lambda = 4.95e-02, selected 84 features \n",
      "10 epochs, val_objective 2.16e+00, val_loss 5.21e-01, regularization 3.32e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5208443999290466\n",
      "epoch: 1\n",
      "loss: 0.5207479000091553\n",
      "epoch: 2\n",
      "loss: 0.5206500291824341\n",
      "epoch: 3\n",
      "loss: 0.520552933216095\n",
      "epoch: 4\n",
      "loss: 0.5204557180404663\n",
      "epoch: 5\n",
      "loss: 0.5203589200973511\n",
      "epoch: 6\n",
      "loss: 0.5202623605728149\n",
      "epoch: 7\n",
      "loss: 0.5201656818389893\n",
      "epoch: 8\n",
      "loss: 0.5200693011283875\n",
      "epoch: 9\n",
      "loss: 0.5199750065803528\n",
      "Lambda = 5.07e-02, selected 84 features \n",
      "10 epochs, val_objective 2.20e+00, val_loss 5.20e-01, regularization 3.32e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5198782682418823\n",
      "epoch: 1\n",
      "loss: 0.5197833180427551\n",
      "epoch: 2\n",
      "loss: 0.5196890234947205\n",
      "epoch: 3\n",
      "loss: 0.5195948481559753\n",
      "epoch: 4\n",
      "loss: 0.5195004940032959\n",
      "epoch: 5\n",
      "loss: 0.5194054245948792\n",
      "epoch: 6\n",
      "loss: 0.5193124413490295\n",
      "epoch: 7\n",
      "loss: 0.5192165970802307\n",
      "epoch: 8\n",
      "loss: 0.5191218852996826\n",
      "epoch: 9\n",
      "loss: 0.5190284848213196\n",
      "Lambda = 5.20e-02, selected 84 features \n",
      "10 epochs, val_objective 2.25e+00, val_loss 5.19e-01, regularization 3.33e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5189335346221924\n",
      "epoch: 1\n",
      "loss: 0.5188402533531189\n",
      "epoch: 2\n",
      "loss: 0.5187461376190186\n",
      "epoch: 3\n",
      "loss: 0.5186524987220764\n",
      "epoch: 4\n",
      "loss: 0.5185588002204895\n",
      "epoch: 5\n",
      "loss: 0.5184645652770996\n",
      "epoch: 6\n",
      "loss: 0.5183714628219604\n",
      "epoch: 7\n",
      "loss: 0.5182769298553467\n",
      "epoch: 8\n",
      "loss: 0.5181829929351807\n",
      "epoch: 9\n",
      "loss: 0.5180889368057251\n",
      "Lambda = 5.33e-02, selected 84 features \n",
      "10 epochs, val_objective 2.29e+00, val_loss 5.18e-01, regularization 3.33e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5179961323738098\n",
      "epoch: 1\n",
      "loss: 0.5179027318954468\n",
      "epoch: 2\n",
      "loss: 0.5178101062774658\n",
      "epoch: 3\n",
      "loss: 0.5177173614501953\n",
      "epoch: 4\n",
      "loss: 0.5176242589950562\n",
      "epoch: 5\n",
      "loss: 0.5175314545631409\n",
      "epoch: 6\n",
      "loss: 0.5174393057823181\n",
      "epoch: 7\n",
      "loss: 0.5173470377922058\n",
      "epoch: 8\n",
      "loss: 0.5172539949417114\n",
      "epoch: 9\n",
      "loss: 0.5171628594398499\n",
      "Lambda = 5.46e-02, selected 84 features \n",
      "10 epochs, val_objective 2.34e+00, val_loss 5.17e-01, regularization 3.33e+01, l2_regularization 1.63e+01\n",
      "epoch: 0\n",
      "loss: 0.5170698165893555\n",
      "epoch: 1\n",
      "loss: 0.5169785618782043\n",
      "epoch: 2\n",
      "loss: 0.5168880224227905\n",
      "epoch: 3\n",
      "loss: 0.516796350479126\n",
      "epoch: 4\n",
      "loss: 0.5167055130004883\n",
      "epoch: 5\n",
      "loss: 0.5166146755218506\n",
      "epoch: 6\n",
      "loss: 0.5165245532989502\n",
      "epoch: 7\n",
      "loss: 0.5164332389831543\n",
      "epoch: 8\n",
      "loss: 0.5163416862487793\n",
      "epoch: 9\n",
      "loss: 0.5162530541419983\n",
      "Lambda = 5.60e-02, selected 84 features \n",
      "10 epochs, val_objective 2.38e+00, val_loss 5.16e-01, regularization 3.34e+01, l2_regularization 1.64e+01\n",
      "epoch: 0\n",
      "loss: 0.5161605477333069\n",
      "epoch: 1\n",
      "loss: 0.5160703659057617\n",
      "epoch: 2\n",
      "loss: 0.5159805417060852\n",
      "epoch: 3\n",
      "loss: 0.5158904194831848\n",
      "epoch: 4\n",
      "loss: 0.5158002972602844\n",
      "epoch: 5\n",
      "loss: 0.5157104134559631\n",
      "epoch: 6\n",
      "loss: 0.5156201720237732\n",
      "epoch: 7\n",
      "loss: 0.5155306458473206\n",
      "epoch: 8\n",
      "loss: 0.5154407024383545\n",
      "epoch: 9\n",
      "loss: 0.5153509974479675\n",
      "Lambda = 5.74e-02, selected 84 features \n",
      "10 epochs, val_objective 2.43e+00, val_loss 5.15e-01, regularization 3.34e+01, l2_regularization 1.64e+01\n",
      "epoch: 0\n",
      "loss: 0.5152613520622253\n",
      "epoch: 1\n",
      "loss: 0.5151719450950623\n",
      "epoch: 2\n",
      "loss: 0.5150832533836365\n",
      "epoch: 3\n",
      "loss: 0.5149943232536316\n",
      "epoch: 4\n",
      "loss: 0.5149053335189819\n",
      "epoch: 5\n",
      "loss: 0.5148168802261353\n",
      "epoch: 6\n",
      "loss: 0.5147274732589722\n",
      "epoch: 7\n",
      "loss: 0.5146399736404419\n",
      "epoch: 8\n",
      "loss: 0.5145506262779236\n",
      "epoch: 9\n",
      "loss: 0.5144619345664978\n",
      "Lambda = 5.88e-02, selected 84 features \n",
      "10 epochs, val_objective 2.48e+00, val_loss 5.14e-01, regularization 3.34e+01, l2_regularization 1.64e+01\n",
      "epoch: 0\n",
      "loss: 0.514373779296875\n",
      "epoch: 1\n",
      "loss: 0.514286458492279\n",
      "epoch: 2\n",
      "loss: 0.5141991972923279\n",
      "epoch: 3\n",
      "loss: 0.5141109228134155\n",
      "epoch: 4\n",
      "loss: 0.5140237212181091\n",
      "epoch: 5\n",
      "loss: 0.5139362215995789\n",
      "epoch: 6\n",
      "loss: 0.5138529539108276\n",
      "epoch: 7\n",
      "loss: 0.5137613415718079\n",
      "epoch: 8\n",
      "loss: 0.5136746168136597\n",
      "epoch: 9\n",
      "loss: 0.5135881900787354\n",
      "Lambda = 6.03e-02, selected 84 features \n",
      "10 epochs, val_objective 2.53e+00, val_loss 5.14e-01, regularization 3.34e+01, l2_regularization 1.64e+01\n",
      "epoch: 0\n",
      "loss: 0.5135005712509155\n",
      "epoch: 1\n",
      "loss: 0.5134143233299255\n",
      "epoch: 2\n",
      "loss: 0.513327956199646\n",
      "epoch: 3\n",
      "loss: 0.5132416486740112\n",
      "epoch: 4\n",
      "loss: 0.5131549835205078\n",
      "epoch: 5\n",
      "loss: 0.5130689740180969\n",
      "epoch: 6\n",
      "loss: 0.5129828453063965\n",
      "epoch: 7\n",
      "loss: 0.512895941734314\n",
      "epoch: 8\n",
      "loss: 0.5128099322319031\n",
      "epoch: 9\n",
      "loss: 0.5127236843109131\n",
      "Lambda = 6.18e-02, selected 84 features \n",
      "10 epochs, val_objective 2.58e+00, val_loss 5.13e-01, regularization 3.35e+01, l2_regularization 1.64e+01\n",
      "epoch: 0\n",
      "loss: 0.5126376152038574\n",
      "epoch: 1\n",
      "loss: 0.5125524997711182\n",
      "epoch: 2\n",
      "loss: 0.5124692916870117\n",
      "epoch: 3\n",
      "loss: 0.512382447719574\n",
      "epoch: 4\n",
      "loss: 0.5122988224029541\n",
      "epoch: 5\n",
      "loss: 0.512213945388794\n",
      "epoch: 6\n",
      "loss: 0.5121285319328308\n",
      "epoch: 7\n",
      "loss: 0.5120443105697632\n",
      "epoch: 8\n",
      "loss: 0.5119603276252747\n",
      "epoch: 9\n",
      "loss: 0.5118754506111145\n",
      "Lambda = 6.33e-02, selected 84 features \n",
      "10 epochs, val_objective 2.63e+00, val_loss 5.12e-01, regularization 3.35e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.5117906332015991\n",
      "epoch: 1\n",
      "loss: 0.5117076635360718\n",
      "epoch: 2\n",
      "loss: 0.5116230249404907\n",
      "epoch: 3\n",
      "loss: 0.5115398168563843\n",
      "epoch: 4\n",
      "loss: 0.5114560723304749\n",
      "epoch: 5\n",
      "loss: 0.5113732814788818\n",
      "epoch: 6\n",
      "loss: 0.511289656162262\n",
      "epoch: 7\n",
      "loss: 0.5112059712409973\n",
      "epoch: 8\n",
      "loss: 0.5111224055290222\n",
      "epoch: 9\n",
      "loss: 0.511040449142456\n",
      "Lambda = 6.49e-02, selected 84 features \n",
      "10 epochs, val_objective 2.69e+00, val_loss 5.11e-01, regularization 3.35e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.5109558701515198\n",
      "epoch: 1\n",
      "loss: 0.5108728408813477\n",
      "epoch: 2\n",
      "loss: 0.5107908248901367\n",
      "epoch: 3\n",
      "loss: 0.5107086300849915\n",
      "epoch: 4\n",
      "loss: 0.5106261372566223\n",
      "epoch: 5\n",
      "loss: 0.5105436444282532\n",
      "epoch: 6\n",
      "loss: 0.5104613900184631\n",
      "epoch: 7\n",
      "loss: 0.5103790760040283\n",
      "epoch: 8\n",
      "loss: 0.5102964043617249\n",
      "epoch: 9\n",
      "loss: 0.5102147459983826\n",
      "Lambda = 6.65e-02, selected 84 features \n",
      "10 epochs, val_objective 2.74e+00, val_loss 5.10e-01, regularization 3.35e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.510132908821106\n",
      "epoch: 1\n",
      "loss: 0.5100509524345398\n",
      "epoch: 2\n",
      "loss: 0.5099700689315796\n",
      "epoch: 3\n",
      "loss: 0.5098894238471985\n",
      "epoch: 4\n",
      "loss: 0.5098088979721069\n",
      "epoch: 5\n",
      "loss: 0.5097284317016602\n",
      "epoch: 6\n",
      "loss: 0.5096473097801208\n",
      "epoch: 7\n",
      "loss: 0.5095664262771606\n",
      "epoch: 8\n",
      "loss: 0.5094855427742004\n",
      "epoch: 9\n",
      "loss: 0.5094043016433716\n",
      "Lambda = 6.82e-02, selected 84 features \n",
      "10 epochs, val_objective 2.80e+00, val_loss 5.09e-01, regularization 3.36e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.5093234777450562\n",
      "epoch: 1\n",
      "loss: 0.5092432498931885\n",
      "epoch: 2\n",
      "loss: 0.5091632008552551\n",
      "epoch: 3\n",
      "loss: 0.5090839862823486\n",
      "epoch: 4\n",
      "loss: 0.5090032815933228\n",
      "epoch: 5\n",
      "loss: 0.5089234113693237\n",
      "epoch: 6\n",
      "loss: 0.5088436007499695\n",
      "epoch: 7\n",
      "loss: 0.5087639093399048\n",
      "epoch: 8\n",
      "loss: 0.5086846947669983\n",
      "epoch: 9\n",
      "loss: 0.5086050629615784\n",
      "Lambda = 6.99e-02, selected 84 features \n",
      "10 epochs, val_objective 2.86e+00, val_loss 5.09e-01, regularization 3.36e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.5085253715515137\n",
      "epoch: 1\n",
      "loss: 0.5084465146064758\n",
      "epoch: 2\n",
      "loss: 0.5083683729171753\n",
      "epoch: 3\n",
      "loss: 0.5082896947860718\n",
      "epoch: 4\n",
      "loss: 0.5082112550735474\n",
      "epoch: 5\n",
      "loss: 0.5081362128257751\n",
      "epoch: 6\n",
      "loss: 0.508055567741394\n",
      "epoch: 7\n",
      "loss: 0.5079779028892517\n",
      "epoch: 8\n",
      "loss: 0.5079002976417542\n",
      "epoch: 9\n",
      "loss: 0.5078222751617432\n",
      "Lambda = 7.17e-02, selected 84 features \n",
      "10 epochs, val_objective 2.92e+00, val_loss 5.08e-01, regularization 3.36e+01, l2_regularization 1.65e+01\n",
      "epoch: 0\n",
      "loss: 0.5077446103096008\n",
      "epoch: 1\n",
      "loss: 0.5076674222946167\n",
      "epoch: 2\n",
      "loss: 0.507590115070343\n",
      "epoch: 3\n",
      "loss: 0.5075134038925171\n",
      "epoch: 4\n",
      "loss: 0.5074366927146912\n",
      "epoch: 5\n",
      "loss: 0.507359504699707\n",
      "epoch: 6\n",
      "loss: 0.5072826147079468\n",
      "epoch: 7\n",
      "loss: 0.5072070956230164\n",
      "epoch: 8\n",
      "loss: 0.5071297287940979\n",
      "epoch: 9\n",
      "loss: 0.5070533156394958\n",
      "Lambda = 7.34e-02, selected 84 features \n",
      "10 epochs, val_objective 2.98e+00, val_loss 5.07e-01, regularization 3.36e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.5069765448570251\n",
      "epoch: 1\n",
      "loss: 0.5069006681442261\n",
      "epoch: 2\n",
      "loss: 0.5068250298500061\n",
      "epoch: 3\n",
      "loss: 0.5067487955093384\n",
      "epoch: 4\n",
      "loss: 0.5066732168197632\n",
      "epoch: 5\n",
      "loss: 0.5065979957580566\n",
      "epoch: 6\n",
      "loss: 0.5065217018127441\n",
      "epoch: 7\n",
      "loss: 0.5064462423324585\n",
      "epoch: 8\n",
      "loss: 0.5063722133636475\n",
      "epoch: 9\n",
      "loss: 0.5062949061393738\n",
      "Lambda = 7.53e-02, selected 84 features \n",
      "10 epochs, val_objective 3.04e+00, val_loss 5.06e-01, regularization 3.37e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.5062193870544434\n",
      "epoch: 1\n",
      "loss: 0.5061454176902771\n",
      "epoch: 2\n",
      "loss: 0.5060706734657288\n",
      "epoch: 3\n",
      "loss: 0.5059966444969177\n",
      "epoch: 4\n",
      "loss: 0.5059232115745544\n",
      "epoch: 5\n",
      "loss: 0.5058479309082031\n",
      "epoch: 6\n",
      "loss: 0.5057734251022339\n",
      "epoch: 7\n",
      "loss: 0.5056998133659363\n",
      "epoch: 8\n",
      "loss: 0.5056250691413879\n",
      "epoch: 9\n",
      "loss: 0.5055519342422485\n",
      "Lambda = 7.72e-02, selected 84 features \n",
      "10 epochs, val_objective 3.10e+00, val_loss 5.05e-01, regularization 3.37e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.5054773092269897\n",
      "epoch: 1\n",
      "loss: 0.5054048895835876\n",
      "epoch: 2\n",
      "loss: 0.505332350730896\n",
      "epoch: 3\n",
      "loss: 0.5052595138549805\n",
      "epoch: 4\n",
      "loss: 0.5051870942115784\n",
      "epoch: 5\n",
      "loss: 0.5051136612892151\n",
      "epoch: 6\n",
      "loss: 0.5050409436225891\n",
      "epoch: 7\n",
      "loss: 0.5049681067466736\n",
      "epoch: 8\n",
      "loss: 0.5048955082893372\n",
      "epoch: 9\n",
      "loss: 0.5048222541809082\n",
      "Lambda = 7.91e-02, selected 84 features \n",
      "10 epochs, val_objective 3.17e+00, val_loss 5.05e-01, regularization 3.37e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.50475013256073\n",
      "epoch: 1\n",
      "loss: 0.5046790838241577\n",
      "epoch: 2\n",
      "loss: 0.5046067237854004\n",
      "epoch: 3\n",
      "loss: 0.5045352578163147\n",
      "epoch: 4\n",
      "loss: 0.5044640302658081\n",
      "epoch: 5\n",
      "loss: 0.5043922662734985\n",
      "epoch: 6\n",
      "loss: 0.5043215751647949\n",
      "epoch: 7\n",
      "loss: 0.5042504668235779\n",
      "epoch: 8\n",
      "loss: 0.5041792988777161\n",
      "epoch: 9\n",
      "loss: 0.5041085481643677\n",
      "Lambda = 8.11e-02, selected 84 features \n",
      "10 epochs, val_objective 3.24e+00, val_loss 5.04e-01, regularization 3.37e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.5040372014045715\n",
      "epoch: 1\n",
      "loss: 0.5039669275283813\n",
      "epoch: 2\n",
      "loss: 0.5038964748382568\n",
      "epoch: 3\n",
      "loss: 0.5038257241249084\n",
      "epoch: 4\n",
      "loss: 0.5037562251091003\n",
      "epoch: 5\n",
      "loss: 0.5036863088607788\n",
      "epoch: 6\n",
      "loss: 0.5036172866821289\n",
      "epoch: 7\n",
      "loss: 0.503547728061676\n",
      "epoch: 8\n",
      "loss: 0.5034798383712769\n",
      "epoch: 9\n",
      "loss: 0.5034089684486389\n",
      "Lambda = 8.31e-02, selected 84 features \n",
      "10 epochs, val_objective 3.31e+00, val_loss 5.03e-01, regularization 3.37e+01, l2_regularization 1.66e+01\n",
      "epoch: 0\n",
      "loss: 0.5033384561538696\n",
      "epoch: 1\n",
      "loss: 0.5032698512077332\n",
      "epoch: 2\n",
      "loss: 0.5032017230987549\n",
      "epoch: 3\n",
      "loss: 0.5031327605247498\n",
      "epoch: 4\n",
      "loss: 0.5030645132064819\n",
      "epoch: 5\n",
      "loss: 0.5029958486557007\n",
      "epoch: 6\n",
      "loss: 0.5029288530349731\n",
      "epoch: 7\n",
      "loss: 0.5028585195541382\n",
      "epoch: 8\n",
      "loss: 0.5027903318405151\n",
      "epoch: 9\n",
      "loss: 0.5027213096618652\n",
      "Lambda = 8.52e-02, selected 84 features \n",
      "10 epochs, val_objective 3.38e+00, val_loss 5.03e-01, regularization 3.37e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.5026533603668213\n",
      "epoch: 1\n",
      "loss: 0.5025864243507385\n",
      "epoch: 2\n",
      "loss: 0.5025199055671692\n",
      "epoch: 3\n",
      "loss: 0.5024532079696655\n",
      "epoch: 4\n",
      "loss: 0.5023851990699768\n",
      "epoch: 5\n",
      "loss: 0.5023183822631836\n",
      "epoch: 6\n",
      "loss: 0.5022510886192322\n",
      "epoch: 7\n",
      "loss: 0.5021839141845703\n",
      "epoch: 8\n",
      "loss: 0.502116858959198\n",
      "epoch: 9\n",
      "loss: 0.5020506381988525\n",
      "Lambda = 8.73e-02, selected 84 features \n",
      "10 epochs, val_objective 3.45e+00, val_loss 5.02e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.5019830465316772\n",
      "epoch: 1\n",
      "loss: 0.5019172430038452\n",
      "epoch: 2\n",
      "loss: 0.501851499080658\n",
      "epoch: 3\n",
      "loss: 0.5017856955528259\n",
      "epoch: 4\n",
      "loss: 0.5017198324203491\n",
      "epoch: 5\n",
      "loss: 0.5016558170318604\n",
      "epoch: 6\n",
      "loss: 0.5015888810157776\n",
      "epoch: 7\n",
      "loss: 0.5015246272087097\n",
      "epoch: 8\n",
      "loss: 0.5014581680297852\n",
      "epoch: 9\n",
      "loss: 0.5013926029205322\n",
      "Lambda = 8.95e-02, selected 84 features \n",
      "10 epochs, val_objective 3.52e+00, val_loss 5.01e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.5013274550437927\n",
      "epoch: 1\n",
      "loss: 0.5012632012367249\n",
      "epoch: 2\n",
      "loss: 0.5011996030807495\n",
      "epoch: 3\n",
      "loss: 0.5011359453201294\n",
      "epoch: 4\n",
      "loss: 0.501072883605957\n",
      "epoch: 5\n",
      "loss: 0.501010000705719\n",
      "epoch: 6\n",
      "loss: 0.5009438991546631\n",
      "epoch: 7\n",
      "loss: 0.500880777835846\n",
      "epoch: 8\n",
      "loss: 0.5008168816566467\n",
      "epoch: 9\n",
      "loss: 0.5007530450820923\n",
      "Lambda = 9.17e-02, selected 84 features \n",
      "10 epochs, val_objective 3.60e+00, val_loss 5.01e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.5006891489028931\n",
      "epoch: 1\n",
      "loss: 0.5006268620491028\n",
      "epoch: 2\n",
      "loss: 0.5005645751953125\n",
      "epoch: 3\n",
      "loss: 0.500501811504364\n",
      "epoch: 4\n",
      "loss: 0.5004395842552185\n",
      "epoch: 5\n",
      "loss: 0.5003765821456909\n",
      "epoch: 6\n",
      "loss: 0.5003151893615723\n",
      "epoch: 7\n",
      "loss: 0.5002515912055969\n",
      "epoch: 8\n",
      "loss: 0.500189483165741\n",
      "epoch: 9\n",
      "loss: 0.5001275539398193\n",
      "Lambda = 9.40e-02, selected 84 features \n",
      "10 epochs, val_objective 3.68e+00, val_loss 5.00e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.5000650882720947\n",
      "epoch: 1\n",
      "loss: 0.5000043511390686\n",
      "epoch: 2\n",
      "loss: 0.49994271993637085\n",
      "epoch: 3\n",
      "loss: 0.4998817443847656\n",
      "epoch: 4\n",
      "loss: 0.4998205304145813\n",
      "epoch: 5\n",
      "loss: 0.49975961446762085\n",
      "epoch: 6\n",
      "loss: 0.499698281288147\n",
      "epoch: 7\n",
      "loss: 0.49963799118995667\n",
      "epoch: 8\n",
      "loss: 0.49957695603370667\n",
      "epoch: 9\n",
      "loss: 0.4995160698890686\n",
      "Lambda = 9.64e-02, selected 84 features \n",
      "10 epochs, val_objective 3.76e+00, val_loss 4.99e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.4994550049304962\n",
      "epoch: 1\n",
      "loss: 0.4993954002857208\n",
      "epoch: 2\n",
      "loss: 0.49933677911758423\n",
      "epoch: 3\n",
      "loss: 0.49927622079849243\n",
      "epoch: 4\n",
      "loss: 0.4992166757583618\n",
      "epoch: 5\n",
      "loss: 0.49915799498558044\n",
      "epoch: 6\n",
      "loss: 0.4990980625152588\n",
      "epoch: 7\n",
      "loss: 0.4990381896495819\n",
      "epoch: 8\n",
      "loss: 0.49897894263267517\n",
      "epoch: 9\n",
      "loss: 0.4989200234413147\n",
      "Lambda = 9.88e-02, selected 84 features \n",
      "10 epochs, val_objective 3.84e+00, val_loss 4.99e-01, regularization 3.38e+01, l2_regularization 1.67e+01\n",
      "epoch: 0\n",
      "loss: 0.49886032938957214\n",
      "epoch: 1\n",
      "loss: 0.49880313873291016\n",
      "epoch: 2\n",
      "loss: 0.49874529242515564\n",
      "epoch: 3\n",
      "loss: 0.4986870586872101\n",
      "epoch: 4\n",
      "loss: 0.4986298680305481\n",
      "epoch: 5\n",
      "loss: 0.49857157468795776\n",
      "epoch: 6\n",
      "loss: 0.49851393699645996\n",
      "epoch: 7\n",
      "loss: 0.49845731258392334\n",
      "epoch: 8\n",
      "loss: 0.49839869141578674\n",
      "epoch: 9\n",
      "loss: 0.4983411133289337\n",
      "Lambda = 1.01e-01, selected 84 features \n",
      "10 epochs, val_objective 3.92e+00, val_loss 4.98e-01, regularization 3.38e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.4982834756374359\n",
      "epoch: 1\n",
      "loss: 0.49822714924812317\n",
      "epoch: 2\n",
      "loss: 0.49817079305648804\n",
      "epoch: 3\n",
      "loss: 0.49811479449272156\n",
      "epoch: 4\n",
      "loss: 0.4980586767196655\n",
      "epoch: 5\n",
      "loss: 0.49800199270248413\n",
      "epoch: 6\n",
      "loss: 0.4979458749294281\n",
      "epoch: 7\n",
      "loss: 0.4978899657726288\n",
      "epoch: 8\n",
      "loss: 0.49783584475517273\n",
      "epoch: 9\n",
      "loss: 0.4977778196334839\n",
      "Lambda = 1.04e-01, selected 84 features \n",
      "10 epochs, val_objective 4.01e+00, val_loss 4.98e-01, regularization 3.38e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.4977220296859741\n",
      "epoch: 1\n",
      "loss: 0.4976697564125061\n",
      "epoch: 2\n",
      "loss: 0.4976139962673187\n",
      "epoch: 3\n",
      "loss: 0.4975592792034149\n",
      "epoch: 4\n",
      "loss: 0.49750497937202454\n",
      "epoch: 5\n",
      "loss: 0.4974505603313446\n",
      "epoch: 6\n",
      "loss: 0.4973953068256378\n",
      "epoch: 7\n",
      "loss: 0.4973413050174713\n",
      "epoch: 8\n",
      "loss: 0.49728670716285706\n",
      "epoch: 9\n",
      "loss: 0.49723291397094727\n",
      "Lambda = 1.06e-01, selected 84 features \n",
      "10 epochs, val_objective 4.10e+00, val_loss 4.97e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.4971776008605957\n",
      "epoch: 1\n",
      "loss: 0.4971250593662262\n",
      "epoch: 2\n",
      "loss: 0.49707216024398804\n",
      "epoch: 3\n",
      "loss: 0.49702000617980957\n",
      "epoch: 4\n",
      "loss: 0.49696654081344604\n",
      "epoch: 5\n",
      "loss: 0.4969136416912079\n",
      "epoch: 6\n",
      "loss: 0.49686160683631897\n",
      "epoch: 7\n",
      "loss: 0.4968087673187256\n",
      "epoch: 8\n",
      "loss: 0.4967559576034546\n",
      "epoch: 9\n",
      "loss: 0.4967029392719269\n",
      "Lambda = 1.09e-01, selected 84 features \n",
      "10 epochs, val_objective 4.19e+00, val_loss 4.97e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.49665170907974243\n",
      "epoch: 1\n",
      "loss: 0.4965990483760834\n",
      "epoch: 2\n",
      "loss: 0.4965478777885437\n",
      "epoch: 3\n",
      "loss: 0.49649709463119507\n",
      "epoch: 4\n",
      "loss: 0.49644550681114197\n",
      "epoch: 5\n",
      "loss: 0.4963947534561157\n",
      "epoch: 6\n",
      "loss: 0.49634313583374023\n",
      "epoch: 7\n",
      "loss: 0.4962918758392334\n",
      "epoch: 8\n",
      "loss: 0.4962407648563385\n",
      "epoch: 9\n",
      "loss: 0.49618974328041077\n",
      "Lambda = 1.12e-01, selected 84 features \n",
      "10 epochs, val_objective 4.28e+00, val_loss 4.96e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.4961388111114502\n",
      "epoch: 1\n",
      "loss: 0.4960895776748657\n",
      "epoch: 2\n",
      "loss: 0.4960401654243469\n",
      "epoch: 3\n",
      "loss: 0.495990127325058\n",
      "epoch: 4\n",
      "loss: 0.49594104290008545\n",
      "epoch: 5\n",
      "loss: 0.4958917796611786\n",
      "epoch: 6\n",
      "loss: 0.49584221839904785\n",
      "epoch: 7\n",
      "loss: 0.49579283595085144\n",
      "epoch: 8\n",
      "loss: 0.49574291706085205\n",
      "epoch: 9\n",
      "loss: 0.4956934452056885\n",
      "Lambda = 1.15e-01, selected 84 features \n",
      "10 epochs, val_objective 4.38e+00, val_loss 4.96e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.49564409255981445\n",
      "epoch: 1\n",
      "loss: 0.49559614062309265\n",
      "epoch: 2\n",
      "loss: 0.4955484867095947\n",
      "epoch: 3\n",
      "loss: 0.4955005347728729\n",
      "epoch: 4\n",
      "loss: 0.4954527020454407\n",
      "epoch: 5\n",
      "loss: 0.49540549516677856\n",
      "epoch: 6\n",
      "loss: 0.4953576922416687\n",
      "epoch: 7\n",
      "loss: 0.495309978723526\n",
      "epoch: 8\n",
      "loss: 0.49526193737983704\n",
      "epoch: 9\n",
      "loss: 0.4952153265476227\n",
      "Lambda = 1.17e-01, selected 84 features \n",
      "10 epochs, val_objective 4.47e+00, val_loss 4.95e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.49516651034355164\n",
      "epoch: 1\n",
      "loss: 0.49512094259262085\n",
      "epoch: 2\n",
      "loss: 0.4950747787952423\n",
      "epoch: 3\n",
      "loss: 0.49502918124198914\n",
      "epoch: 4\n",
      "loss: 0.49498265981674194\n",
      "epoch: 5\n",
      "loss: 0.4949365556240082\n",
      "epoch: 6\n",
      "loss: 0.49489086866378784\n",
      "epoch: 7\n",
      "loss: 0.4948447346687317\n",
      "epoch: 8\n",
      "loss: 0.4947986602783203\n",
      "epoch: 9\n",
      "loss: 0.4947528839111328\n",
      "Lambda = 1.20e-01, selected 84 features \n",
      "10 epochs, val_objective 4.57e+00, val_loss 4.95e-01, regularization 3.39e+01, l2_regularization 1.68e+01\n",
      "epoch: 0\n",
      "loss: 0.49470755457878113\n",
      "epoch: 1\n",
      "loss: 0.4946627914905548\n",
      "epoch: 2\n",
      "loss: 0.4946185052394867\n",
      "epoch: 3\n",
      "loss: 0.49457424879074097\n",
      "epoch: 4\n",
      "loss: 0.49453121423721313\n",
      "epoch: 5\n",
      "loss: 0.4944864511489868\n",
      "epoch: 6\n",
      "loss: 0.4944422245025635\n",
      "epoch: 7\n",
      "loss: 0.494398295879364\n",
      "epoch: 8\n",
      "loss: 0.4943547248840332\n",
      "epoch: 9\n",
      "loss: 0.4943109452724457\n",
      "Lambda = 1.23e-01, selected 84 features \n",
      "10 epochs, val_objective 4.67e+00, val_loss 4.94e-01, regularization 3.39e+01, l2_regularization 1.69e+01\n",
      "epoch: 0\n",
      "loss: 0.4942665994167328\n",
      "epoch: 1\n",
      "loss: 0.49422499537467957\n",
      "epoch: 2\n",
      "loss: 0.4941820204257965\n",
      "epoch: 3\n",
      "loss: 0.49414074420928955\n",
      "epoch: 4\n",
      "loss: 0.49409791827201843\n",
      "epoch: 5\n",
      "loss: 0.4940560758113861\n",
      "epoch: 6\n",
      "loss: 0.49401330947875977\n",
      "epoch: 7\n",
      "loss: 0.49397146701812744\n",
      "epoch: 8\n",
      "loss: 0.4939306378364563\n",
      "epoch: 9\n",
      "loss: 0.4938872456550598\n",
      "Lambda = 1.26e-01, selected 84 features \n",
      "10 epochs, val_objective 4.78e+00, val_loss 4.94e-01, regularization 3.39e+01, l2_regularization 1.69e+01\n",
      "epoch: 0\n",
      "loss: 0.4938454329967499\n",
      "epoch: 1\n",
      "loss: 0.49380558729171753\n",
      "epoch: 2\n",
      "loss: 0.49376487731933594\n",
      "epoch: 3\n",
      "loss: 0.49372485280036926\n",
      "epoch: 4\n",
      "loss: 0.49368441104888916\n",
      "epoch: 5\n",
      "loss: 0.4936444163322449\n",
      "epoch: 6\n",
      "loss: 0.493604838848114\n",
      "epoch: 7\n",
      "loss: 0.493563711643219\n",
      "epoch: 8\n",
      "loss: 0.49352362751960754\n",
      "epoch: 9\n",
      "loss: 0.49348291754722595\n",
      "Lambda = 1.30e-01, selected 84 features \n",
      "10 epochs, val_objective 4.88e+00, val_loss 4.93e-01, regularization 3.39e+01, l2_regularization 1.69e+01\n",
      "epoch: 0\n",
      "loss: 0.49344271421432495\n",
      "epoch: 1\n",
      "loss: 0.49340444803237915\n",
      "epoch: 2\n",
      "loss: 0.49336549639701843\n",
      "epoch: 3\n",
      "loss: 0.4933273494243622\n",
      "epoch: 4\n",
      "loss: 0.49328866600990295\n",
      "epoch: 5\n",
      "loss: 0.4932500123977661\n",
      "epoch: 6\n",
      "loss: 0.4932113289833069\n",
      "epoch: 7\n",
      "loss: 0.49317437410354614\n",
      "epoch: 8\n",
      "loss: 0.49313437938690186\n",
      "epoch: 9\n",
      "loss: 0.49309614300727844\n",
      "Lambda = 1.33e-01, selected 84 features \n",
      "10 epochs, val_objective 4.99e+00, val_loss 4.93e-01, regularization 3.39e+01, l2_regularization 1.69e+01\n",
      "epoch: 0\n",
      "loss: 0.49305814504623413\n",
      "epoch: 1\n",
      "loss: 0.4930219054222107\n",
      "epoch: 2\n",
      "loss: 0.49298566579818726\n",
      "epoch: 3\n",
      "loss: 0.49294865131378174\n",
      "epoch: 4\n",
      "loss: 0.4929114580154419\n",
      "epoch: 5\n",
      "loss: 0.49287474155426025\n",
      "epoch: 6\n",
      "loss: 0.4928377866744995\n",
      "epoch: 7\n",
      "loss: 0.49280107021331787\n",
      "epoch: 8\n",
      "loss: 0.4927636682987213\n",
      "epoch: 9\n",
      "loss: 0.49272653460502625\n",
      "Lambda = 1.36e-01, selected 84 features \n",
      "10 epochs, val_objective 5.11e+00, val_loss 4.93e-01, regularization 3.39e+01, l2_regularization 1.69e+01\n",
      "epoch: 0\n",
      "loss: 4.988934516906738\n",
      "epoch: 1\n",
      "loss: 4.967841148376465\n",
      "epoch: 2\n",
      "loss: 4.947465896606445\n",
      "epoch: 3\n",
      "loss: 4.927479267120361\n",
      "epoch: 4\n",
      "loss: 4.907822132110596\n",
      "epoch: 5\n",
      "loss: 4.888700485229492\n",
      "epoch: 6\n",
      "loss: 4.87002420425415\n",
      "epoch: 7\n",
      "loss: 4.851817607879639\n",
      "epoch: 8\n",
      "loss: 4.834465980529785\n",
      "epoch: 9\n",
      "loss: 4.817636013031006\n",
      "epoch: 10\n",
      "loss: 4.801114082336426\n",
      "epoch: 11\n",
      "loss: 4.784931182861328\n",
      "epoch: 12\n",
      "loss: 4.769046306610107\n",
      "epoch: 13\n",
      "loss: 4.7536845207214355\n",
      "epoch: 14\n",
      "loss: 4.739511489868164\n",
      "epoch: 15\n",
      "loss: 4.725949287414551\n",
      "epoch: 16\n",
      "loss: 4.712684154510498\n",
      "epoch: 17\n",
      "loss: 4.699960231781006\n",
      "epoch: 18\n",
      "loss: 4.687402725219727\n",
      "epoch: 19\n",
      "loss: 4.674999237060547\n",
      "epoch: 20\n",
      "loss: 4.662703990936279\n",
      "epoch: 21\n",
      "loss: 4.650566577911377\n",
      "epoch: 22\n",
      "loss: 4.63859224319458\n",
      "epoch: 23\n",
      "loss: 4.626996040344238\n",
      "epoch: 24\n",
      "loss: 4.6154656410217285\n",
      "epoch: 25\n",
      "loss: 4.603992938995361\n",
      "epoch: 26\n",
      "loss: 4.592909336090088\n",
      "epoch: 27\n",
      "loss: 4.582374572753906\n",
      "epoch: 28\n",
      "loss: 4.572081089019775\n",
      "epoch: 29\n",
      "loss: 4.561834812164307\n",
      "epoch: 30\n",
      "loss: 4.551592826843262\n",
      "epoch: 31\n",
      "loss: 4.541404724121094\n",
      "epoch: 32\n",
      "loss: 4.531553268432617\n",
      "epoch: 33\n",
      "loss: 4.521735191345215\n",
      "epoch: 34\n",
      "loss: 4.511999607086182\n",
      "epoch: 35\n",
      "loss: 4.502277851104736\n",
      "epoch: 36\n",
      "loss: 4.4925432205200195\n",
      "epoch: 37\n",
      "loss: 4.482823848724365\n",
      "epoch: 38\n",
      "loss: 4.473175048828125\n",
      "epoch: 39\n",
      "loss: 4.463521957397461\n",
      "epoch: 40\n",
      "loss: 4.453856945037842\n",
      "epoch: 41\n",
      "loss: 4.444219589233398\n",
      "epoch: 42\n",
      "loss: 4.434562683105469\n",
      "epoch: 43\n",
      "loss: 4.424951076507568\n",
      "epoch: 44\n",
      "loss: 4.415400981903076\n",
      "epoch: 45\n",
      "loss: 4.405874729156494\n",
      "epoch: 46\n",
      "loss: 4.396328926086426\n",
      "epoch: 47\n",
      "loss: 4.3868231773376465\n",
      "epoch: 48\n",
      "loss: 4.377947807312012\n",
      "epoch: 49\n",
      "loss: 4.369158744812012\n",
      "epoch: 50\n",
      "loss: 4.360405445098877\n",
      "epoch: 51\n",
      "loss: 4.351581573486328\n",
      "epoch: 52\n",
      "loss: 4.34265661239624\n",
      "epoch: 53\n",
      "loss: 4.333661079406738\n",
      "epoch: 54\n",
      "loss: 4.324814319610596\n",
      "epoch: 55\n",
      "loss: 4.315921783447266\n",
      "epoch: 56\n",
      "loss: 4.306935787200928\n",
      "epoch: 57\n",
      "loss: 4.298212051391602\n",
      "epoch: 58\n",
      "loss: 4.289447784423828\n",
      "epoch: 59\n",
      "loss: 4.280625820159912\n",
      "epoch: 60\n",
      "loss: 4.271814823150635\n",
      "epoch: 61\n",
      "loss: 4.262975692749023\n",
      "epoch: 62\n",
      "loss: 4.254064559936523\n",
      "epoch: 63\n",
      "loss: 4.24526834487915\n",
      "epoch: 64\n",
      "loss: 4.236437797546387\n",
      "epoch: 65\n",
      "loss: 4.227598667144775\n",
      "epoch: 66\n",
      "loss: 4.2188401222229\n",
      "epoch: 67\n",
      "loss: 4.210164546966553\n",
      "epoch: 68\n",
      "loss: 4.201572895050049\n",
      "epoch: 69\n",
      "loss: 4.193121433258057\n",
      "epoch: 70\n",
      "loss: 4.184811592102051\n",
      "epoch: 71\n",
      "loss: 4.176508903503418\n",
      "epoch: 72\n",
      "loss: 4.16841459274292\n",
      "epoch: 73\n",
      "loss: 4.160294532775879\n",
      "epoch: 74\n",
      "loss: 4.152143955230713\n",
      "epoch: 75\n",
      "loss: 4.1439385414123535\n",
      "epoch: 76\n",
      "loss: 4.13572883605957\n",
      "epoch: 77\n",
      "loss: 4.127482891082764\n",
      "epoch: 78\n",
      "loss: 4.119148254394531\n",
      "epoch: 79\n",
      "loss: 4.1107916831970215\n",
      "epoch: 80\n",
      "loss: 4.102336406707764\n",
      "epoch: 81\n",
      "loss: 4.093864917755127\n",
      "epoch: 82\n",
      "loss: 4.08538818359375\n",
      "epoch: 83\n",
      "loss: 4.076940536499023\n",
      "epoch: 84\n",
      "loss: 4.068380832672119\n",
      "epoch: 85\n",
      "loss: 4.059727191925049\n",
      "epoch: 86\n",
      "loss: 4.051146507263184\n",
      "epoch: 87\n",
      "loss: 4.042520523071289\n",
      "epoch: 88\n",
      "loss: 4.033790588378906\n",
      "epoch: 89\n",
      "loss: 4.025070667266846\n",
      "epoch: 90\n",
      "loss: 4.016283988952637\n",
      "epoch: 91\n",
      "loss: 4.007647514343262\n",
      "epoch: 92\n",
      "loss: 3.999089002609253\n",
      "epoch: 93\n",
      "loss: 3.9905004501342773\n",
      "epoch: 94\n",
      "loss: 3.9820384979248047\n",
      "epoch: 95\n",
      "loss: 3.9735870361328125\n",
      "epoch: 96\n",
      "loss: 3.9651145935058594\n",
      "epoch: 97\n",
      "loss: 3.9565930366516113\n",
      "epoch: 98\n",
      "loss: 3.948040008544922\n",
      "epoch: 99\n",
      "loss: 3.9393601417541504\n",
      "epoch: 100\n",
      "loss: 3.9306163787841797\n",
      "epoch: 101\n",
      "loss: 3.921835422515869\n",
      "epoch: 102\n",
      "loss: 3.9130401611328125\n",
      "epoch: 103\n",
      "loss: 3.904177188873291\n",
      "epoch: 104\n",
      "loss: 3.8952765464782715\n",
      "epoch: 105\n",
      "loss: 3.886417865753174\n",
      "epoch: 106\n",
      "loss: 3.8775453567504883\n",
      "epoch: 107\n",
      "loss: 3.8687868118286133\n",
      "epoch: 108\n",
      "loss: 3.860081195831299\n",
      "epoch: 109\n",
      "loss: 3.851370334625244\n",
      "epoch: 110\n",
      "loss: 3.8426694869995117\n",
      "epoch: 111\n",
      "loss: 3.8338236808776855\n",
      "epoch: 112\n",
      "loss: 3.824930191040039\n",
      "epoch: 113\n",
      "loss: 3.816019058227539\n",
      "epoch: 114\n",
      "loss: 3.807051420211792\n",
      "epoch: 115\n",
      "loss: 3.797914981842041\n",
      "epoch: 116\n",
      "loss: 3.788727283477783\n",
      "epoch: 117\n",
      "loss: 3.7795140743255615\n",
      "epoch: 118\n",
      "loss: 3.7702369689941406\n",
      "epoch: 119\n",
      "loss: 3.760946273803711\n",
      "epoch: 120\n",
      "loss: 3.7517004013061523\n",
      "epoch: 121\n",
      "loss: 3.742366313934326\n",
      "epoch: 122\n",
      "loss: 3.7330174446105957\n",
      "epoch: 123\n",
      "loss: 3.72371244430542\n",
      "epoch: 124\n",
      "loss: 3.7144622802734375\n",
      "epoch: 125\n",
      "loss: 3.7051851749420166\n",
      "epoch: 126\n",
      "loss: 3.6958250999450684\n",
      "epoch: 127\n",
      "loss: 3.686276435852051\n",
      "epoch: 128\n",
      "loss: 3.6766700744628906\n",
      "epoch: 129\n",
      "loss: 3.667147636413574\n",
      "epoch: 130\n",
      "loss: 3.657541036605835\n",
      "epoch: 131\n",
      "loss: 3.6478896141052246\n",
      "epoch: 132\n",
      "loss: 3.638343334197998\n",
      "epoch: 133\n",
      "loss: 3.6287453174591064\n",
      "epoch: 134\n",
      "loss: 3.6191139221191406\n",
      "epoch: 135\n",
      "loss: 3.6094305515289307\n",
      "epoch: 136\n",
      "loss: 3.599790096282959\n",
      "epoch: 137\n",
      "loss: 3.590130567550659\n",
      "epoch: 138\n",
      "loss: 3.5804896354675293\n",
      "epoch: 139\n",
      "loss: 3.5708446502685547\n",
      "epoch: 140\n",
      "loss: 3.5611655712127686\n",
      "epoch: 141\n",
      "loss: 3.5515079498291016\n",
      "epoch: 142\n",
      "loss: 3.541903495788574\n",
      "epoch: 143\n",
      "loss: 3.5322604179382324\n",
      "epoch: 144\n",
      "loss: 3.522669792175293\n",
      "epoch: 145\n",
      "loss: 3.5131452083587646\n",
      "epoch: 146\n",
      "loss: 3.5035510063171387\n",
      "epoch: 147\n",
      "loss: 3.493898391723633\n",
      "epoch: 148\n",
      "loss: 3.4842519760131836\n",
      "epoch: 149\n",
      "loss: 3.474600315093994\n",
      "epoch: 150\n",
      "loss: 3.46490216255188\n",
      "epoch: 151\n",
      "loss: 3.4552783966064453\n",
      "epoch: 152\n",
      "loss: 3.4457154273986816\n",
      "epoch: 153\n",
      "loss: 3.4361889362335205\n",
      "epoch: 154\n",
      "loss: 3.426685333251953\n",
      "epoch: 155\n",
      "loss: 3.417232036590576\n",
      "epoch: 156\n",
      "loss: 3.40775728225708\n",
      "epoch: 157\n",
      "loss: 3.3982481956481934\n",
      "epoch: 158\n",
      "loss: 3.3888020515441895\n",
      "epoch: 159\n",
      "loss: 3.3793084621429443\n",
      "epoch: 160\n",
      "loss: 3.3698501586914062\n",
      "epoch: 161\n",
      "loss: 3.360459327697754\n",
      "epoch: 162\n",
      "loss: 3.351083755493164\n",
      "epoch: 163\n",
      "loss: 3.3417367935180664\n",
      "epoch: 164\n",
      "loss: 3.3323802947998047\n",
      "epoch: 165\n",
      "loss: 3.323075532913208\n",
      "epoch: 166\n",
      "loss: 3.313724994659424\n",
      "epoch: 167\n",
      "loss: 3.304378032684326\n",
      "epoch: 168\n",
      "loss: 3.295151710510254\n",
      "epoch: 169\n",
      "loss: 3.2858762741088867\n",
      "epoch: 170\n",
      "loss: 3.2766265869140625\n",
      "epoch: 171\n",
      "loss: 3.267427444458008\n",
      "epoch: 172\n",
      "loss: 3.2583298683166504\n",
      "epoch: 173\n",
      "loss: 3.2491979598999023\n",
      "epoch: 174\n",
      "loss: 3.240079879760742\n",
      "epoch: 175\n",
      "loss: 3.2310824394226074\n",
      "epoch: 176\n",
      "loss: 3.222071647644043\n",
      "epoch: 177\n",
      "loss: 3.2131409645080566\n",
      "epoch: 178\n",
      "loss: 3.204191207885742\n",
      "epoch: 179\n",
      "loss: 3.195140838623047\n",
      "epoch: 180\n",
      "loss: 3.1862857341766357\n",
      "epoch: 181\n",
      "loss: 3.1774439811706543\n",
      "epoch: 182\n",
      "loss: 3.1685714721679688\n",
      "epoch: 183\n",
      "loss: 3.1598048210144043\n",
      "epoch: 184\n",
      "loss: 3.1509482860565186\n",
      "epoch: 185\n",
      "loss: 3.1421642303466797\n",
      "epoch: 186\n",
      "loss: 3.1334190368652344\n",
      "epoch: 187\n",
      "loss: 3.124660015106201\n",
      "epoch: 188\n",
      "loss: 3.1158862113952637\n",
      "epoch: 189\n",
      "loss: 3.10725474357605\n",
      "epoch: 190\n",
      "loss: 3.098590850830078\n",
      "epoch: 191\n",
      "loss: 3.089961051940918\n",
      "epoch: 192\n",
      "loss: 3.081357955932617\n",
      "epoch: 193\n",
      "loss: 3.0727977752685547\n",
      "epoch: 194\n",
      "loss: 3.0643749237060547\n",
      "epoch: 195\n",
      "loss: 3.055972099304199\n",
      "epoch: 196\n",
      "loss: 3.0475387573242188\n",
      "epoch: 197\n",
      "loss: 3.0390665531158447\n",
      "epoch: 198\n",
      "loss: 3.0306906700134277\n",
      "epoch: 199\n",
      "loss: 3.022366523742676\n",
      "epoch: 200\n",
      "loss: 3.0140163898468018\n",
      "epoch: 201\n",
      "loss: 3.0056710243225098\n",
      "epoch: 202\n",
      "loss: 2.9972801208496094\n",
      "epoch: 203\n",
      "loss: 2.9890294075012207\n",
      "epoch: 204\n",
      "loss: 2.9808359146118164\n",
      "epoch: 205\n",
      "loss: 2.9726500511169434\n",
      "epoch: 206\n",
      "loss: 2.9644603729248047\n",
      "epoch: 207\n",
      "loss: 2.9563519954681396\n",
      "epoch: 208\n",
      "loss: 2.9482030868530273\n",
      "epoch: 209\n",
      "loss: 2.940098762512207\n",
      "epoch: 210\n",
      "loss: 2.932131767272949\n",
      "epoch: 211\n",
      "loss: 2.9241466522216797\n",
      "epoch: 212\n",
      "loss: 2.916231155395508\n",
      "epoch: 213\n",
      "loss: 2.9082188606262207\n",
      "epoch: 214\n",
      "loss: 2.900259017944336\n",
      "epoch: 215\n",
      "loss: 2.8924076557159424\n",
      "epoch: 216\n",
      "loss: 2.8845951557159424\n",
      "epoch: 217\n",
      "loss: 2.87686824798584\n",
      "epoch: 218\n",
      "loss: 2.869185447692871\n",
      "epoch: 219\n",
      "loss: 2.8614158630371094\n",
      "epoch: 220\n",
      "loss: 2.8537864685058594\n",
      "epoch: 221\n",
      "loss: 2.8461456298828125\n",
      "epoch: 222\n",
      "loss: 2.838460922241211\n",
      "epoch: 223\n",
      "loss: 2.830886125564575\n",
      "epoch: 224\n",
      "loss: 2.8232779502868652\n",
      "epoch: 225\n",
      "loss: 2.8157777786254883\n",
      "epoch: 226\n",
      "loss: 2.8083081245422363\n",
      "epoch: 227\n",
      "loss: 2.8007826805114746\n",
      "epoch: 228\n",
      "loss: 2.7932820320129395\n",
      "epoch: 229\n",
      "loss: 2.785834789276123\n",
      "epoch: 230\n",
      "loss: 2.778458595275879\n",
      "epoch: 231\n",
      "loss: 2.771091938018799\n",
      "epoch: 232\n",
      "loss: 2.7636990547180176\n",
      "epoch: 233\n",
      "loss: 2.7562897205352783\n",
      "epoch: 234\n",
      "loss: 2.748936176300049\n",
      "epoch: 235\n",
      "loss: 2.7417006492614746\n",
      "epoch: 236\n",
      "loss: 2.7343878746032715\n",
      "epoch: 237\n",
      "loss: 2.7271244525909424\n",
      "epoch: 238\n",
      "loss: 2.7199411392211914\n",
      "epoch: 239\n",
      "loss: 2.7128119468688965\n",
      "epoch: 240\n",
      "loss: 2.70572566986084\n",
      "epoch: 241\n",
      "loss: 2.698552131652832\n",
      "epoch: 242\n",
      "loss: 2.6915180683135986\n",
      "epoch: 243\n",
      "loss: 2.6843996047973633\n",
      "epoch: 244\n",
      "loss: 2.67722225189209\n",
      "epoch: 245\n",
      "loss: 2.67020320892334\n",
      "epoch: 246\n",
      "loss: 2.6631550788879395\n",
      "epoch: 247\n",
      "loss: 2.656073570251465\n",
      "epoch: 248\n",
      "loss: 2.649190902709961\n",
      "epoch: 249\n",
      "loss: 2.642274856567383\n",
      "epoch: 250\n",
      "loss: 2.635193347930908\n",
      "epoch: 251\n",
      "loss: 2.6282286643981934\n",
      "epoch: 252\n",
      "loss: 2.6212964057922363\n",
      "epoch: 253\n",
      "loss: 2.614471435546875\n",
      "epoch: 254\n",
      "loss: 2.607536792755127\n",
      "epoch: 255\n",
      "loss: 2.6008195877075195\n",
      "epoch: 256\n",
      "loss: 2.594238758087158\n",
      "epoch: 257\n",
      "loss: 2.587440013885498\n",
      "epoch: 258\n",
      "loss: 2.5808629989624023\n",
      "epoch: 259\n",
      "loss: 2.5743136405944824\n",
      "epoch: 260\n",
      "loss: 2.5676798820495605\n",
      "epoch: 261\n",
      "loss: 2.561101198196411\n",
      "epoch: 262\n",
      "loss: 2.5546114444732666\n",
      "epoch: 263\n",
      "loss: 2.5480785369873047\n",
      "epoch: 264\n",
      "loss: 2.541461944580078\n",
      "epoch: 265\n",
      "loss: 2.5347952842712402\n",
      "epoch: 266\n",
      "loss: 2.528163433074951\n",
      "epoch: 267\n",
      "loss: 2.5217156410217285\n",
      "epoch: 268\n",
      "loss: 2.515397548675537\n",
      "epoch: 269\n",
      "loss: 2.508890151977539\n",
      "epoch: 270\n",
      "loss: 2.5023984909057617\n",
      "epoch: 271\n",
      "loss: 2.496077299118042\n",
      "epoch: 272\n",
      "loss: 2.4896864891052246\n",
      "epoch: 273\n",
      "loss: 2.483276128768921\n",
      "epoch: 274\n",
      "loss: 2.476959466934204\n",
      "epoch: 275\n",
      "loss: 2.4707107543945312\n",
      "epoch: 276\n",
      "loss: 2.4644062519073486\n",
      "epoch: 277\n",
      "loss: 2.458026885986328\n",
      "epoch: 278\n",
      "loss: 2.4517483711242676\n",
      "epoch: 279\n",
      "loss: 2.445518970489502\n",
      "epoch: 280\n",
      "loss: 2.43922758102417\n",
      "epoch: 281\n",
      "loss: 2.4328737258911133\n",
      "epoch: 282\n",
      "loss: 2.426638603210449\n",
      "epoch: 283\n",
      "loss: 2.4203476905822754\n",
      "epoch: 284\n",
      "loss: 2.4142003059387207\n",
      "epoch: 285\n",
      "loss: 2.4079604148864746\n",
      "epoch: 286\n",
      "loss: 2.4017462730407715\n",
      "epoch: 287\n",
      "loss: 2.3955211639404297\n",
      "epoch: 288\n",
      "loss: 2.3892858028411865\n",
      "epoch: 289\n",
      "loss: 2.383136749267578\n",
      "epoch: 290\n",
      "loss: 2.3769893646240234\n",
      "epoch: 291\n",
      "loss: 2.370790958404541\n",
      "epoch: 292\n",
      "loss: 2.364656448364258\n",
      "epoch: 293\n",
      "loss: 2.358604669570923\n",
      "epoch: 294\n",
      "loss: 2.352447509765625\n",
      "epoch: 295\n",
      "loss: 2.346367359161377\n",
      "epoch: 296\n",
      "loss: 2.3403658866882324\n",
      "epoch: 297\n",
      "loss: 2.3344454765319824\n",
      "epoch: 298\n",
      "loss: 2.3283157348632812\n",
      "epoch: 299\n",
      "loss: 2.3223934173583984\n",
      "epoch: 300\n",
      "loss: 2.3165283203125\n",
      "epoch: 301\n",
      "loss: 2.310682535171509\n",
      "epoch: 302\n",
      "loss: 2.3047094345092773\n",
      "epoch: 303\n",
      "loss: 2.2987136840820312\n",
      "epoch: 304\n",
      "loss: 2.292628288269043\n",
      "epoch: 305\n",
      "loss: 2.2865171432495117\n",
      "epoch: 306\n",
      "loss: 2.2804760932922363\n",
      "epoch: 307\n",
      "loss: 2.274468183517456\n",
      "epoch: 308\n",
      "loss: 2.2684218883514404\n",
      "epoch: 309\n",
      "loss: 2.2623648643493652\n",
      "epoch: 310\n",
      "loss: 2.2563376426696777\n",
      "epoch: 311\n",
      "loss: 2.2502870559692383\n",
      "epoch: 312\n",
      "loss: 2.244302749633789\n",
      "epoch: 313\n",
      "loss: 2.2383289337158203\n",
      "epoch: 314\n",
      "loss: 2.2323708534240723\n",
      "epoch: 315\n",
      "loss: 2.226215362548828\n",
      "epoch: 316\n",
      "loss: 2.2199549674987793\n",
      "epoch: 317\n",
      "loss: 2.2136456966400146\n",
      "epoch: 318\n",
      "loss: 2.2073984146118164\n",
      "epoch: 319\n",
      "loss: 2.2012462615966797\n",
      "epoch: 320\n",
      "loss: 2.195129871368408\n",
      "epoch: 321\n",
      "loss: 2.188971996307373\n",
      "epoch: 322\n",
      "loss: 2.1828651428222656\n",
      "epoch: 323\n",
      "loss: 2.176884174346924\n",
      "epoch: 324\n",
      "loss: 2.1708269119262695\n",
      "epoch: 325\n",
      "loss: 2.1648683547973633\n",
      "epoch: 326\n",
      "loss: 2.158963203430176\n",
      "epoch: 327\n",
      "loss: 2.1530699729919434\n",
      "epoch: 328\n",
      "loss: 2.1472039222717285\n",
      "epoch: 329\n",
      "loss: 2.141352415084839\n",
      "epoch: 330\n",
      "loss: 2.135606288909912\n",
      "epoch: 331\n",
      "loss: 2.1298630237579346\n",
      "epoch: 332\n",
      "loss: 2.1241776943206787\n",
      "epoch: 333\n",
      "loss: 2.1187644004821777\n",
      "epoch: 334\n",
      "loss: 2.113149642944336\n",
      "epoch: 335\n",
      "loss: 2.107612133026123\n",
      "epoch: 336\n",
      "loss: 2.102224826812744\n",
      "epoch: 337\n",
      "loss: 2.096778392791748\n",
      "epoch: 338\n",
      "loss: 2.0913896560668945\n",
      "epoch: 339\n",
      "loss: 2.0860252380371094\n",
      "epoch: 340\n",
      "loss: 2.0807230472564697\n",
      "epoch: 341\n",
      "loss: 2.0754950046539307\n",
      "epoch: 342\n",
      "loss: 2.070272445678711\n",
      "epoch: 343\n",
      "loss: 2.0651471614837646\n",
      "epoch: 344\n",
      "loss: 2.059925079345703\n",
      "epoch: 345\n",
      "loss: 2.054807662963867\n",
      "epoch: 346\n",
      "loss: 2.049715280532837\n",
      "epoch: 347\n",
      "loss: 2.0447211265563965\n",
      "epoch: 348\n",
      "loss: 2.0396957397460938\n",
      "epoch: 349\n",
      "loss: 2.0346903800964355\n",
      "epoch: 350\n",
      "loss: 2.0298283100128174\n",
      "epoch: 351\n",
      "loss: 2.025017738342285\n",
      "epoch: 352\n",
      "loss: 2.0202536582946777\n",
      "epoch: 353\n",
      "loss: 2.01547908782959\n",
      "epoch: 354\n",
      "loss: 2.0108015537261963\n",
      "epoch: 355\n",
      "loss: 2.0061042308807373\n",
      "epoch: 356\n",
      "loss: 2.001467704772949\n",
      "epoch: 357\n",
      "loss: 1.9967576265335083\n",
      "epoch: 358\n",
      "loss: 1.9921280145645142\n",
      "epoch: 359\n",
      "loss: 1.987501621246338\n",
      "epoch: 360\n",
      "loss: 1.982914924621582\n",
      "epoch: 361\n",
      "loss: 1.9783934354782104\n",
      "epoch: 362\n",
      "loss: 1.9739598035812378\n",
      "epoch: 363\n",
      "loss: 1.9694219827651978\n",
      "epoch: 364\n",
      "loss: 1.9649814367294312\n",
      "epoch: 365\n",
      "loss: 1.960557460784912\n",
      "epoch: 366\n",
      "loss: 1.9562263488769531\n",
      "epoch: 367\n",
      "loss: 1.9517669677734375\n",
      "epoch: 368\n",
      "loss: 1.9473892450332642\n",
      "epoch: 369\n",
      "loss: 1.943099021911621\n",
      "epoch: 370\n",
      "loss: 1.9386954307556152\n",
      "epoch: 371\n",
      "loss: 1.934486746788025\n",
      "epoch: 372\n",
      "loss: 1.9301459789276123\n",
      "epoch: 373\n",
      "loss: 1.925908088684082\n",
      "epoch: 374\n",
      "loss: 1.9216779470443726\n",
      "epoch: 375\n",
      "loss: 1.917370080947876\n",
      "epoch: 376\n",
      "loss: 1.9131534099578857\n",
      "epoch: 377\n",
      "loss: 1.9089967012405396\n",
      "epoch: 378\n",
      "loss: 1.9047828912734985\n",
      "epoch: 379\n",
      "loss: 1.9006255865097046\n",
      "epoch: 380\n",
      "loss: 1.8965380191802979\n",
      "epoch: 381\n",
      "loss: 1.892377495765686\n",
      "epoch: 382\n",
      "loss: 1.888290286064148\n",
      "epoch: 383\n",
      "loss: 1.884205937385559\n",
      "epoch: 384\n",
      "loss: 1.880152940750122\n",
      "epoch: 385\n",
      "loss: 1.876111388206482\n",
      "epoch: 386\n",
      "loss: 1.8721586465835571\n",
      "epoch: 387\n",
      "loss: 1.8681398630142212\n",
      "epoch: 388\n",
      "loss: 1.8642511367797852\n",
      "epoch: 389\n",
      "loss: 1.8602770566940308\n",
      "epoch: 390\n",
      "loss: 1.8563642501831055\n",
      "epoch: 391\n",
      "loss: 1.8524292707443237\n",
      "epoch: 392\n",
      "loss: 1.8486101627349854\n",
      "epoch: 393\n",
      "loss: 1.8447473049163818\n",
      "epoch: 394\n",
      "loss: 1.8410402536392212\n",
      "epoch: 395\n",
      "loss: 1.8373125791549683\n",
      "epoch: 396\n",
      "loss: 1.8334444761276245\n",
      "epoch: 397\n",
      "loss: 1.8297064304351807\n",
      "epoch: 398\n",
      "loss: 1.8259738683700562\n",
      "epoch: 399\n",
      "loss: 1.822263240814209\n",
      "epoch: 400\n",
      "loss: 1.818565845489502\n",
      "epoch: 401\n",
      "loss: 1.8148709535598755\n",
      "epoch: 402\n",
      "loss: 1.811156988143921\n",
      "epoch: 403\n",
      "loss: 1.8075469732284546\n",
      "epoch: 404\n",
      "loss: 1.8038846254348755\n",
      "epoch: 405\n",
      "loss: 1.8002631664276123\n",
      "epoch: 406\n",
      "loss: 1.7966605424880981\n",
      "epoch: 407\n",
      "loss: 1.7930799722671509\n",
      "epoch: 408\n",
      "loss: 1.78947913646698\n",
      "epoch: 409\n",
      "loss: 1.7858787775039673\n",
      "epoch: 410\n",
      "loss: 1.782348394393921\n",
      "epoch: 411\n",
      "loss: 1.7787423133850098\n",
      "epoch: 412\n",
      "loss: 1.7753543853759766\n",
      "epoch: 413\n",
      "loss: 1.7716972827911377\n",
      "epoch: 414\n",
      "loss: 1.7682383060455322\n",
      "epoch: 415\n",
      "loss: 1.7647441625595093\n",
      "epoch: 416\n",
      "loss: 1.7612720727920532\n",
      "epoch: 417\n",
      "loss: 1.7578436136245728\n",
      "epoch: 418\n",
      "loss: 1.7544001340866089\n",
      "epoch: 419\n",
      "loss: 1.7509855031967163\n",
      "epoch: 420\n",
      "loss: 1.7476181983947754\n",
      "epoch: 421\n",
      "loss: 1.744202733039856\n",
      "epoch: 422\n",
      "loss: 1.7408220767974854\n",
      "epoch: 423\n",
      "loss: 1.7375497817993164\n",
      "epoch: 424\n",
      "loss: 1.7341076135635376\n",
      "epoch: 425\n",
      "loss: 1.730759620666504\n",
      "epoch: 426\n",
      "loss: 1.7274144887924194\n",
      "epoch: 427\n",
      "loss: 1.724078893661499\n",
      "epoch: 428\n",
      "loss: 1.7207728624343872\n",
      "epoch: 429\n",
      "loss: 1.7176798582077026\n",
      "epoch: 430\n",
      "loss: 1.7142274379730225\n",
      "epoch: 431\n",
      "loss: 1.711021900177002\n",
      "epoch: 432\n",
      "loss: 1.7078124284744263\n",
      "epoch: 433\n",
      "loss: 1.7045992612838745\n",
      "epoch: 434\n",
      "loss: 1.7013944387435913\n",
      "epoch: 435\n",
      "loss: 1.698378562927246\n",
      "epoch: 436\n",
      "loss: 1.6951802968978882\n",
      "epoch: 437\n",
      "loss: 1.6920158863067627\n",
      "epoch: 438\n",
      "loss: 1.6889305114746094\n",
      "epoch: 439\n",
      "loss: 1.685818076133728\n",
      "epoch: 440\n",
      "loss: 1.6827276945114136\n",
      "epoch: 441\n",
      "loss: 1.679634928703308\n",
      "epoch: 442\n",
      "loss: 1.6765577793121338\n",
      "epoch: 443\n",
      "loss: 1.673597812652588\n",
      "epoch: 444\n",
      "loss: 1.6704363822937012\n",
      "epoch: 445\n",
      "loss: 1.6674120426177979\n",
      "epoch: 446\n",
      "loss: 1.6643880605697632\n",
      "epoch: 447\n",
      "loss: 1.661474585533142\n",
      "epoch: 448\n",
      "loss: 1.6584217548370361\n",
      "epoch: 449\n",
      "loss: 1.6554489135742188\n",
      "epoch: 450\n",
      "loss: 1.6524680852890015\n",
      "epoch: 451\n",
      "loss: 1.6494829654693604\n",
      "epoch: 452\n",
      "loss: 1.646594762802124\n",
      "epoch: 453\n",
      "loss: 1.6435595750808716\n",
      "epoch: 454\n",
      "loss: 1.6406230926513672\n",
      "epoch: 455\n",
      "loss: 1.6376993656158447\n",
      "epoch: 456\n",
      "loss: 1.6347739696502686\n",
      "epoch: 457\n",
      "loss: 1.6318588256835938\n",
      "epoch: 458\n",
      "loss: 1.6290544271469116\n",
      "epoch: 459\n",
      "loss: 1.626177191734314\n",
      "epoch: 460\n",
      "loss: 1.6233474016189575\n",
      "epoch: 461\n",
      "loss: 1.6205806732177734\n",
      "epoch: 462\n",
      "loss: 1.617817997932434\n",
      "epoch: 463\n",
      "loss: 1.6151396036148071\n",
      "epoch: 464\n",
      "loss: 1.6123859882354736\n",
      "epoch: 465\n",
      "loss: 1.6095778942108154\n",
      "epoch: 466\n",
      "loss: 1.6068189144134521\n",
      "epoch: 467\n",
      "loss: 1.6040799617767334\n",
      "epoch: 468\n",
      "loss: 1.6013954877853394\n",
      "epoch: 469\n",
      "loss: 1.5986883640289307\n",
      "epoch: 470\n",
      "loss: 1.5959877967834473\n",
      "epoch: 471\n",
      "loss: 1.5932893753051758\n",
      "epoch: 472\n",
      "loss: 1.5906513929367065\n",
      "epoch: 473\n",
      "loss: 1.5879437923431396\n",
      "epoch: 474\n",
      "loss: 1.585312008857727\n",
      "epoch: 475\n",
      "loss: 1.5826444625854492\n",
      "epoch: 476\n",
      "loss: 1.5800811052322388\n",
      "epoch: 477\n",
      "loss: 1.5774198770523071\n",
      "epoch: 478\n",
      "loss: 1.574828863143921\n",
      "epoch: 479\n",
      "loss: 1.5722373723983765\n",
      "epoch: 480\n",
      "loss: 1.5696380138397217\n",
      "epoch: 481\n",
      "loss: 1.5670337677001953\n",
      "epoch: 482\n",
      "loss: 1.5644383430480957\n",
      "epoch: 483\n",
      "loss: 1.561916708946228\n",
      "epoch: 484\n",
      "loss: 1.5593068599700928\n",
      "epoch: 485\n",
      "loss: 1.5567572116851807\n",
      "epoch: 486\n",
      "loss: 1.5542067289352417\n",
      "epoch: 487\n",
      "loss: 1.5516581535339355\n",
      "epoch: 488\n",
      "loss: 1.5492336750030518\n",
      "epoch: 489\n",
      "loss: 1.5466138124465942\n",
      "epoch: 490\n",
      "loss: 1.544147253036499\n",
      "epoch: 491\n",
      "loss: 1.5416535139083862\n",
      "epoch: 492\n",
      "loss: 1.5392423868179321\n",
      "epoch: 493\n",
      "loss: 1.5367438793182373\n",
      "epoch: 494\n",
      "loss: 1.5343855619430542\n",
      "epoch: 495\n",
      "loss: 1.5318492650985718\n",
      "epoch: 496\n",
      "loss: 1.529429316520691\n",
      "epoch: 497\n",
      "loss: 1.5269925594329834\n",
      "epoch: 498\n",
      "loss: 1.5245577096939087\n",
      "epoch: 499\n",
      "loss: 1.5221800804138184\n",
      "epoch: 500\n",
      "loss: 1.519778847694397\n",
      "epoch: 501\n",
      "loss: 1.517362117767334\n",
      "epoch: 502\n",
      "loss: 1.5150001049041748\n",
      "epoch: 503\n",
      "loss: 1.5126538276672363\n",
      "epoch: 504\n",
      "loss: 1.5102930068969727\n",
      "epoch: 505\n",
      "loss: 1.5079578161239624\n",
      "epoch: 506\n",
      "loss: 1.5056178569793701\n",
      "epoch: 507\n",
      "loss: 1.503321886062622\n",
      "epoch: 508\n",
      "loss: 1.5009647607803345\n",
      "epoch: 509\n",
      "loss: 1.4986549615859985\n",
      "epoch: 510\n",
      "loss: 1.4963470697402954\n",
      "epoch: 511\n",
      "loss: 1.4940557479858398\n",
      "epoch: 512\n",
      "loss: 1.4917593002319336\n",
      "epoch: 513\n",
      "loss: 1.489460825920105\n",
      "epoch: 514\n",
      "loss: 1.4872416257858276\n",
      "epoch: 515\n",
      "loss: 1.4849236011505127\n",
      "epoch: 516\n",
      "loss: 1.4826934337615967\n",
      "epoch: 517\n",
      "loss: 1.480594515800476\n",
      "epoch: 518\n",
      "loss: 1.4782582521438599\n",
      "epoch: 519\n",
      "loss: 1.4760890007019043\n",
      "epoch: 520\n",
      "loss: 1.4738495349884033\n",
      "epoch: 521\n",
      "loss: 1.4716596603393555\n",
      "epoch: 522\n",
      "loss: 1.4694736003875732\n",
      "epoch: 523\n",
      "loss: 1.4673417806625366\n",
      "epoch: 524\n",
      "loss: 1.4651172161102295\n",
      "epoch: 525\n",
      "loss: 1.4629478454589844\n",
      "epoch: 526\n",
      "loss: 1.4607819318771362\n",
      "epoch: 527\n",
      "loss: 1.4586875438690186\n",
      "epoch: 528\n",
      "loss: 1.456486463546753\n",
      "epoch: 529\n",
      "loss: 1.454375982284546\n",
      "epoch: 530\n",
      "loss: 1.4522526264190674\n",
      "epoch: 531\n",
      "loss: 1.450237512588501\n",
      "epoch: 532\n",
      "loss: 1.448075294494629\n",
      "epoch: 533\n",
      "loss: 1.4460411071777344\n",
      "epoch: 534\n",
      "loss: 1.4439404010772705\n",
      "epoch: 535\n",
      "loss: 1.4418866634368896\n",
      "epoch: 536\n",
      "loss: 1.4398715496063232\n",
      "epoch: 537\n",
      "loss: 1.437788486480713\n",
      "epoch: 538\n",
      "loss: 1.435746431350708\n",
      "epoch: 539\n",
      "loss: 1.4337100982666016\n",
      "epoch: 540\n",
      "loss: 1.431670904159546\n",
      "epoch: 541\n",
      "loss: 1.4296386241912842\n",
      "epoch: 542\n",
      "loss: 1.427605390548706\n",
      "epoch: 543\n",
      "loss: 1.4255962371826172\n",
      "epoch: 544\n",
      "loss: 1.423555850982666\n",
      "epoch: 545\n",
      "loss: 1.421668529510498\n",
      "epoch: 546\n",
      "loss: 1.4196370840072632\n",
      "epoch: 547\n",
      "loss: 1.4177117347717285\n",
      "epoch: 548\n",
      "loss: 1.4157631397247314\n",
      "epoch: 549\n",
      "loss: 1.4138245582580566\n",
      "epoch: 550\n",
      "loss: 1.4118765592575073\n",
      "epoch: 551\n",
      "loss: 1.4099268913269043\n",
      "epoch: 552\n",
      "loss: 1.4080110788345337\n",
      "epoch: 553\n",
      "loss: 1.406067132949829\n",
      "epoch: 554\n",
      "loss: 1.4041409492492676\n",
      "epoch: 555\n",
      "loss: 1.4022111892700195\n",
      "epoch: 556\n",
      "loss: 1.400285005569458\n",
      "epoch: 557\n",
      "loss: 1.3984050750732422\n",
      "epoch: 558\n",
      "loss: 1.39646577835083\n",
      "epoch: 559\n",
      "loss: 1.3945679664611816\n",
      "epoch: 560\n",
      "loss: 1.392681360244751\n",
      "epoch: 561\n",
      "loss: 1.390825867652893\n",
      "epoch: 562\n",
      "loss: 1.388946533203125\n",
      "epoch: 563\n",
      "loss: 1.3871253728866577\n",
      "epoch: 564\n",
      "loss: 1.3852636814117432\n",
      "epoch: 565\n",
      "loss: 1.3834446668624878\n",
      "epoch: 566\n",
      "loss: 1.3816299438476562\n",
      "epoch: 567\n",
      "loss: 1.3798253536224365\n",
      "epoch: 568\n",
      "loss: 1.3780102729797363\n",
      "epoch: 569\n",
      "loss: 1.3761827945709229\n",
      "epoch: 570\n",
      "loss: 1.3743788003921509\n",
      "epoch: 571\n",
      "loss: 1.3725725412368774\n",
      "epoch: 572\n",
      "loss: 1.3707587718963623\n",
      "epoch: 573\n",
      "loss: 1.36893892288208\n",
      "epoch: 574\n",
      "loss: 1.3672153949737549\n",
      "epoch: 575\n",
      "loss: 1.3653860092163086\n",
      "epoch: 576\n",
      "loss: 1.3636116981506348\n",
      "epoch: 577\n",
      "loss: 1.3618977069854736\n",
      "epoch: 578\n",
      "loss: 1.3601267337799072\n",
      "epoch: 579\n",
      "loss: 1.3584126234054565\n",
      "epoch: 580\n",
      "loss: 1.3566784858703613\n",
      "epoch: 581\n",
      "loss: 1.3549500703811646\n",
      "epoch: 582\n",
      "loss: 1.3532285690307617\n",
      "epoch: 583\n",
      "loss: 1.3514913320541382\n",
      "epoch: 584\n",
      "loss: 1.3497600555419922\n",
      "epoch: 585\n",
      "loss: 1.3480371236801147\n",
      "epoch: 586\n",
      "loss: 1.3463144302368164\n",
      "epoch: 587\n",
      "loss: 1.344598412513733\n",
      "epoch: 588\n",
      "loss: 1.342879056930542\n",
      "epoch: 589\n",
      "loss: 1.341162919998169\n",
      "epoch: 590\n",
      "loss: 1.3394492864608765\n",
      "epoch: 591\n",
      "loss: 1.3377419710159302\n",
      "epoch: 592\n",
      "loss: 1.3360695838928223\n",
      "epoch: 593\n",
      "loss: 1.3343346118927002\n",
      "epoch: 594\n",
      "loss: 1.3326406478881836\n",
      "epoch: 595\n",
      "loss: 1.3310047388076782\n",
      "epoch: 596\n",
      "loss: 1.3293051719665527\n",
      "epoch: 597\n",
      "loss: 1.3276993036270142\n",
      "epoch: 598\n",
      "loss: 1.3260002136230469\n",
      "epoch: 599\n",
      "loss: 1.3243803977966309\n",
      "epoch: 600\n",
      "loss: 1.322756290435791\n",
      "epoch: 601\n",
      "loss: 1.3211305141448975\n",
      "epoch: 602\n",
      "loss: 1.3195009231567383\n",
      "epoch: 603\n",
      "loss: 1.317866563796997\n",
      "epoch: 604\n",
      "loss: 1.3162293434143066\n",
      "epoch: 605\n",
      "loss: 1.3146092891693115\n",
      "epoch: 606\n",
      "loss: 1.3129799365997314\n",
      "epoch: 607\n",
      "loss: 1.311342477798462\n",
      "epoch: 608\n",
      "loss: 1.309718132019043\n",
      "epoch: 609\n",
      "loss: 1.3081104755401611\n",
      "epoch: 610\n",
      "loss: 1.3065119981765747\n",
      "epoch: 611\n",
      "loss: 1.3049038648605347\n",
      "epoch: 612\n",
      "loss: 1.303291916847229\n",
      "epoch: 613\n",
      "loss: 1.3017349243164062\n",
      "epoch: 614\n",
      "loss: 1.3001117706298828\n",
      "epoch: 615\n",
      "loss: 1.2985954284667969\n",
      "epoch: 616\n",
      "loss: 1.2969820499420166\n",
      "epoch: 617\n",
      "loss: 1.2954027652740479\n",
      "epoch: 618\n",
      "loss: 1.2938393354415894\n",
      "epoch: 619\n",
      "loss: 1.2922687530517578\n",
      "epoch: 620\n",
      "loss: 1.2907025814056396\n",
      "epoch: 621\n",
      "loss: 1.2891950607299805\n",
      "epoch: 622\n",
      "loss: 1.2876088619232178\n",
      "epoch: 623\n",
      "loss: 1.2860615253448486\n",
      "epoch: 624\n",
      "loss: 1.2845447063446045\n",
      "epoch: 625\n",
      "loss: 1.2830193042755127\n",
      "epoch: 626\n",
      "loss: 1.2814829349517822\n",
      "epoch: 627\n",
      "loss: 1.2799338102340698\n",
      "epoch: 628\n",
      "loss: 1.278397560119629\n",
      "epoch: 629\n",
      "loss: 1.2768408060073853\n",
      "epoch: 630\n",
      "loss: 1.2753454446792603\n",
      "epoch: 631\n",
      "loss: 1.2739243507385254\n",
      "epoch: 632\n",
      "loss: 1.2723052501678467\n",
      "epoch: 633\n",
      "loss: 1.2708218097686768\n",
      "epoch: 634\n",
      "loss: 1.2693384885787964\n",
      "epoch: 635\n",
      "loss: 1.2678390741348267\n",
      "epoch: 636\n",
      "loss: 1.266352891921997\n",
      "epoch: 637\n",
      "loss: 1.2648603916168213\n",
      "epoch: 638\n",
      "loss: 1.2633781433105469\n",
      "epoch: 639\n",
      "loss: 1.2618916034698486\n",
      "epoch: 640\n",
      "loss: 1.2604014873504639\n",
      "epoch: 641\n",
      "loss: 1.2589361667633057\n",
      "epoch: 642\n",
      "loss: 1.2574390172958374\n",
      "epoch: 643\n",
      "loss: 1.2559566497802734\n",
      "epoch: 644\n",
      "loss: 1.2544901371002197\n",
      "epoch: 645\n",
      "loss: 1.253066062927246\n",
      "epoch: 646\n",
      "loss: 1.2515535354614258\n",
      "epoch: 647\n",
      "loss: 1.2500890493392944\n",
      "epoch: 648\n",
      "loss: 1.2486238479614258\n",
      "epoch: 649\n",
      "loss: 1.2471706867218018\n",
      "epoch: 650\n",
      "loss: 1.2457058429718018\n",
      "epoch: 651\n",
      "loss: 1.2442491054534912\n",
      "epoch: 652\n",
      "loss: 1.2428005933761597\n",
      "epoch: 653\n",
      "loss: 1.2413663864135742\n",
      "epoch: 654\n",
      "loss: 1.2399513721466064\n",
      "epoch: 655\n",
      "loss: 1.2384803295135498\n",
      "epoch: 656\n",
      "loss: 1.237051248550415\n",
      "epoch: 657\n",
      "loss: 1.2356693744659424\n",
      "epoch: 658\n",
      "loss: 1.2342369556427002\n",
      "epoch: 659\n",
      "loss: 1.2328053712844849\n",
      "epoch: 660\n",
      "loss: 1.2314081192016602\n",
      "epoch: 661\n",
      "loss: 1.230006456375122\n",
      "epoch: 662\n",
      "loss: 1.2286005020141602\n",
      "epoch: 663\n",
      "loss: 1.2271993160247803\n",
      "epoch: 664\n",
      "loss: 1.2257936000823975\n",
      "epoch: 665\n",
      "loss: 1.2244024276733398\n",
      "epoch: 666\n",
      "loss: 1.2229971885681152\n",
      "epoch: 667\n",
      "loss: 1.221604585647583\n",
      "epoch: 668\n",
      "loss: 1.2202104330062866\n",
      "epoch: 669\n",
      "loss: 1.2188160419464111\n",
      "epoch: 670\n",
      "loss: 1.2174240350723267\n",
      "epoch: 671\n",
      "loss: 1.2160353660583496\n",
      "epoch: 672\n",
      "loss: 1.2146879434585571\n",
      "epoch: 673\n",
      "loss: 1.213310956954956\n",
      "epoch: 674\n",
      "loss: 1.2119085788726807\n",
      "epoch: 675\n",
      "loss: 1.210540771484375\n",
      "epoch: 676\n",
      "loss: 1.2092264890670776\n",
      "epoch: 677\n",
      "loss: 1.207801103591919\n",
      "epoch: 678\n",
      "loss: 1.2064564228057861\n",
      "epoch: 679\n",
      "loss: 1.2051000595092773\n",
      "epoch: 680\n",
      "loss: 1.2037580013275146\n",
      "epoch: 681\n",
      "loss: 1.2024481296539307\n",
      "epoch: 682\n",
      "loss: 1.2010917663574219\n",
      "epoch: 683\n",
      "loss: 1.1997491121292114\n",
      "epoch: 684\n",
      "loss: 1.1984105110168457\n",
      "epoch: 685\n",
      "loss: 1.1970751285552979\n",
      "epoch: 686\n",
      "loss: 1.1957426071166992\n",
      "epoch: 687\n",
      "loss: 1.1944084167480469\n",
      "epoch: 688\n",
      "loss: 1.193086862564087\n",
      "epoch: 689\n",
      "loss: 1.1917716264724731\n",
      "epoch: 690\n",
      "loss: 1.1904513835906982\n",
      "epoch: 691\n",
      "loss: 1.1891236305236816\n",
      "epoch: 692\n",
      "loss: 1.1877954006195068\n",
      "epoch: 693\n",
      "loss: 1.186469316482544\n",
      "epoch: 694\n",
      "loss: 1.185166358947754\n",
      "epoch: 695\n",
      "loss: 1.183844804763794\n",
      "epoch: 696\n",
      "loss: 1.1825286149978638\n",
      "epoch: 697\n",
      "loss: 1.1812286376953125\n",
      "epoch: 698\n",
      "loss: 1.179927945137024\n",
      "epoch: 699\n",
      "loss: 1.178626537322998\n",
      "epoch: 700\n",
      "loss: 1.1773607730865479\n",
      "epoch: 701\n",
      "loss: 1.1760995388031006\n",
      "epoch: 702\n",
      "loss: 1.17477285861969\n",
      "epoch: 703\n",
      "loss: 1.1735634803771973\n",
      "epoch: 704\n",
      "loss: 1.172255039215088\n",
      "epoch: 705\n",
      "loss: 1.1709831953048706\n",
      "epoch: 706\n",
      "loss: 1.1697150468826294\n",
      "epoch: 707\n",
      "loss: 1.1684482097625732\n",
      "epoch: 708\n",
      "loss: 1.1671772003173828\n",
      "epoch: 709\n",
      "loss: 1.1659066677093506\n",
      "epoch: 710\n",
      "loss: 1.1646366119384766\n",
      "epoch: 711\n",
      "loss: 1.1633827686309814\n",
      "epoch: 712\n",
      "loss: 1.1621111631393433\n",
      "epoch: 713\n",
      "loss: 1.1608459949493408\n",
      "epoch: 714\n",
      "loss: 1.1595847606658936\n",
      "epoch: 715\n",
      "loss: 1.1583287715911865\n",
      "epoch: 716\n",
      "loss: 1.1570708751678467\n",
      "epoch: 717\n",
      "loss: 1.155835747718811\n",
      "epoch: 718\n",
      "loss: 1.1545770168304443\n",
      "epoch: 719\n",
      "loss: 1.1533349752426147\n",
      "epoch: 720\n",
      "loss: 1.1520891189575195\n",
      "epoch: 721\n",
      "loss: 1.1508369445800781\n",
      "epoch: 722\n",
      "loss: 1.1495826244354248\n",
      "epoch: 723\n",
      "loss: 1.1483755111694336\n",
      "epoch: 724\n",
      "loss: 1.1471089124679565\n",
      "epoch: 725\n",
      "loss: 1.145880937576294\n",
      "epoch: 726\n",
      "loss: 1.1446449756622314\n",
      "epoch: 727\n",
      "loss: 1.1434388160705566\n",
      "epoch: 728\n",
      "loss: 1.142221212387085\n",
      "epoch: 729\n",
      "loss: 1.140993595123291\n",
      "epoch: 730\n",
      "loss: 1.1397817134857178\n",
      "epoch: 731\n",
      "loss: 1.1385369300842285\n",
      "epoch: 732\n",
      "loss: 1.137366771697998\n",
      "epoch: 733\n",
      "loss: 1.1361273527145386\n",
      "epoch: 734\n",
      "loss: 1.1349220275878906\n",
      "epoch: 735\n",
      "loss: 1.1337305307388306\n",
      "epoch: 736\n",
      "loss: 1.1325316429138184\n",
      "epoch: 737\n",
      "loss: 1.1313343048095703\n",
      "epoch: 738\n",
      "loss: 1.1301345825195312\n",
      "epoch: 739\n",
      "loss: 1.1289472579956055\n",
      "epoch: 740\n",
      "loss: 1.1277413368225098\n",
      "epoch: 741\n",
      "loss: 1.1265604496002197\n",
      "epoch: 742\n",
      "loss: 1.1253700256347656\n",
      "epoch: 743\n",
      "loss: 1.1241796016693115\n",
      "epoch: 744\n",
      "loss: 1.1229935884475708\n",
      "epoch: 745\n",
      "loss: 1.1218167543411255\n",
      "epoch: 746\n",
      "loss: 1.120627999305725\n",
      "epoch: 747\n",
      "loss: 1.1194539070129395\n",
      "epoch: 748\n",
      "loss: 1.1182794570922852\n",
      "epoch: 749\n",
      "loss: 1.1171084642410278\n",
      "epoch: 750\n",
      "loss: 1.1159281730651855\n",
      "epoch: 751\n",
      "loss: 1.1148171424865723\n",
      "epoch: 752\n",
      "loss: 1.1136146783828735\n",
      "epoch: 753\n",
      "loss: 1.1125164031982422\n",
      "epoch: 754\n",
      "loss: 1.1113413572311401\n",
      "epoch: 755\n",
      "loss: 1.1102206707000732\n",
      "epoch: 756\n",
      "loss: 1.1090669631958008\n",
      "epoch: 757\n",
      "loss: 1.1079245805740356\n",
      "epoch: 758\n",
      "loss: 1.1067880392074585\n",
      "epoch: 759\n",
      "loss: 1.105642318725586\n",
      "epoch: 760\n",
      "loss: 1.1045241355895996\n",
      "epoch: 761\n",
      "loss: 1.1033624410629272\n",
      "epoch: 762\n",
      "loss: 1.1022320985794067\n",
      "epoch: 763\n",
      "loss: 1.1010987758636475\n",
      "epoch: 764\n",
      "loss: 1.0999616384506226\n",
      "epoch: 765\n",
      "loss: 1.0988268852233887\n",
      "epoch: 766\n",
      "loss: 1.097707748413086\n",
      "epoch: 767\n",
      "loss: 1.096573829650879\n",
      "epoch: 768\n",
      "loss: 1.0954830646514893\n",
      "epoch: 769\n",
      "loss: 1.0943336486816406\n",
      "epoch: 770\n",
      "loss: 1.0932340621948242\n",
      "epoch: 771\n",
      "loss: 1.0921317338943481\n",
      "epoch: 772\n",
      "loss: 1.0910265445709229\n",
      "epoch: 773\n",
      "loss: 1.0899176597595215\n",
      "epoch: 774\n",
      "loss: 1.0888290405273438\n",
      "epoch: 775\n",
      "loss: 1.0877310037612915\n",
      "epoch: 776\n",
      "loss: 1.0866360664367676\n",
      "epoch: 777\n",
      "loss: 1.0855356454849243\n",
      "epoch: 778\n",
      "loss: 1.084436058998108\n",
      "epoch: 779\n",
      "loss: 1.0833395719528198\n",
      "epoch: 780\n",
      "loss: 1.0822426080703735\n",
      "epoch: 781\n",
      "loss: 1.081148624420166\n",
      "epoch: 782\n",
      "loss: 1.080051302909851\n",
      "epoch: 783\n",
      "loss: 1.078967809677124\n",
      "epoch: 784\n",
      "loss: 1.0778858661651611\n",
      "epoch: 785\n",
      "loss: 1.0768181085586548\n",
      "epoch: 786\n",
      "loss: 1.075757622718811\n",
      "epoch: 787\n",
      "loss: 1.0747284889221191\n",
      "epoch: 788\n",
      "loss: 1.0736522674560547\n",
      "epoch: 789\n",
      "loss: 1.0725820064544678\n",
      "epoch: 790\n",
      "loss: 1.071530818939209\n",
      "epoch: 791\n",
      "loss: 1.070454716682434\n",
      "epoch: 792\n",
      "loss: 1.0694085359573364\n",
      "epoch: 793\n",
      "loss: 1.0683746337890625\n",
      "epoch: 794\n",
      "loss: 1.0673052072525024\n",
      "epoch: 795\n",
      "loss: 1.0662678480148315\n",
      "epoch: 796\n",
      "loss: 1.0652306079864502\n",
      "epoch: 797\n",
      "loss: 1.0641825199127197\n",
      "epoch: 798\n",
      "loss: 1.0631288290023804\n",
      "epoch: 799\n",
      "loss: 1.0620821714401245\n",
      "epoch: 800\n",
      "loss: 1.0610382556915283\n",
      "epoch: 801\n",
      "loss: 1.059991717338562\n",
      "epoch: 802\n",
      "loss: 1.0589690208435059\n",
      "epoch: 803\n",
      "loss: 1.057908535003662\n",
      "epoch: 804\n",
      "loss: 1.0568760633468628\n",
      "epoch: 805\n",
      "loss: 1.0558409690856934\n",
      "epoch: 806\n",
      "loss: 1.0548075437545776\n",
      "epoch: 807\n",
      "loss: 1.0537738800048828\n",
      "epoch: 808\n",
      "loss: 1.052737832069397\n",
      "epoch: 809\n",
      "loss: 1.051700472831726\n",
      "epoch: 810\n",
      "loss: 1.0507044792175293\n",
      "epoch: 811\n",
      "loss: 1.049649715423584\n",
      "epoch: 812\n",
      "loss: 1.048656940460205\n",
      "epoch: 813\n",
      "loss: 1.0476175546646118\n",
      "epoch: 814\n",
      "loss: 1.0466110706329346\n",
      "epoch: 815\n",
      "loss: 1.0456037521362305\n",
      "epoch: 816\n",
      "loss: 1.0445891618728638\n",
      "epoch: 817\n",
      "loss: 1.0435733795166016\n",
      "epoch: 818\n",
      "loss: 1.0425626039505005\n",
      "epoch: 819\n",
      "loss: 1.04158616065979\n",
      "epoch: 820\n",
      "loss: 1.0405961275100708\n",
      "epoch: 821\n",
      "loss: 1.039597511291504\n",
      "epoch: 822\n",
      "loss: 1.0386016368865967\n",
      "epoch: 823\n",
      "loss: 1.0376133918762207\n",
      "epoch: 824\n",
      "loss: 1.0366361141204834\n",
      "epoch: 825\n",
      "loss: 1.0356525182724\n",
      "epoch: 826\n",
      "loss: 1.0346546173095703\n",
      "epoch: 827\n",
      "loss: 1.0336655378341675\n",
      "epoch: 828\n",
      "loss: 1.0326918363571167\n",
      "epoch: 829\n",
      "loss: 1.031693935394287\n",
      "epoch: 830\n",
      "loss: 1.0307159423828125\n",
      "epoch: 831\n",
      "loss: 1.029740810394287\n",
      "epoch: 832\n",
      "loss: 1.0287657976150513\n",
      "epoch: 833\n",
      "loss: 1.0277886390686035\n",
      "epoch: 834\n",
      "loss: 1.0268149375915527\n",
      "epoch: 835\n",
      "loss: 1.0258398056030273\n",
      "epoch: 836\n",
      "loss: 1.024869441986084\n",
      "epoch: 837\n",
      "loss: 1.0239156484603882\n",
      "epoch: 838\n",
      "loss: 1.0229370594024658\n",
      "epoch: 839\n",
      "loss: 1.021975040435791\n",
      "epoch: 840\n",
      "loss: 1.021011233329773\n",
      "epoch: 841\n",
      "loss: 1.0200462341308594\n",
      "epoch: 842\n",
      "loss: 1.0190823078155518\n",
      "epoch: 843\n",
      "loss: 1.0181256532669067\n",
      "epoch: 844\n",
      "loss: 1.0171648263931274\n",
      "epoch: 845\n",
      "loss: 1.0162168741226196\n",
      "epoch: 846\n",
      "loss: 1.0152866840362549\n",
      "epoch: 847\n",
      "loss: 1.0143284797668457\n",
      "epoch: 848\n",
      "loss: 1.0133787393569946\n",
      "epoch: 849\n",
      "loss: 1.0124480724334717\n",
      "epoch: 850\n",
      "loss: 1.0115771293640137\n",
      "epoch: 851\n",
      "loss: 1.0105911493301392\n",
      "epoch: 852\n",
      "loss: 1.0096964836120605\n",
      "epoch: 853\n",
      "loss: 1.008751630783081\n",
      "epoch: 854\n",
      "loss: 1.0078275203704834\n",
      "epoch: 855\n",
      "loss: 1.0069093704223633\n",
      "epoch: 856\n",
      "loss: 1.0059863328933716\n",
      "epoch: 857\n",
      "loss: 1.0050549507141113\n",
      "epoch: 858\n",
      "loss: 1.0041308403015137\n",
      "epoch: 859\n",
      "loss: 1.0032012462615967\n",
      "epoch: 860\n",
      "loss: 1.0022729635238647\n",
      "epoch: 861\n",
      "loss: 1.001347541809082\n",
      "epoch: 862\n",
      "loss: 1.0004240274429321\n",
      "epoch: 863\n",
      "loss: 0.9995036125183105\n",
      "epoch: 864\n",
      "loss: 0.9985835552215576\n",
      "epoch: 865\n",
      "loss: 0.9976831078529358\n",
      "epoch: 866\n",
      "loss: 0.9967585802078247\n",
      "epoch: 867\n",
      "loss: 0.9958524107933044\n",
      "epoch: 868\n",
      "loss: 0.9949342012405396\n",
      "epoch: 869\n",
      "loss: 0.9940299987792969\n",
      "epoch: 870\n",
      "loss: 0.99313884973526\n",
      "epoch: 871\n",
      "loss: 0.992218554019928\n",
      "epoch: 872\n",
      "loss: 0.9913199543952942\n",
      "epoch: 873\n",
      "loss: 0.9904193878173828\n",
      "epoch: 874\n",
      "loss: 0.9895340800285339\n",
      "epoch: 875\n",
      "loss: 0.9886300563812256\n",
      "epoch: 876\n",
      "loss: 0.987738847732544\n",
      "epoch: 877\n",
      "loss: 0.9868435859680176\n",
      "epoch: 878\n",
      "loss: 0.9859520196914673\n",
      "epoch: 879\n",
      "loss: 0.9850585460662842\n",
      "epoch: 880\n",
      "loss: 0.9841651916503906\n",
      "epoch: 881\n",
      "loss: 0.9832758903503418\n",
      "epoch: 882\n",
      "loss: 0.9823852181434631\n",
      "epoch: 883\n",
      "loss: 0.9814956188201904\n",
      "epoch: 884\n",
      "loss: 0.980617105960846\n",
      "epoch: 885\n",
      "loss: 0.9797599911689758\n",
      "epoch: 886\n",
      "loss: 0.9788838624954224\n",
      "epoch: 887\n",
      "loss: 0.9779981970787048\n",
      "epoch: 888\n",
      "loss: 0.9771195650100708\n",
      "epoch: 889\n",
      "loss: 0.976270318031311\n",
      "epoch: 890\n",
      "loss: 0.9754065275192261\n",
      "epoch: 891\n",
      "loss: 0.9745534658432007\n",
      "epoch: 892\n",
      "loss: 0.9736953377723694\n",
      "epoch: 893\n",
      "loss: 0.9728351831436157\n",
      "epoch: 894\n",
      "loss: 0.9719778299331665\n",
      "epoch: 895\n",
      "loss: 0.9711093902587891\n",
      "epoch: 896\n",
      "loss: 0.9702516794204712\n",
      "epoch: 897\n",
      "loss: 0.9693957567214966\n",
      "epoch: 898\n",
      "loss: 0.9685412049293518\n",
      "epoch: 899\n",
      "loss: 0.9676903486251831\n",
      "epoch: 900\n",
      "loss: 0.9668291807174683\n",
      "epoch: 901\n",
      "loss: 0.9659877419471741\n",
      "epoch: 902\n",
      "loss: 0.9651246070861816\n",
      "epoch: 903\n",
      "loss: 0.9642775654792786\n",
      "epoch: 904\n",
      "loss: 0.9634300470352173\n",
      "epoch: 905\n",
      "loss: 0.962600827217102\n",
      "epoch: 906\n",
      "loss: 0.9617379307746887\n",
      "epoch: 907\n",
      "loss: 0.9608997702598572\n",
      "epoch: 908\n",
      "loss: 0.9600511193275452\n",
      "epoch: 909\n",
      "loss: 0.9592090249061584\n",
      "epoch: 910\n",
      "loss: 0.9583665728569031\n",
      "epoch: 911\n",
      "loss: 0.9575217962265015\n",
      "epoch: 912\n",
      "loss: 0.9566940069198608\n",
      "epoch: 913\n",
      "loss: 0.955839216709137\n",
      "epoch: 914\n",
      "loss: 0.9550056457519531\n",
      "epoch: 915\n",
      "loss: 0.9542000889778137\n",
      "epoch: 916\n",
      "loss: 0.9533376693725586\n",
      "epoch: 917\n",
      "loss: 0.9525089263916016\n",
      "epoch: 918\n",
      "loss: 0.951711893081665\n",
      "epoch: 919\n",
      "loss: 0.9509438872337341\n",
      "epoch: 920\n",
      "loss: 0.9500561952590942\n",
      "epoch: 921\n",
      "loss: 0.9492571949958801\n",
      "epoch: 922\n",
      "loss: 0.9484500288963318\n",
      "epoch: 923\n",
      "loss: 0.9476372003555298\n",
      "epoch: 924\n",
      "loss: 0.9468294382095337\n",
      "epoch: 925\n",
      "loss: 0.9460136294364929\n",
      "epoch: 926\n",
      "loss: 0.9452009797096252\n",
      "epoch: 927\n",
      "loss: 0.9443873763084412\n",
      "epoch: 928\n",
      "loss: 0.9435750246047974\n",
      "epoch: 929\n",
      "loss: 0.9427609443664551\n",
      "epoch: 930\n",
      "loss: 0.9419471025466919\n",
      "epoch: 931\n",
      "loss: 0.9411345720291138\n",
      "epoch: 932\n",
      "loss: 0.940331220626831\n",
      "epoch: 933\n",
      "loss: 0.9395152926445007\n",
      "epoch: 934\n",
      "loss: 0.9387245178222656\n",
      "epoch: 935\n",
      "loss: 0.9379111528396606\n",
      "epoch: 936\n",
      "loss: 0.9371440410614014\n",
      "epoch: 937\n",
      "loss: 0.9363349080085754\n",
      "epoch: 938\n",
      "loss: 0.9355524778366089\n",
      "epoch: 939\n",
      "loss: 0.934766948223114\n",
      "epoch: 940\n",
      "loss: 0.9339829683303833\n",
      "epoch: 941\n",
      "loss: 0.9331903457641602\n",
      "epoch: 942\n",
      "loss: 0.9324021935462952\n",
      "epoch: 943\n",
      "loss: 0.931617259979248\n",
      "epoch: 944\n",
      "loss: 0.9308657646179199\n",
      "epoch: 945\n",
      "loss: 0.9300540089607239\n",
      "epoch: 946\n",
      "loss: 0.9292733073234558\n",
      "epoch: 947\n",
      "loss: 0.9285003542900085\n",
      "epoch: 948\n",
      "loss: 0.9277210235595703\n",
      "epoch: 949\n",
      "loss: 0.9269492626190186\n",
      "epoch: 950\n",
      "loss: 0.9261757731437683\n",
      "epoch: 951\n",
      "loss: 0.9254043102264404\n",
      "epoch: 952\n",
      "loss: 0.924627423286438\n",
      "epoch: 953\n",
      "loss: 0.9238545298576355\n",
      "epoch: 954\n",
      "loss: 0.9230928421020508\n",
      "epoch: 955\n",
      "loss: 0.9223171472549438\n",
      "epoch: 956\n",
      "loss: 0.9215525984764099\n",
      "epoch: 957\n",
      "loss: 0.9207862615585327\n",
      "epoch: 958\n",
      "loss: 0.9200242757797241\n",
      "epoch: 959\n",
      "loss: 0.9192560911178589\n",
      "epoch: 960\n",
      "loss: 0.9184927940368652\n",
      "epoch: 961\n",
      "loss: 0.9177279472351074\n",
      "epoch: 962\n",
      "loss: 0.9169958829879761\n",
      "epoch: 963\n",
      "loss: 0.9162241816520691\n",
      "epoch: 964\n",
      "loss: 0.9154784083366394\n",
      "epoch: 965\n",
      "loss: 0.9147247076034546\n",
      "epoch: 966\n",
      "loss: 0.9139753580093384\n",
      "epoch: 967\n",
      "loss: 0.9132345914840698\n",
      "epoch: 968\n",
      "loss: 0.9124776721000671\n",
      "epoch: 969\n",
      "loss: 0.9117318987846375\n",
      "epoch: 970\n",
      "loss: 0.9109998345375061\n",
      "epoch: 971\n",
      "loss: 0.9102510213851929\n",
      "epoch: 972\n",
      "loss: 0.9095035791397095\n",
      "epoch: 973\n",
      "loss: 0.9087646007537842\n",
      "epoch: 974\n",
      "loss: 0.9080447554588318\n",
      "epoch: 975\n",
      "loss: 0.9072892665863037\n",
      "epoch: 976\n",
      "loss: 0.906552791595459\n",
      "epoch: 977\n",
      "loss: 0.9058184027671814\n",
      "epoch: 978\n",
      "loss: 0.9050905108451843\n",
      "epoch: 979\n",
      "loss: 0.904346227645874\n",
      "epoch: 980\n",
      "loss: 0.903610110282898\n",
      "epoch: 981\n",
      "loss: 0.9028733372688293\n",
      "epoch: 982\n",
      "loss: 0.9021357893943787\n",
      "epoch: 983\n",
      "loss: 0.9014007449150085\n",
      "epoch: 984\n",
      "loss: 0.9006742238998413\n",
      "epoch: 985\n",
      "loss: 0.8999341726303101\n",
      "epoch: 986\n",
      "loss: 0.8992021679878235\n",
      "epoch: 987\n",
      "loss: 0.8984711170196533\n",
      "epoch: 988\n",
      "loss: 0.8977542519569397\n",
      "epoch: 989\n",
      "loss: 0.8970149755477905\n",
      "epoch: 990\n",
      "loss: 0.8962902426719666\n",
      "epoch: 991\n",
      "loss: 0.8955803513526917\n",
      "epoch: 992\n",
      "loss: 0.8948457837104797\n",
      "epoch: 993\n",
      "loss: 0.8941243886947632\n",
      "epoch: 994\n",
      "loss: 0.8934046626091003\n",
      "epoch: 995\n",
      "loss: 0.8926836848258972\n",
      "epoch: 996\n",
      "loss: 0.8919687271118164\n",
      "epoch: 997\n",
      "loss: 0.8912521600723267\n",
      "epoch: 998\n",
      "loss: 0.8905382752418518\n",
      "epoch: 999\n",
      "loss: 0.8898295760154724\n",
      "Initialized dense model\n",
      "1000 epochs, val_objective 8.89e-01, val_loss 8.89e-01, regularization 2.28e+01, l2_regularization 1.14e+01\n",
      "epoch: 0\n",
      "loss: 0.8892061710357666\n",
      "epoch: 1\n",
      "loss: 0.889105498790741\n",
      "epoch: 2\n",
      "loss: 0.8889740705490112\n",
      "epoch: 3\n",
      "loss: 0.8888329267501831\n",
      "epoch: 4\n",
      "loss: 0.8886505365371704\n",
      "epoch: 5\n",
      "loss: 0.8884317874908447\n",
      "epoch: 6\n",
      "loss: 0.8881820440292358\n",
      "epoch: 7\n",
      "loss: 0.8879026174545288\n",
      "epoch: 8\n",
      "loss: 0.8875977396965027\n",
      "epoch: 9\n",
      "loss: 0.8872720003128052\n",
      "Lambda = 3.52e-03, selected 84 features \n",
      "10 epochs, val_objective 9.67e-01, val_loss 8.87e-01, regularization 2.29e+01, l2_regularization 1.15e+01\n"
     ]
    }
   ],
   "source": [
    "w_lasso=w_fastcph.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef8c961b-df78-490a-b319-0d37f6e5060e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003523036437696125"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lasso.fit_lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9e1b271c-1b52-4fa1-9a77-f7b67af316e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.345862  ,  -7.1754155 , -24.634806  ,  -7.1119146 ,\n",
       "        -2.4740973 , -24.452074  , -24.44435   , -13.838684  ,\n",
       "        -1.7425482 , -21.197063  , -18.61378   , -10.23551   ,\n",
       "         0.46123374, -11.6502495 , -16.697102  ,  -8.088986  ,\n",
       "        -6.4313602 ,  -0.7819879 ,  -8.450064  , -18.331095  ,\n",
       "       -20.881905  , -25.489023  , -11.401772  , -10.583445  ,\n",
       "        -8.251638  ,   4.3901577 ,  -5.185144  , -22.502214  ,\n",
       "        -3.3516102 ,  -9.552089  ,   0.52958584, -16.273197  ,\n",
       "        -4.147382  ,   0.46969783, -27.83774   , -27.202974  ,\n",
       "       -11.805508  ,  -3.069765  ,  14.027991  , -22.521828  ,\n",
       "        -7.019005  , -16.55804   , -23.008898  ,  -6.505207  ,\n",
       "       -16.505173  ,  -1.6809804 , -24.238497  ,  -2.6657643 ,\n",
       "        -7.6268926 ,  -7.7024508 ], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32e258cb-c868-4498-b06a-ced8608a99f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995648389904265, 0.6018099547511312)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_lasso.score(X_train, y_train), w_lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f8b123b6-ee2b-4012-b234-3dc977ada410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4339.5, 2169.75, 8679.0]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#survhive.get_time(y_train)\n",
    "median_time = numpy.median(survhive.get_time(y_train))\n",
    "times =[ median_time, median_time/2, median_time*2]\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "456a6c74-5fe4-4686-832a-ff656a0a18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = w_lasso.predict_survival(X_train[4:5], times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9b98b57a-2316-496c-8b8b-19ff1fe932d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.87416170e+04, 1.38645206e-01, 2.43241167e+06]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d1e3a60e-7373-40e6-a796-54d3e0023352",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = survhive.metrics.make_time_dependent_scorer(\n",
    "            survhive.metrics.neg_brier_score,\n",
    "            time_mode=\"quantiles\",\n",
    "            time_values=[0.1, 0.25, 0.4, 0.5, 0.6, 0.75],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc49cb40-c233-4b9a-951a-68cd241af18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/ivan/Unito/conda/envs/fastcph/lib/python3.8/site-packages/sksurv/linear_model/coxph.py:124: RuntimeWarning: overflow encountered in power\n",
      "  funcs[i] = StepFunction(x=self.baseline_survival_.x, y=np.power(self.baseline_survival_.y, risk_score[i]))\n",
      "/usr/local/ivan/Unito/conda/envs/fastcph/lib/python3.8/site-packages/sksurv/linear_model/coxph.py:124: RuntimeWarning: overflow encountered in power\n",
      "  funcs[i] = StepFunction(x=self.baseline_survival_.x, y=np.power(self.baseline_survival_.y, risk_score[i]))\n",
      "/usr/local/ivan/Unito/survhive/survhive/metrics.py:96: RuntimeWarning: overflow encountered in square\n",
      "  return -numpy.square(err).mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.004527416573666531, -inf)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorer(w_lasso, X_train, y_train), scorer(w_lasso, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50658dd5-ab48-4251-ab86-626c4ccd22a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([StepFunction(x=array([ 269.,  289.,  394.,  404.,  421.,  434.,  530.,  649.,  690.,\n",
       "               723.,  730.,  794.,  880.,  910.,  958.,  994., 1023., 1112.,\n",
       "              1136., 1171., 1206., 1215., 1259., 1290., 1554., 1591., 1598.,\n",
       "              1693., 1703., 1721., 1731., 1742., 1743., 1781., 1815., 2105.,\n",
       "              2196., 2225., 2349., 2454., 2549., 2561., 2604., 2647., 2672.,\n",
       "              2892., 2909., 2965., 2972., 3121., 3188., 3296., 3311., 3313.,\n",
       "              3492., 3544., 3555., 3614., 3721., 3771., 3800., 3822., 3900.,\n",
       "              3914., 3976., 4052., 4066., 4085., 4178., 4181., 4227., 4279.,\n",
       "              4297., 4327., 4352., 4353., 4415., 4472., 4487., 4537., 4555.,\n",
       "              4596., 4603., 4642., 4652., 4672., 4675., 4681., 4691., 4791.,\n",
       "              4830., 4857., 4863., 4866., 4925., 4952., 4980., 5012., 5014.,\n",
       "              5035., 5110., 5181., 5228., 5238., 5306., 5316., 5379., 5519.,\n",
       "              5533., 5571., 5602., 5612., 5651., 5653., 5661., 5673., 5683.,\n",
       "              5684., 5685., 5693., 5714., 5723., 5768., 5779., 5816., 5823.,\n",
       "              5860., 5917., 5945., 5947., 5984., 6003., 6007., 6132., 6135.,\n",
       "              6161., 6232., 6240., 6249., 6281., 6295., 6298., 6399., 6507.,\n",
       "              6591., 6943., 7023., 7057.]), y=array([9.99999924e-01, 9.99999619e-01, 9.99998092e-01, 9.99995664e-01,\n",
       "              9.99990195e-01, 9.99978104e-01, 9.99951477e-01, 9.99893362e-01,\n",
       "              9.99767404e-01, 9.99591625e-01, 9.99283694e-01, 9.98808454e-01,\n",
       "              9.98026546e-01, 9.96727181e-01, 9.96727181e-01, 9.94725650e-01,\n",
       "              9.91558960e-01, 9.86527899e-01, 9.78541865e-01, 9.65821277e-01,\n",
       "              9.45771778e-01, 9.45771778e-01, 9.19784815e-01, 9.19784815e-01,\n",
       "              9.19784815e-01, 9.19784815e-01, 8.81965592e-01, 8.27878782e-01,\n",
       "              8.27878782e-01, 7.52252484e-01, 6.49937747e-01, 4.99188571e-01,\n",
       "              3.17349653e-01, 3.17349653e-01, 1.38645206e-01, 1.38645206e-01,\n",
       "              1.38645206e-01, 1.38645206e-01, 2.37965942e-02, 4.55865957e-05,\n",
       "              4.55865957e-05, 4.55865957e-05, 8.20624043e-01, 8.20624043e-01,\n",
       "              8.20624043e-01, 8.20624043e-01, 8.20624043e-01, 8.20624043e-01,\n",
       "              8.20624043e-01, 2.52965607e+01, 2.52965607e+01, 2.52965607e+01,\n",
       "              2.52965607e+01, 2.36926098e+02, 2.36926098e+02, 2.36926098e+02,\n",
       "              2.36926098e+02, 2.36926098e+02, 2.36926098e+02, 2.36926098e+02,\n",
       "              2.36926098e+02, 1.71829117e+03, 1.71829117e+03, 1.71829117e+03,\n",
       "              1.11600458e+04, 1.11600458e+04, 6.87416170e+04, 6.87416170e+04,\n",
       "              6.87416170e+04, 6.87416170e+04, 6.87416170e+04, 6.87416170e+04,\n",
       "              6.87416170e+04, 6.87416170e+04, 6.87416170e+04, 4.11686726e+05,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06,\n",
       "              2.43241167e+06, 2.43241167e+06, 2.43241167e+06, 2.43241167e+06]), a=1.0, b=0.0)                       ,\n",
       "       StepFunction(x=array([ 269.,  289.,  394.,  404.,  421.,  434.,  530.,  649.,  690.,\n",
       "               723.,  730.,  794.,  880.,  910.,  958.,  994., 1023., 1112.,\n",
       "              1136., 1171., 1206., 1215., 1259., 1290., 1554., 1591., 1598.,\n",
       "              1693., 1703., 1721., 1731., 1742., 1743., 1781., 1815., 2105.,\n",
       "              2196., 2225., 2349., 2454., 2549., 2561., 2604., 2647., 2672.,\n",
       "              2892., 2909., 2965., 2972., 3121., 3188., 3296., 3311., 3313.,\n",
       "              3492., 3544., 3555., 3614., 3721., 3771., 3800., 3822., 3900.,\n",
       "              3914., 3976., 4052., 4066., 4085., 4178., 4181., 4227., 4279.,\n",
       "              4297., 4327., 4352., 4353., 4415., 4472., 4487., 4537., 4555.,\n",
       "              4596., 4603., 4642., 4652., 4672., 4675., 4681., 4691., 4791.,\n",
       "              4830., 4857., 4863., 4866., 4925., 4952., 4980., 5012., 5014.,\n",
       "              5035., 5110., 5181., 5228., 5238., 5306., 5316., 5379., 5519.,\n",
       "              5533., 5571., 5602., 5612., 5651., 5653., 5661., 5673., 5683.,\n",
       "              5684., 5685., 5693., 5714., 5723., 5768., 5779., 5816., 5823.,\n",
       "              5860., 5917., 5945., 5947., 5984., 6003., 6007., 6132., 6135.,\n",
       "              6161., 6232., 6240., 6249., 6281., 6295., 6298., 6399., 6507.,\n",
       "              6591., 6943., 7023., 7057.]), y=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "              1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), a=1.0, b=0.0)                                           ],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_breslow = w_lasso.breslow_estimator_.get_survival_function(w_lasso.predict(X_train[4:6]))\n",
    "w_breslow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db9c2e6b-56ca-4920-a2e2-0ca2faa3445a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.39113534, 1.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r= numpy.array(\n",
    "    [ numpy.interp( _t, p.x, p.y, left=1.0 ) for _t in [median_time, median_time/2]  for p in w_breslow]\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b40da-ea95-4de6-8199-7d8c430ec8f9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2674ae8-2b6b-4c54-8f50-619050293d77",
   "metadata": {},
   "source": [
    "## Test with CV-based lambda selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1295d-27e9-4d3d-8e00-05baa4264ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_set=survhive.survival_crossval_splitter(X_train,y_train,\n",
    "                                           n_splits=3,\n",
    "                                           n_repeats=1,\n",
    "                                           rng_seed=seed)\n",
    "cv_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c13e4b-ce00-4ac5-ae84-bfd6e305fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcph.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1812f8-321d-4fd7-8eb4-e1ce23a4968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcph_cv=lassonet.LassoNetCoxRegressorCV(cv=cv_set,\n",
    "                                          hidden_dims=(8,8),\n",
    "                                          tie_approximation='breslow',\n",
    "                                          path_multiplier=1.02,\n",
    "                                           backtrack=True,\n",
    "                                          torch_seed=seed,random_state=seed)\n",
    "fastcph_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abd531-ba09-4faf-a129-dfd9006f1b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastcph_cv.fit(X_train,yup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56769315-9ca3-4960-8b86-a6b73d0758f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcph_cv.best_cv_scores_, fastcph_cv.best_lambda_, fastcph_cv.best_cv_score_, fastcph_cv.best_selected_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3a65e-f937-4508-809d-ae749d8347bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcph_cv.score(X_test,yup_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9b3ff-1617-471e-9c8b-bce2b31f698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastcph_cv.path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e61f13-47ef-42d3-adc7-3f283d2dca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastch_cv.path_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99236d45-cc0c-4251-bae5-bc3f2ecccc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitted_fastcph.path_.loss , fitted_fastcph.path_.lambda \n",
    "\n",
    "lambda_cv_min = min(fastcph_cv.path_, key=(lambda x: x.objective))\n",
    "#lambda_min = min([(_.objective, _.lambda_)  for _ in fitted_fastcph.path_])\n",
    "lambda_cv_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e92ad3-5789-4ae6-b729-c1e91b57f946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fastcph.set_params(lambda_seq=[lambda_cv_min.lambda_],\n",
    "                   random_state=seed,torch_seed=seed,\n",
    "                   verbose=1)\n",
    "refit_fastcph_cv=fastcph.fit(X_train,yup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10153f-b986-42ea-9c46-c008e22c6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_fastcph_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a38ce-5f15-4c6c-ab56-0c48c56d0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_fastcph_cv.score(X_test,yup_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
